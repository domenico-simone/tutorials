{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 Welcome! This website is a collection of Bioinformatics tutorials originally developed by Hadrien Gourl\u00e9 for a bioinformatics course at the Swedish University of Agricultural Sciences and during various bioinformatics workshops around the globe. In this website they are specifically adapted for the Bioinformatics course BI1324 (30224), held in 2020. Feel free to follow them online, or to use and modify them for your own teaching. Available lessons \u00b6 Home The command-line File Formats Quality Control and Trimming Mapping and Variant Calling De-novo Genome Assembly Genome Annotation Pan-Genome Analysis Metabarcoding Whole Metagenome Sequencing Metagenome assembly RNA-Seq Introduction to Nanopore Sequencing Contributing \u00b6 A typo? Something that irks you? Submit an issue or a pull request. In no particular order, the follwing people have contributed to these tutorials: Hadrien Gourl\u00e9 Oskar Karlsson-Lindsj\u00f6 Juliette Hayer Domenico Simone License \u00b6 This work is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.","title":"Home"},{"location":"#home","text":"Welcome! This website is a collection of Bioinformatics tutorials originally developed by Hadrien Gourl\u00e9 for a bioinformatics course at the Swedish University of Agricultural Sciences and during various bioinformatics workshops around the globe. In this website they are specifically adapted for the Bioinformatics course BI1324 (30224), held in 2020. Feel free to follow them online, or to use and modify them for your own teaching.","title":"Home"},{"location":"#available-lessons","text":"Home The command-line File Formats Quality Control and Trimming Mapping and Variant Calling De-novo Genome Assembly Genome Annotation Pan-Genome Analysis Metabarcoding Whole Metagenome Sequencing Metagenome assembly RNA-Seq Introduction to Nanopore Sequencing","title":"Available lessons"},{"location":"#contributing","text":"A typo? Something that irks you? Submit an issue or a pull request. In no particular order, the follwing people have contributed to these tutorials: Hadrien Gourl\u00e9 Oskar Karlsson-Lindsj\u00f6 Juliette Hayer Domenico Simone","title":"Contributing"},{"location":"#license","text":"This work is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.","title":"License"},{"location":"16S/","text":"Metabarcoding \u00b6 This tutorial is aimed at being a walkthrough of the DADA2 pipeline. It uses the data of the now famous MiSeq SOP by the Mothur authors but analyses the data using DADA2. DADA2 is a relatively new method to analyse amplicon data which uses exact variants instead of OTUs. The advantages of the DADA2 method is described in the paper Before Starting \u00b6 There are two ways to follow this tutorial: you can copy and paste all the codes blocks below in R directly, or you can download this document in the Rmarkdown format and execute the cells. Link to the document in Rmarkdown Install and Load Packages \u00b6 First install DADA2 and other necessary packages source ( 'https://bioconductor.org/biocLite.R' ) biocLite ( 'dada2' ) biocLite ( 'phyloseq' ) biocLite ( 'DECIPHER' ) install.packages ( 'ggplot2' ) install.packages ( 'phangorn' ) Now load the packages and verify you have the correct DADA2 version library ( dada2 ) library ( ggplot2 ) library ( phyloseq ) library ( phangorn ) library ( DECIPHER ) packageVersion ( 'dada2' ) Download the Data \u00b6 You will also need to download the data, as well as the SILVA database Warning If you are following the tutorial on the website, the following block of commands has to be executed outside of R. If you run this tutorial with the R notebook, you can simply execute the cell block wget http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip unzip MiSeqSOPData.zip rm -r __MACOSX/ cd MiSeq_SOP wget https://zenodo.org/record/824551/files/silva_nr_v128_train_set.fa.gz wget https://zenodo.org/record/824551/files/silva_species_assignment_v128.fa.gz cd .. Back in R, check that you have downloaded the data path <- 'MiSeq_SOP' list.files ( path ) Filtering and Trimming \u00b6 First we create two lists with the sorted name of the reads: one for the forward reads, one for the reverse reads raw_forward <- sort ( list.files ( path , pattern = \"_R1_001.fastq\" , full.names = TRUE )) raw_reverse <- sort ( list.files ( path , pattern = \"_R2_001.fastq\" , full.names = TRUE )) # we also need the sample names sample_names <- sapply ( strsplit ( basename ( raw_forward ), \"_\" ), `[` , # extracts the first element of a subset 1 ) then we visualise the quality of our reads plotQualityProfile ( raw_forward[1 : 2 ] ) plotQualityProfile ( raw_reverse[1 : 2 ] ) Question What do you think of the read quality? The forward reads are good quality (although dropping a bit at the end as usual) while the reverse are way worse. Based on these profiles, we will truncate the forward reads at position 240 and the reverse reads at position 160 where the quality distribution crashes. Note in this tutorial we perform the trimming using DADA2's own functions. If you wish to do it outside of DADA2, you can refer to the Quality Control tutorial Dada2 requires us to define the name of our output files # place filtered files in filtered/ subdirectory filtered_path <- file.path ( path , \"filtered\" ) filtered_forward <- file.path ( filtered_path , paste0 ( sample_names , \"_R1_trimmed.fastq.gz\" )) filtered_reverse <- file.path ( filtered_path , paste0 ( sample_names , \"_R2_trimmed.fastq.gz\" )) We\u2019ll use standard filtering parameters: maxN=0 (DADA22 requires no Ns), truncQ=2 , rm.phix=TRUE and maxEE=2 . The maxEE parameter sets the maximum number of \u201cexpected errors\u201d allowed in a read, which according to the USEARCH authors is a better filter than simply averaging quality scores. out <- filterAndTrim ( raw_forward , filtered_forward , raw_reverse , filtered_reverse , truncLen = c ( 240 , 160 ), maxN = 0 , maxEE = c ( 2 , 2 ), truncQ = 2 , rm.phix = TRUE , compress = TRUE , multithread = TRUE ) head ( out ) Learn the Error Rates \u00b6 The DADA2 algorithm depends on a parametric error model and every amplicon dataset has a slightly different error rate. The learnErrors of Dada2 learns the error model from the data and will help DADA2 to fits its method to your data errors_forward <- learnErrors ( filtered_forward , multithread = TRUE ) errors_reverse <- learnErrors ( filtered_reverse , multithread = TRUE ) then we visualise the estimated error rates plotErrors ( errors_forward , nominalQ = TRUE ) + theme_minimal () Question Do you think the error model fits your data correctly? Dereplication \u00b6 From the Dada2 documentation: Dereplication combines all identical sequencing reads into into \u201cunique sequences\u201d with a corresponding \u201cabundance\u201d: the number of reads with that unique sequence. Dereplication substantially reduces computation time by eliminating redundant comparisons. derep_forward <- derepFastq ( filtered_forward , verbose = TRUE ) derep_reverse <- derepFastq ( filtered_reverse , verbose = TRUE ) # name the derep-class objects by the sample names names ( derep_forward ) <- sample_names names ( derep_reverse ) <- sample_names Sample inference \u00b6 We are now ready to apply the core sequence-variant inference algorithm to the dereplicated data. dada_forward <- dada ( derep_forward , err = errors_forward , multithread = TRUE ) dada_reverse <- dada ( derep_reverse , err = errors_reverse , multithread = TRUE ) # inspect the dada-class object dada_forward[[1]] The DADA2 algorithm inferred 128 real sequence variants from the 1979 unique sequences in the first sample. Merge Paired-end Reads \u00b6 Now that the reads are trimmed, dereplicated and error-corrected we can merge them together merged_reads <- mergePairs ( dada_forward , derep_forward , dada_reverse , derep_reverse , verbose = TRUE ) # inspect the merger data.frame from the first sample head ( merged_reads[[1]] ) Construct Sequence Table \u00b6 We can now construct a sequence table of our mouse samples, a higher-resolution version of the OTU table produced by traditional methods. seq_table <- makeSequenceTable ( merged_reads ) dim ( seq_table ) # inspect distribution of sequence lengths table ( nchar ( getSequences ( seq_table ))) Remove Chimeras \u00b6 The dada method used earlier removes substitutions and indel errors but chimeras remain. We remove the chimeras with seq_table_nochim <- removeBimeraDenovo ( seq_table , method = 'consensus' , multithread = TRUE , verbose = TRUE ) dim ( seq_table_nochim ) # which percentage of our reads did we keep? sum ( seq_table_nochim ) / sum ( seq_table ) As a final check of our progress, we\u2019ll look at the number of reads that made it through each step in the pipeline get_n <- function ( x ) sum ( getUniques ( x )) track <- cbind ( out , sapply ( dada_forward , get_n ), sapply ( merged_reads , get_n ), rowSums ( seq_table ), rowSums ( seq_table_nochim )) colnames ( track ) <- c ( 'input' , 'filtered' , 'denoised' , 'merged' , 'tabled' , 'nonchim' ) rownames ( track ) <- sample_names head ( track ) We kept the majority of our reads! Assign Taxonomy \u00b6 Now we assign taxonomy to our sequences using the SILVA database taxa <- assignTaxonomy ( seq_table_nochim , 'MiSeq_SOP/silva_nr_v128_train_set.fa.gz' , multithread = TRUE ) taxa <- addSpecies ( taxa , 'MiSeq_SOP/silva_species_assignment_v128.fa.gz' ) for inspecting the classification taxa_print <- taxa # removing sequence rownames for display only rownames ( taxa_print ) <- NULL head ( taxa_print ) Phylogenetic Tree \u00b6 DADA2 is reference-free so we have to build the tree ourselves We first align our sequences sequences <- getSequences ( seq_table ) names ( sequences ) <- sequences # this propagates to the tip labels of the tree alignment <- AlignSeqs ( DNAStringSet ( sequences ), anchor = NA ) Then we build a neighbour-joining tree then fit a maximum likelihood tree using the neighbour-joining tree as a starting point phang_align <- phyDat ( as ( alignment , 'matrix' ), type = 'DNA' ) dm <- dist.ml ( phang_align ) treeNJ <- NJ ( dm ) # note, tip order != sequence order fit = pml ( treeNJ , data = phang_align ) ## negative edges length changed to 0! fitGTR <- update ( fit , k = 4 , inv = 0.2 ) fitGTR <- optim.pml ( fitGTR , model = 'GTR' , optInv = TRUE , optGamma = TRUE , rearrangement = 'stochastic' , control = pml.control ( trace = 0 )) detach ( 'package:phangorn' , unload = TRUE ) Phyloseq \u00b6 First load the metadata sample_data <- read.table ( 'https://hadrieng.github.io/tutorials/data/16S_metadata.txt' , header = TRUE , row.names = \"sample_name\" ) We can now construct a phyloseq object from our output and newly created metadata physeq <- phyloseq ( otu_table ( seq_table_nochim , taxa_are_rows = FALSE ), sample_data ( sample_data ), tax_table ( taxa ), phy_tree ( fitGTR $ tree )) # remove mock sample physeq <- prune_samples ( sample_names ( physeq ) != 'Mock' , physeq ) physeq Let's look at the alpha diversity of our samples plot_richness ( physeq , x = 'day' , measures = c ( 'Shannon' , 'Fisher' ), color = 'when' ) + theme_minimal () No obvious differences. Let's look at ordination methods (beta diversity) We can perform an MDS with euclidean distance (mathematically equivalent to a PCA) ord <- ordinate ( physeq , 'MDS' , 'euclidean' ) plot_ordination ( physeq , ord , type = 'samples' , color = 'when' , title = 'PCA of the samples from the MiSeq SOP' ) + theme_minimal () now with the Bray-Curtis distance ord <- ordinate ( physeq , 'NMDS' , 'bray' ) plot_ordination ( physeq , ord , type = 'samples' , color = 'when' , title = 'PCA of the samples from the MiSeq SOP' ) + theme_minimal () There we can see a clear difference between our samples. Let us take a look a the distribution of the most abundant families top20 <- names ( sort ( taxa_sums ( physeq ), decreasing = TRUE )) [1 : 20 ] physeq_top20 <- transform_sample_counts ( physeq , function ( OTU ) OTU / sum ( OTU )) physeq_top20 <- prune_taxa ( top20 , physeq_top20 ) plot_bar ( physeq_top20 , x = 'day' , fill = 'Family' ) + facet_wrap ( ~ when , scales = 'free_x' ) + theme_minimal () We can place them in a tree bacteroidetes <- subset_taxa ( physeq , Phylum %in% c ( 'Bacteroidetes' )) plot_tree ( bacteroidetes , ladderize = 'left' , size = 'abundance' , color = 'when' , label.tips = 'Family' )","title":"Metabarcoding"},{"location":"16S/#metabarcoding","text":"This tutorial is aimed at being a walkthrough of the DADA2 pipeline. It uses the data of the now famous MiSeq SOP by the Mothur authors but analyses the data using DADA2. DADA2 is a relatively new method to analyse amplicon data which uses exact variants instead of OTUs. The advantages of the DADA2 method is described in the paper","title":"Metabarcoding"},{"location":"16S/#before-starting","text":"There are two ways to follow this tutorial: you can copy and paste all the codes blocks below in R directly, or you can download this document in the Rmarkdown format and execute the cells. Link to the document in Rmarkdown","title":"Before Starting"},{"location":"16S/#install-and-load-packages","text":"First install DADA2 and other necessary packages source ( 'https://bioconductor.org/biocLite.R' ) biocLite ( 'dada2' ) biocLite ( 'phyloseq' ) biocLite ( 'DECIPHER' ) install.packages ( 'ggplot2' ) install.packages ( 'phangorn' ) Now load the packages and verify you have the correct DADA2 version library ( dada2 ) library ( ggplot2 ) library ( phyloseq ) library ( phangorn ) library ( DECIPHER ) packageVersion ( 'dada2' )","title":"Install and Load Packages"},{"location":"16S/#download-the-data","text":"You will also need to download the data, as well as the SILVA database Warning If you are following the tutorial on the website, the following block of commands has to be executed outside of R. If you run this tutorial with the R notebook, you can simply execute the cell block wget http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip unzip MiSeqSOPData.zip rm -r __MACOSX/ cd MiSeq_SOP wget https://zenodo.org/record/824551/files/silva_nr_v128_train_set.fa.gz wget https://zenodo.org/record/824551/files/silva_species_assignment_v128.fa.gz cd .. Back in R, check that you have downloaded the data path <- 'MiSeq_SOP' list.files ( path )","title":"Download the Data"},{"location":"16S/#filtering-and-trimming","text":"First we create two lists with the sorted name of the reads: one for the forward reads, one for the reverse reads raw_forward <- sort ( list.files ( path , pattern = \"_R1_001.fastq\" , full.names = TRUE )) raw_reverse <- sort ( list.files ( path , pattern = \"_R2_001.fastq\" , full.names = TRUE )) # we also need the sample names sample_names <- sapply ( strsplit ( basename ( raw_forward ), \"_\" ), `[` , # extracts the first element of a subset 1 ) then we visualise the quality of our reads plotQualityProfile ( raw_forward[1 : 2 ] ) plotQualityProfile ( raw_reverse[1 : 2 ] ) Question What do you think of the read quality? The forward reads are good quality (although dropping a bit at the end as usual) while the reverse are way worse. Based on these profiles, we will truncate the forward reads at position 240 and the reverse reads at position 160 where the quality distribution crashes. Note in this tutorial we perform the trimming using DADA2's own functions. If you wish to do it outside of DADA2, you can refer to the Quality Control tutorial Dada2 requires us to define the name of our output files # place filtered files in filtered/ subdirectory filtered_path <- file.path ( path , \"filtered\" ) filtered_forward <- file.path ( filtered_path , paste0 ( sample_names , \"_R1_trimmed.fastq.gz\" )) filtered_reverse <- file.path ( filtered_path , paste0 ( sample_names , \"_R2_trimmed.fastq.gz\" )) We\u2019ll use standard filtering parameters: maxN=0 (DADA22 requires no Ns), truncQ=2 , rm.phix=TRUE and maxEE=2 . The maxEE parameter sets the maximum number of \u201cexpected errors\u201d allowed in a read, which according to the USEARCH authors is a better filter than simply averaging quality scores. out <- filterAndTrim ( raw_forward , filtered_forward , raw_reverse , filtered_reverse , truncLen = c ( 240 , 160 ), maxN = 0 , maxEE = c ( 2 , 2 ), truncQ = 2 , rm.phix = TRUE , compress = TRUE , multithread = TRUE ) head ( out )","title":"Filtering and Trimming"},{"location":"16S/#learn-the-error-rates","text":"The DADA2 algorithm depends on a parametric error model and every amplicon dataset has a slightly different error rate. The learnErrors of Dada2 learns the error model from the data and will help DADA2 to fits its method to your data errors_forward <- learnErrors ( filtered_forward , multithread = TRUE ) errors_reverse <- learnErrors ( filtered_reverse , multithread = TRUE ) then we visualise the estimated error rates plotErrors ( errors_forward , nominalQ = TRUE ) + theme_minimal () Question Do you think the error model fits your data correctly?","title":"Learn the Error Rates"},{"location":"16S/#dereplication","text":"From the Dada2 documentation: Dereplication combines all identical sequencing reads into into \u201cunique sequences\u201d with a corresponding \u201cabundance\u201d: the number of reads with that unique sequence. Dereplication substantially reduces computation time by eliminating redundant comparisons. derep_forward <- derepFastq ( filtered_forward , verbose = TRUE ) derep_reverse <- derepFastq ( filtered_reverse , verbose = TRUE ) # name the derep-class objects by the sample names names ( derep_forward ) <- sample_names names ( derep_reverse ) <- sample_names","title":"Dereplication"},{"location":"16S/#sample-inference","text":"We are now ready to apply the core sequence-variant inference algorithm to the dereplicated data. dada_forward <- dada ( derep_forward , err = errors_forward , multithread = TRUE ) dada_reverse <- dada ( derep_reverse , err = errors_reverse , multithread = TRUE ) # inspect the dada-class object dada_forward[[1]] The DADA2 algorithm inferred 128 real sequence variants from the 1979 unique sequences in the first sample.","title":"Sample inference"},{"location":"16S/#merge-paired-end-reads","text":"Now that the reads are trimmed, dereplicated and error-corrected we can merge them together merged_reads <- mergePairs ( dada_forward , derep_forward , dada_reverse , derep_reverse , verbose = TRUE ) # inspect the merger data.frame from the first sample head ( merged_reads[[1]] )","title":"Merge Paired-end Reads"},{"location":"16S/#construct-sequence-table","text":"We can now construct a sequence table of our mouse samples, a higher-resolution version of the OTU table produced by traditional methods. seq_table <- makeSequenceTable ( merged_reads ) dim ( seq_table ) # inspect distribution of sequence lengths table ( nchar ( getSequences ( seq_table )))","title":"Construct Sequence Table"},{"location":"16S/#remove-chimeras","text":"The dada method used earlier removes substitutions and indel errors but chimeras remain. We remove the chimeras with seq_table_nochim <- removeBimeraDenovo ( seq_table , method = 'consensus' , multithread = TRUE , verbose = TRUE ) dim ( seq_table_nochim ) # which percentage of our reads did we keep? sum ( seq_table_nochim ) / sum ( seq_table ) As a final check of our progress, we\u2019ll look at the number of reads that made it through each step in the pipeline get_n <- function ( x ) sum ( getUniques ( x )) track <- cbind ( out , sapply ( dada_forward , get_n ), sapply ( merged_reads , get_n ), rowSums ( seq_table ), rowSums ( seq_table_nochim )) colnames ( track ) <- c ( 'input' , 'filtered' , 'denoised' , 'merged' , 'tabled' , 'nonchim' ) rownames ( track ) <- sample_names head ( track ) We kept the majority of our reads!","title":"Remove Chimeras"},{"location":"16S/#assign-taxonomy","text":"Now we assign taxonomy to our sequences using the SILVA database taxa <- assignTaxonomy ( seq_table_nochim , 'MiSeq_SOP/silva_nr_v128_train_set.fa.gz' , multithread = TRUE ) taxa <- addSpecies ( taxa , 'MiSeq_SOP/silva_species_assignment_v128.fa.gz' ) for inspecting the classification taxa_print <- taxa # removing sequence rownames for display only rownames ( taxa_print ) <- NULL head ( taxa_print )","title":"Assign Taxonomy"},{"location":"16S/#phylogenetic-tree","text":"DADA2 is reference-free so we have to build the tree ourselves We first align our sequences sequences <- getSequences ( seq_table ) names ( sequences ) <- sequences # this propagates to the tip labels of the tree alignment <- AlignSeqs ( DNAStringSet ( sequences ), anchor = NA ) Then we build a neighbour-joining tree then fit a maximum likelihood tree using the neighbour-joining tree as a starting point phang_align <- phyDat ( as ( alignment , 'matrix' ), type = 'DNA' ) dm <- dist.ml ( phang_align ) treeNJ <- NJ ( dm ) # note, tip order != sequence order fit = pml ( treeNJ , data = phang_align ) ## negative edges length changed to 0! fitGTR <- update ( fit , k = 4 , inv = 0.2 ) fitGTR <- optim.pml ( fitGTR , model = 'GTR' , optInv = TRUE , optGamma = TRUE , rearrangement = 'stochastic' , control = pml.control ( trace = 0 )) detach ( 'package:phangorn' , unload = TRUE )","title":"Phylogenetic Tree"},{"location":"16S/#phyloseq","text":"First load the metadata sample_data <- read.table ( 'https://hadrieng.github.io/tutorials/data/16S_metadata.txt' , header = TRUE , row.names = \"sample_name\" ) We can now construct a phyloseq object from our output and newly created metadata physeq <- phyloseq ( otu_table ( seq_table_nochim , taxa_are_rows = FALSE ), sample_data ( sample_data ), tax_table ( taxa ), phy_tree ( fitGTR $ tree )) # remove mock sample physeq <- prune_samples ( sample_names ( physeq ) != 'Mock' , physeq ) physeq Let's look at the alpha diversity of our samples plot_richness ( physeq , x = 'day' , measures = c ( 'Shannon' , 'Fisher' ), color = 'when' ) + theme_minimal () No obvious differences. Let's look at ordination methods (beta diversity) We can perform an MDS with euclidean distance (mathematically equivalent to a PCA) ord <- ordinate ( physeq , 'MDS' , 'euclidean' ) plot_ordination ( physeq , ord , type = 'samples' , color = 'when' , title = 'PCA of the samples from the MiSeq SOP' ) + theme_minimal () now with the Bray-Curtis distance ord <- ordinate ( physeq , 'NMDS' , 'bray' ) plot_ordination ( physeq , ord , type = 'samples' , color = 'when' , title = 'PCA of the samples from the MiSeq SOP' ) + theme_minimal () There we can see a clear difference between our samples. Let us take a look a the distribution of the most abundant families top20 <- names ( sort ( taxa_sums ( physeq ), decreasing = TRUE )) [1 : 20 ] physeq_top20 <- transform_sample_counts ( physeq , function ( OTU ) OTU / sum ( OTU )) physeq_top20 <- prune_taxa ( top20 , physeq_top20 ) plot_bar ( physeq_top20 , x = 'day' , fill = 'Family' ) + facet_wrap ( ~ when , scales = 'free_x' ) + theme_minimal () We can place them in a tree bacteroidetes <- subset_taxa ( physeq , Phylum %in% c ( 'Bacteroidetes' )) plot_tree ( bacteroidetes , ladderize = 'left' , size = 'abundance' , color = 'when' , label.tips = 'Family' )","title":"Phyloseq"},{"location":"16S_mothur/","text":"Metabarcoding \u00b6 This tutorial is largely inspired of the MiSeq SOP from the Schloss Lab. Kozich JJ, Westcott SL, Baxter NT, Highlander SK, Schloss PD. (2013): Development of a dual-index sequencing strategy and curation pipeline for analyzing amplicon sequence data on the MiSeq Illumina sequencing platform. Applied and Environmental Microbiology. 79(17):5112-20. Table of Contents \u00b6 Introduction Softwares Required for this Tutorial Downloading the Data and Start Mothur Reducing Sequencing and PCR Errors Processing Improved Sequences Analysis OTUs Batch Mode Introduction \u00b6 The 16S rRNA gene is a section of prokaryotic DNA found in all bacteria and archaea. This gene codes for an rRNA, and this rRNA in turn makes up part of the ribosome. The first 'r' in rRNA stands for ribosomal. The ribosome is composed of two subunits, the large subunit (LSU) and the small subunit (SSU). The 16S rRNA gene is a commonly used tool for identifying bacteria for several reasons. First, traditional characterization depended upon phenotypic traits like gram positive or gram negative, bacillus or coccus, etc. Taxonomists today consider analysis of an organism's DNA more reliable than classification based solely on phenotypes. Secondly, researchers may, for a number of reasons, want to identify or classify only the bacteria within a given environmental or medical sample. While there is a homologous gene in eukaryotes, the 18S rRNA gene, it is distinct, thereby rendering the 16S rRNA gene a useful tool for extracting and identifying bacteria as separate from plant, animal, fungal, and protist DNA within the same sample. Thirdly, the 16S rRNA gene is relatively short at 1.5 kb, making it faster and cheaper to sequence than many other unique bacterial genes. Mothur is a command-line computer program for analyzing sequence data from microbial communities and namely 16s data. mothur is licensed under the GPL and is free to use. Softwares Required for this Tutorial \u00b6 mothur mothur_krona Downloading the Data and Start Mothur \u00b6 Firstly, download and unzip the sample dataset: wget http : // www . mothur . org / w / images / d / d6 / MiSeqSOPData . zip unzip MiSeqSOPData . zip In the MiSeq_SOP directory, you'll find the reads files in fastq format, as well as a file called stability.files The first lines of stability.files look like this: F3D0 F3D0_S188_L001_R1_001.fastq F3D0_S188_L001_R2_001.fastq F3D141 F3D141_S207_L001_R1_001.fastq F3D141_S207_L001_R2_001.fastq F3D142 F3D142_S208_L001_R1_001.fastq F3D142_S208_L001_R2_001.fastq F3D143 F3D143_S209_L001_R1_001.fastq F3D143_S209_L001_R2_001.fastq The first column is the name of the sample. The second column is the name of the forward read for that sample and the third columns in the name of the reverse read for that sample. Now it's time to start mothur. Type mothur in your terminal. You should see your prompt changing to mothur > Reducing Sequencing and PCR Errors \u00b6 The first thing we want to do is combine our two sets of reads for each sample and then to combine the data from all of the samples. This is done using the make.contigs command, which requires stability.files as input. This command will extract the sequence and quality score data from your fastq files, create the reverse complement of the reverse read and then join the reads into contigs. make . contigs ( file = stability . files , processors = 8 ) It took 30 secs to process 152360 sequences . Group count : F3D0 7793 F3D1 5869 F3D141 5958 F3D142 3183 F3D143 3178 F3D144 4827 F3D145 7377 F3D146 5021 F3D147 17070 F3D148 12405 F3D149 13083 F3D150 5509 F3D2 19620 F3D3 6758 F3D5 4448 F3D6 7989 F3D7 5129 F3D8 5294 F3D9 7070 Mock 4779 Total of all groups is 152360 Output File Names : stability . trim . contigs . fasta stability . trim . contigs . qual stability . contigs . report stability . scrap . contigs . fasta stability . scrap . contigs . qual stability . contigs . groups The stability.contigs.report file will tell you something about the contig assembly for each read. Let's see what these sequences look like using the summary.seqs command: summary . seqs ( fasta = stability . trim . contigs . fasta ) Start End NBases Ambigs Polymer NumSeqs Minimum : 1 248 248 0 3 1 2 . 5 %- tile : 1 252 252 0 3 3810 25 %- tile : 1 252 252 0 4 38091 Median : 1 252 252 0 4 76181 75 %- tile : 1 253 253 0 5 114271 97 . 5 %- tile : 1 253 253 6 6 148552 Maximum : 1 502 502 249 243 152360 Mean : 1 252 . 811 252 . 811 0 . 70063 4 . 44854 # of Seqs : 152360 This tells us that we have 152360 sequences that for the most part vary between 248 and 253 bases. Interestingly, the longest read in the dataset is 502 bp. Be suspicious of this, the reads are supposed to be 251 bp each. This read clearly didn't assemble well (or at all). Also, note that at least 2.5% of our sequences had some ambiguous base calls. We'll take care of these issues in the next step when we run screen.seqs . screen.seqs(fasta=stability.trim.contigs.fasta, group=stability.contigs.groups, maxambig=0, maxlength=275) You'll notice that mothur remembered that we used 8 processors in make.contigs . To see what else mothur knows about you, run the following: get . current () Current files saved by mothur : fasta = stability . trim . contigs . good . fasta group = stability . contigs . good . groups qfile = stability . trim . contigs . qual processors = 8 summary = stability . trim . contigs . summary What this means is that mothur remembers your latest fasta file and group file as well as the number of processors you have. So you could run: mothur > summary . seqs ( fasta = stability . trim . contigs . good . fasta ) mothur > summary . seqs ( fasta = current ) mothur > summary . seqs () and get the same output for each command. But, now that we have filtered the sequencing errors, let's move to the next step. Processing Improved Sequences \u00b6 We anticipate that many of our sequences are duplicates of each other. Because it's computationally wasteful to align the same sequences several times, we'll make our sequences unique: unique . seqs ( fasta = stability . trim . contigs . good . fasta ) If two sequences have the same identical sequence, then they're considered duplicates and will get merged. In the screen output there are two columns - the first is the number of sequences characterized and the second is the number of unique sequences remaining Another thing to do to make our lives easier is to simplify the names and group files. If you look at the most recent versions of those files you'll see together they are 13 MB. This may not seem like much, but with a full MiSeq run those long sequence names can add up and make life tedious. So we'll run count.seqs to generate a table where the rows are the names of the unique seqeunces and the columns are the names of the groups. The table is then filled with the number of times each unique sequence shows up in each group. This will generate a file called stability.trim.contigs.good.count_table. In subsequent commands we'll use it by using the count option: count . seqs ( name = stability . trim . contigs . good . names , group = stability . contigs . good . groups ) summary . seqs ( count = stability . trim . contigs . good . count_table ) Using stability . trim . contigs . good . unique . fasta as input file for the fasta parameter . Using 8 processors . Start End NBases Ambigs Polymer NumSeqs Minimum : 1 250 250 0 3 1 2 . 5 %- tile : 1 252 252 0 3 3227 25 %- tile : 1 252 252 0 4 32265 Median : 1 252 252 0 4 64530 75 %- tile : 1 253 253 0 5 96794 97 . 5 %- tile : 1 253 253 0 6 125832 Maximum : 1 270 270 0 12 129058 Mean : 1 252 . 462 252 . 462 0 4 . 36663 # of unique seqs : 16477 total # of seqs : 129058 Now we need to align our sequences to the reference alignment. First we need to download the SILVA database. # This step should be done outside mothur wget http : // www . mothur . org / w / images / 9 / 98 / Silva . bacteria . zip unzip Silva . bacteria . zip If you have quit mothur to download the database, rerun the mothur command, then take a look at the database you have downloaded: summary . seqs ( fasta = silva . bacteria / silva . bacteria . fasta , processors = 8 ) Now do the alignment using align.seqs : align . seqs ( fasta = stability . trim . contigs . good . unique . fasta , reference = silva . bacteria / silva . bacteria . fasta ) We can then run summary.seqs again to get a summary of our alignment: summary . seqs ( fasta = stability . trim . contigs . good . unique . align , count = stability . trim . contigs . good . count_table ) You'll see that the bulk of the sequences start at position 13862 and end at position 23444. Some sequences start at position 13144 or 13876 and end at 22587 or 25294. These deviants from the mode positions are likely due to an insertion or deletion at the terminal ends of the aliignments. Sometimes you'll see sequences that start and end at the same position indicating a very poor alignment, which is generally due to non-specific amplification. To make sure that everything overlaps the same region we'll re-run screen.seqs to get sequences that start at or before position 1968 and end at or after position 11550. We'll also set the maximum homopolymer length to 8 since there's nothing in the database with a stretch of 9 or more of the same base in a row (this really could have been done in the first execution of screen.seqs above). Note that we need the count table so that we can update the table for the sequences we're removing and we're also using the summary file so we don't have to figure out again all the start and stop positions: screen . seqs ( fasta = stability . trim . contigs . good . unique . align , count = stability . trim . contigs . good . count_table , summary = stability . trim . contigs . good . unique . summary , start = 13862 , end = 23444 , maxhomop = 8 ) summary . seqs ( fasta = current , count = current ) No we can trim both ends of the aligned reads to be sure the all overlap exactly the same region. We can do this with fliter.seqs filter . seqs ( fasta = stability . trim . contigs . good . unique . good . align , vertical = T , trump = .) We may have introduced redundancy by trimming the ends of the sequences, so we will re-run unique.seqs unique . seqs ( fasta = stability . trim . contigs . good . unique . good . filter . fasta , count = stability . trim . contigs . good . good . count_table ) This identified 3 duplicate sequences that we've now merged with previous unique sequences. The next thing we want to do to further de-noise our sequences is to pre-cluster the sequences using the pre.cluster command allowing for up to 2 differences between sequences. This command will split the sequences by group and then sort them by abundance and go from most abundant to least and identify sequences that are within 2 nt of each other. If they are then they get merged. We generally favor allowing 1 difference for every 100 bp of sequence: pre . cluster ( fasta = stability . trim . contigs . good . unique . good . filter . unique . fasta , count = stability . trim . contigs . good . unique . good . filter . count_table , diffs = 2 ) At this point we have removed as much sequencing error as we can and it is time to turn our attention to removing chimeras. We'll do this using the UCHIME algorithm that is called within mothur using the chimera.uchime command. Again, this command will split the data by sample and check for chimeras. Our preferred way of doing this is to use the abundant sequences as our reference. In addition, if a sequence is flagged as chimeric in one sample, the the default (dereplicate=F) is to remove it from all samples. Our experience suggests that this is a bit aggressive since we've seen rare sequences get flagged as chimeric when they're the most abundant sequence in another sample. This is how we do it: chimera . uchime ( fasta = stability . trim . contigs . good . unique . good . filter . unique . precluster . fasta , count = stability . trim . contigs . good . unique . good . filter . unique . precluster . count_table , dereplicate = t ) Running chimera.uchime with the count file will remove the chimeric sequences from the count file. But you still need to remove those sequences from the fasta file. We do this using remove.seqs : remove . seqs ( fasta = stability . trim . contigs . good . unique . good . filter . unique . precluster . fasta , accnos = stability . trim . contigs . good . unique . good . filter . unique . precluster . denovo . uchime . accnos ) As a final quality control step, we need to see if there are any \"undesirables\" in our dataset. Sometimes when we pick a primer set they will amplify other stuff that gets to this point in the pipeline such as 18S rRNA gene fragments or 16S rRNA from Archaea, chloroplasts, and mitochondira. There's also just the random stuff that we want to get rid of. Let's go ahead and classify those sequences using the Bayesian classifier with the classify.seqs command: classify . seqs ( fasta = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . fasta , count = stability . trim . contigs . good . unique . good . filter . unique . precluster . denovo . uchime . pick . count_table , reference = silva . bacteria / silva . bacteria . fasta , taxonomy = silva . bacteria / silva . bacteria . rdp . tax , cutoff = 80 ) Now that everything is classified we want to remove our undesirables. We do this with the remove.lineage command: remove . lineage ( fasta = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . fasta , count = stability . trim . contigs . good . unique . good . filter . unique . precluster . denovo . uchime . pick . count_table , taxonomy = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . rdp . wang . taxonomy , taxon = Chloroplast - Mitochondria - unknown - Archaea - Eukaryota ) Analysis \u00b6 OTUs \u00b6 We will use cluster.split for clustering sequences into OTUs cluster . split ( fasta = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . pick . fasta , count = stability . trim . contigs . good . unique . good . filter . unique . precluster . denovo . uchime . pick . pick . count_table , taxonomy = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . rdp . wang . pick . taxonomy , splitmethod = classify , taxlevel = 4 , cutoff = 0 . 15 ) We used taxlevel=4 , which corresponds to the level of Order Next we want to know how many sequences are in each OTU from each group and we can do this using the make.shared command . Here we tell mothur that we're really only interested in the 0.03 cutoff level: make . shared ( list = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . pick . an . unique_list . list , count = stability . trim . contigs . good . unique . good . filter . unique . precluster . denovo . uchime . pick . pick . count_table , label = 0 . 03 ) We also want to know the taxonomy for each of our OTUs. We can get the consensus taxonomy for each OTU using the classify.otu command classify . otu ( list = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . pick . an . unique_list . list , count = stability . trim . contigs . good . unique . good . filter . unique . precluster . denovo . uchime . pick . pick . count_table , taxonomy = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . rdp . wang . pick . taxonomy , label = 0.03 ) If you open the file stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.an.unique_list.0.03.cons.taxonomy , you can get information about your OTUs. OTU Size Taxonomy Otu0001 12328 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Porphyromonadaceae ( 100 ); Barnesiella ( 100 ); Barnesiella_unclassified ( 100 ); Otu0002 8918 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Porphyromonadaceae ( 100 ); Barnesiella ( 100 ); Barnesiella_unclassified ( 100 ); Otu0003 7850 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Porphyromonadaceae ( 100 ); Barnesiella ( 100 ); Barnesiella_unclassified ( 100 ); Otu0004 7478 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Porphyromonadaceae ( 100 ); Barnesiella ( 100 ); Barnesiella_unclassified ( 100 ); Otu0005 7478 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Porphyromonadaceae ( 100 ); Barnesiella ( 100 ); Barnesiella_unclassified ( 100 ); Otu0006 6650 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Porphyromonadaceae ( 100 ); Barnesiella ( 100 ); Barnesiella_unclassified ( 100 ); Otu0007 6341 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Bacteroidaceae ( 100 ); Bacteroides ( 100 ); Bacteroides_unclassified ( 100 ); Otu0008 5374 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Rikenellaceae ( 100 ); Alistipes ( 100 ); Alistipes_unclassified ( 100 ); Otu0009 3618 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Porphyromonadaceae ( 100 ); Barnesiella ( 100 ); Barnesiella_unclassified ( 100 ); This is telling you that Otu0001 was observed 12328 times in your sample and that 100% of the sequences were from Barnesiella In order to vizualise the composition of our datasets, we'll use phyloseq, a R package to work with microbiom data. Most of the phyloseq functionalities require aand a tree file. We need to generate it with mothur: dist . seqs ( fasta = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . pick . fasta , output = lt , processors = 8 ) clearcut ( phylip = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . pick . phylip . dist ) Batch Mode \u00b6 It is perfectly acceptable to enter the commands for your analysis from within mothur. We call this the interactive mode. If you are doing a lot these types of analysis or you want to use this SOP on your own data without thinking too much, you can run mothur in batch mode using ./mothur script.batch where script.batch (or whatever name you want, really) is a text file containing all the commands that you previously entered in interactive mode. If you have time, copy all the commands from this tutorial in a file, a try to make mothur work in batch mode! PhyloSeq Analysis \u00b6 First, install and load the phyloseq package: source ( 'http://bioconductor.org/biocLite.R' ) biocLite ( 'phyloseq' ) library ( \"phyloseq\" ) library ( \"ggplot2\" ) library ( \"plyr\" ) theme_set ( theme_bw ()) # set the ggplot theme The PhyloSeq package has an import_mothur function that you can use to import the files you generated with mothur. As an example, import the example mothur data provided by phyloseq as an example: mothlist <- system.file ( \"extdata\" , \"esophagus.fn.list.gz\" , package = \"phyloseq\" ) mothgroup <- system.file ( \"extdata\" , \"esophagus.good.groups.gz\" , package = \"phyloseq\" ) mothtree <- system.file ( \"extdata\" , \"esophagus.tree.gz\" , package = \"phyloseq\" ) show_mothur_cutoffs ( mothlist ) cutoff <- '0.10' x <- import_mothur ( mothlist , mothgroup , mothtree , cutoff ) x Note: If if you ever work with 16s data and decide to use QIIME instead of mothur, phyloseq also has an import_qiime function. Also, newer version of qiime and mothur have the ability to produce a .biom file. \u201cThe biom file format (canonically pronounced \u2018biome\u2019) is designed to be a general-use format for representing counts of observations in one or more biological samples. BIOM is a recognized standard for the Earth Microbiome Project and is a Genomics Standards Consortium candidate project.\u201d More info on http://biom-format.org/ For the rest of this tutorial, we will work with an example dataset provided by the phyloseq package. Load the data with the following command: data ( enterotype ) data ( \"GlobalPatterns\" ) Ordination and distance-based analysis \u00b6 Let's do some preliminary filtering. Remove the OTUs that included all unassigned sequences (\"-1\") enterotype <- subset_species ( enterotype , Genus != \"-1\" ) The available distance methods coded in the phyloseq package: dist_methods <- unlist ( distanceMethodList ) print ( dist_methods ) ## UniFrac1 UniFrac2 DPCoA JSD vegdist1 ## \"unifrac\" \"wunifrac\" \"dpcoa\" \"jsd\" \"manhattan\" ## vegdist2 vegdist3 vegdist4 vegdist5 vegdist6 ## \"euclidean\" \"canberra\" \"bray\" \"kulczynski\" \"jaccard\" ## vegdist7 vegdist8 vegdist9 vegdist10 vegdist11 ## \"gower\" \"altGower\" \"morisita\" \"horn\" \"mountford\" ## vegdist12 vegdist13 vegdist14 vegdist15 betadiver1 ## \"raup\" \"binomial\" \"chao\" \"cao\" \"w\" ## betadiver2 betadiver3 betadiver4 betadiver5 betadiver6 ## \"-1\" \"c\" \"wb\" \"r\" \"I\" ## betadiver7 betadiver8 betadiver9 betadiver10 betadiver11 ## \"e\" \"t\" \"me\" \"j\" \"sor\" ## betadiver12 betadiver13 betadiver14 betadiver15 betadiver16 ## \"m\" \"-2\" \"co\" \"cc\" \"g\" ## betadiver17 betadiver18 betadiver19 betadiver20 betadiver21 ## \"-3\" \"l\" \"19\" \"hk\" \"rlb\" ## betadiver22 betadiver23 betadiver24 dist1 dist2 ## \"sim\" \"gl\" \"z\" \"maximum\" \"binary\" ## dist3 designdist ## \"minkowski\" \"ANY\" Remove the two distance-methods that require a tree, and the generic custom method that requires user-defined distance arguments. # These require tree dist_methods [ ( 1 : 3 ) ] # Remove them from the vector dist_methods <- dist_methods[ - ( 1 : 3 ) ] # This is the user-defined method: dist_methods[ \"designdist\" ] # Remove the user-defined distance dist_methods = dist_methods[ - which ( dist_methods == \"ANY\" ) ] Loop through each distance method, save each plot to a list, called plist. plist <- vector ( \"list\" , length ( dist_methods )) names ( plist ) = dist_methods for ( i in dist_methods ){ # Calculate distance matrix iDist <- distance ( enterotype , method = i ) # Calculate ordination iMDS <- ordinate ( enterotype , \"MDS\" , distance = iDist ) ## Make plot # Don't carry over previous plot (if error, p will be blank) p <- NULL # Create plot, store as temp variable, p p <- plot_ordination ( enterotype , iMDS , color = \"SeqTech\" , shape = \"Enterotype\" ) # Add title to each plot p <- p + ggtitle ( paste ( \"MDS using distance method \" , i , sep = \"\" )) # Save the graphic to file. plist[[i]] = p } Combine results and shade according to Sequencing technology: df = ldply ( plist , function ( x ) x $ data ) names ( df ) [1] <- \"distance\" p = ggplot ( df , aes ( Axis.1 , Axis.2 , color = SeqTech , shape = Enterotype )) p = p + geom_point ( size = 3 , alpha = 0.5 ) p = p + facet_wrap ( ~ distance , scales = \"free\" ) p = p + ggtitle ( \"MDS on various distance metrics for Enterotype dataset\" ) p Print individual plots: print ( plist[[ \"jsd\" ]] ) print ( plist[[ \"jaccard\" ]] ) print ( plist[[ \"bray\" ]] ) print ( plist[[ \"euclidean\" ]] ) Alpha diversity graphics \u00b6 Here is the default graphic produced by the plot_richness function on the GP example dataset: GP <- prune_species ( speciesSums ( GlobalPatterns ) > 0 , GlobalPatterns ) plot_richness ( GP ) Note that in this case, the Fisher calculation results in a warning (but still plots). We can avoid this by specifying a measures argument to plot_richness, which will include just the alpha-diversity measures that we want. plot_richness ( GP , measures = c ( \"Chao1\" , \"Shannon\" )) We can specify a sample variable on which to group/organize samples along the horizontal (x) axis. An experimentally meaningful categorical variable is usually a good choice \u2013 in this case, the \"SampleType\" variable works much better than attempting to interpret the sample names directly (as in the previous plot): plot_richness ( GP , x = \"SampleType\" , measures = c ( \"Chao1\" , \"Shannon\" )) Now suppose we wanted to use an external variable in the plot that isn\u2019t in the GP dataset already \u2013 for example, a logical that indicated whether or not the samples are human-associated. First, define this new variable, human, as a factor (other vectors could also work; or other data you might have describing the samples). sampleData ( GP ) $ human <- getVariable ( GP , \"SampleType\" ) %in% c ( \"Feces\" , \"Mock\" , \"Skin\" , \"Tongue\" ) Now tell plot_richness to map the new human variable on the horizontal axis, and shade the points in different color groups, according to which \"SampleType\" they belong. plot_richness ( GP , x = \"human\" , color = \"SampleType\" , measures = c ( \"Chao1\" , \"Shannon\" )) We can merge samples that are from the environment (SampleType), and make the points bigger with a ggplot2 layer. First, merge the samples. GPst = merge_samples ( GP , \"SampleType\" ) # repair variables that were damaged during merge (coerced to numeric) sample_data ( GPst ) $ SampleType <- factor ( sample_names ( GPst )) sample_data ( GPst ) $ human <- as.logical ( sample_data ( GPst ) $ human ) p = plot_richness ( GPst , x = \"human\" , color = \"SampleType\" , measures = c ( \"Chao1\" , \"Shannon\" )) p + geom_point ( size = 5 , alpha = 0.7 ) Trees \u00b6 head ( phy_tree ( GlobalPatterns ) $ node.label , 10 ) The node data from the GlobalPatterns dataset are strange. They look like they might be bootstrap values, but they sometimes have two decimals. phy_tree ( GlobalPatterns ) $ node.label = substr ( phy_tree ( GlobalPatterns ) $ node.label , 1 , 4 ) Additionally, the dataset has many OTUs, too many to fit them all on a tree. Let's take the 50 more abundant and plot a basic tree: physeq = prune_taxa ( taxa_names ( GlobalPatterns ) [1 : 50 ] , GlobalPatterns ) plot_tree ( physeq ) dots are annotated next to tips (OTUs) in the tree, one for each sample in which that OTU was observed. Let's color the dots by taxonomic ranks, and sample covariates: plot_tree ( physeq , nodelabf = nodeplotboot (), ladderize = \"left\" , color = \"SampleType\" ) by taxonomic class: plot_tree ( physeq , nodelabf = nodeplotboot (), ladderize = \"left\" , color = \"Class\" ) It can be useful to label the tips: plot_tree ( physeq , color = \"SampleType\" , label . tips = \"Genus\" ) Making a radial tree is easy with ggplot2, simply recognizing that our vertically-oriented tree is a cartesian mapping of the data to a graphic \u2013 and that a radial tree is the same mapping, but with polar coordinates instead. plot_tree ( physeq , nodelabf = nodeplotboot ( 60 , 60 , 3 ), color = \"SampleType\" , shape = \"Class\" , ladderize = \"left\" ) + coord_polar ( theta = \"y\" ) Bar plots \u00b6 Bar plots are one of the easiest way to vizualize your data. But be careful, they can be misleading if grouping sample! Let's take a subset of the GlobalPatterns dataset, and produce a basic bar plot: gp.ch = subset_taxa ( GlobalPatterns , Phylum == \"Chlamydiae\" ) plot_bar ( gp.ch ) The dataset is plotted with every sample mapped individually to the horizontal (x) axis, and abundance values mapped to the veritcal (y) axis. At each sample\u2019s horizontal position, the abundance values for each OTU are stacked in order from greatest to least, separate by a thin horizontal line. As long as the parameters you choose to separate the data result in more than one OTU abundance value at the respective position in the plot, the values will be stacked in order as a means of displaying both the sum total value while still representing the individual OTU abundances. The bar plot will be clearer with color to represent the Genus to which each OTU belongs. plot_bar ( gp.ch , fill = \"Genus\" ) Now keep the same fill color, and group the samples together by the SampleType variable; essentially, the environment from which the sample was taken and sequenced. plot_bar ( gp.ch , x = \"SampleType\" , fill = \"Genus\" ) A more complex example using facets: plot_bar ( gp.ch , \"Family\" , fill = \"Genus\" , facet_grid =~ SampleType ) Heatmaps \u00b6 The following two lines subset the dataset to just the top 300 most abundant Bacteria taxa across all samples (in this case, with no prior preprocessing. Not recommended, but quick). data ( \"GlobalPatterns\" ) gpt <- subset_taxa ( GlobalPatterns , Kingdom == \"Bacteria\" ) gpt <- prune_taxa ( names ( sort ( taxa_sums ( gpt ), TRUE ) [1 : 300 ] ), gpt ) plot_heatmap ( gpt , sample.label = \"SampleType\" ) subset a smaller dataset based on an Archaeal phylum gpac <- subset_taxa ( GlobalPatterns , Phylum == \"Crenarchaeota\" ) plot_heatmap ( gpac ) Plot microbiome network \u00b6 There is a random aspect to some of the network layout methods. For complete reproducibility of the images produced later in this tutorial, it is possible to set the random number generator seed explicitly: set.seed(711L) Because we want to use the enterotype designations as a plot feature in these plots, we need to remove the 9 samples for which no enterotype designation was assigned (this will save us the hassle of some pesky warning messages, but everything still works; the offending samples are anyway omitted). enterotype = subset_samples ( enterotype , ! is.na ( Enterotype )) Create an igraph-based network based on the default distance method, \u201cJaccard\u201d, and a maximum distance between connected nodes of 0.3. ig <- make_network ( enterotype , max.dist = 0.3 ) plot_network ( ig , enterotype ) The previous graphic displayed some interesting structure, with one or two major subgraphs comprising a majority of samples. Furthermore, there seemed to be a correlation in the sample naming scheme and position within the network. Instead of trying to read all of the sample names to understand the pattern, let\u2019s map some of the sample variables onto this graphic as color and shape: plot_network ( ig , enterotype , color = \"SeqTech\" , shape = \"Enterotype\" , line_weight = 0.4 , label = NULL ) In the previous examples, the choice of maximum-distance and distance method were informed, but arbitrary. Let\u2019s see what happens when the maximum distance is lowered, decreasing the number of edges in the network ig <- make_network ( enterotype , max.dist = 0.2 ) plot_network ( ig , enterotype , color = \"SeqTech\" , shape = \"Enterotype\" , line_weight = 0.4 , label = NULL ) Let\u2019s repeat the previous exercise, but replace the Jaccard (default) distance method with Bray-Curtis ig <- make_network ( enterotype , dist.fun = \"bray\" , max.dist = 0.3 ) plot_network ( ig , enterotype , color = \"SeqTech\" , shape = \"Enterotype\" , line_weight = 0.4 , label = NULL )","title":"Metabarcoding"},{"location":"16S_mothur/#metabarcoding","text":"This tutorial is largely inspired of the MiSeq SOP from the Schloss Lab. Kozich JJ, Westcott SL, Baxter NT, Highlander SK, Schloss PD. (2013): Development of a dual-index sequencing strategy and curation pipeline for analyzing amplicon sequence data on the MiSeq Illumina sequencing platform. Applied and Environmental Microbiology. 79(17):5112-20.","title":"Metabarcoding"},{"location":"16S_mothur/#table-of-contents","text":"Introduction Softwares Required for this Tutorial Downloading the Data and Start Mothur Reducing Sequencing and PCR Errors Processing Improved Sequences Analysis OTUs Batch Mode","title":"Table of Contents"},{"location":"16S_mothur/#introduction","text":"The 16S rRNA gene is a section of prokaryotic DNA found in all bacteria and archaea. This gene codes for an rRNA, and this rRNA in turn makes up part of the ribosome. The first 'r' in rRNA stands for ribosomal. The ribosome is composed of two subunits, the large subunit (LSU) and the small subunit (SSU). The 16S rRNA gene is a commonly used tool for identifying bacteria for several reasons. First, traditional characterization depended upon phenotypic traits like gram positive or gram negative, bacillus or coccus, etc. Taxonomists today consider analysis of an organism's DNA more reliable than classification based solely on phenotypes. Secondly, researchers may, for a number of reasons, want to identify or classify only the bacteria within a given environmental or medical sample. While there is a homologous gene in eukaryotes, the 18S rRNA gene, it is distinct, thereby rendering the 16S rRNA gene a useful tool for extracting and identifying bacteria as separate from plant, animal, fungal, and protist DNA within the same sample. Thirdly, the 16S rRNA gene is relatively short at 1.5 kb, making it faster and cheaper to sequence than many other unique bacterial genes. Mothur is a command-line computer program for analyzing sequence data from microbial communities and namely 16s data. mothur is licensed under the GPL and is free to use.","title":"Introduction"},{"location":"16S_mothur/#softwares-required-for-this-tutorial","text":"mothur mothur_krona","title":"Softwares Required for this Tutorial"},{"location":"16S_mothur/#downloading-the-data-and-start-mothur","text":"Firstly, download and unzip the sample dataset: wget http : // www . mothur . org / w / images / d / d6 / MiSeqSOPData . zip unzip MiSeqSOPData . zip In the MiSeq_SOP directory, you'll find the reads files in fastq format, as well as a file called stability.files The first lines of stability.files look like this: F3D0 F3D0_S188_L001_R1_001.fastq F3D0_S188_L001_R2_001.fastq F3D141 F3D141_S207_L001_R1_001.fastq F3D141_S207_L001_R2_001.fastq F3D142 F3D142_S208_L001_R1_001.fastq F3D142_S208_L001_R2_001.fastq F3D143 F3D143_S209_L001_R1_001.fastq F3D143_S209_L001_R2_001.fastq The first column is the name of the sample. The second column is the name of the forward read for that sample and the third columns in the name of the reverse read for that sample. Now it's time to start mothur. Type mothur in your terminal. You should see your prompt changing to mothur >","title":"Downloading the Data and Start Mothur"},{"location":"16S_mothur/#reducing-sequencing-and-pcr-errors","text":"The first thing we want to do is combine our two sets of reads for each sample and then to combine the data from all of the samples. This is done using the make.contigs command, which requires stability.files as input. This command will extract the sequence and quality score data from your fastq files, create the reverse complement of the reverse read and then join the reads into contigs. make . contigs ( file = stability . files , processors = 8 ) It took 30 secs to process 152360 sequences . Group count : F3D0 7793 F3D1 5869 F3D141 5958 F3D142 3183 F3D143 3178 F3D144 4827 F3D145 7377 F3D146 5021 F3D147 17070 F3D148 12405 F3D149 13083 F3D150 5509 F3D2 19620 F3D3 6758 F3D5 4448 F3D6 7989 F3D7 5129 F3D8 5294 F3D9 7070 Mock 4779 Total of all groups is 152360 Output File Names : stability . trim . contigs . fasta stability . trim . contigs . qual stability . contigs . report stability . scrap . contigs . fasta stability . scrap . contigs . qual stability . contigs . groups The stability.contigs.report file will tell you something about the contig assembly for each read. Let's see what these sequences look like using the summary.seqs command: summary . seqs ( fasta = stability . trim . contigs . fasta ) Start End NBases Ambigs Polymer NumSeqs Minimum : 1 248 248 0 3 1 2 . 5 %- tile : 1 252 252 0 3 3810 25 %- tile : 1 252 252 0 4 38091 Median : 1 252 252 0 4 76181 75 %- tile : 1 253 253 0 5 114271 97 . 5 %- tile : 1 253 253 6 6 148552 Maximum : 1 502 502 249 243 152360 Mean : 1 252 . 811 252 . 811 0 . 70063 4 . 44854 # of Seqs : 152360 This tells us that we have 152360 sequences that for the most part vary between 248 and 253 bases. Interestingly, the longest read in the dataset is 502 bp. Be suspicious of this, the reads are supposed to be 251 bp each. This read clearly didn't assemble well (or at all). Also, note that at least 2.5% of our sequences had some ambiguous base calls. We'll take care of these issues in the next step when we run screen.seqs . screen.seqs(fasta=stability.trim.contigs.fasta, group=stability.contigs.groups, maxambig=0, maxlength=275) You'll notice that mothur remembered that we used 8 processors in make.contigs . To see what else mothur knows about you, run the following: get . current () Current files saved by mothur : fasta = stability . trim . contigs . good . fasta group = stability . contigs . good . groups qfile = stability . trim . contigs . qual processors = 8 summary = stability . trim . contigs . summary What this means is that mothur remembers your latest fasta file and group file as well as the number of processors you have. So you could run: mothur > summary . seqs ( fasta = stability . trim . contigs . good . fasta ) mothur > summary . seqs ( fasta = current ) mothur > summary . seqs () and get the same output for each command. But, now that we have filtered the sequencing errors, let's move to the next step.","title":"Reducing Sequencing and PCR Errors"},{"location":"16S_mothur/#processing-improved-sequences","text":"We anticipate that many of our sequences are duplicates of each other. Because it's computationally wasteful to align the same sequences several times, we'll make our sequences unique: unique . seqs ( fasta = stability . trim . contigs . good . fasta ) If two sequences have the same identical sequence, then they're considered duplicates and will get merged. In the screen output there are two columns - the first is the number of sequences characterized and the second is the number of unique sequences remaining Another thing to do to make our lives easier is to simplify the names and group files. If you look at the most recent versions of those files you'll see together they are 13 MB. This may not seem like much, but with a full MiSeq run those long sequence names can add up and make life tedious. So we'll run count.seqs to generate a table where the rows are the names of the unique seqeunces and the columns are the names of the groups. The table is then filled with the number of times each unique sequence shows up in each group. This will generate a file called stability.trim.contigs.good.count_table. In subsequent commands we'll use it by using the count option: count . seqs ( name = stability . trim . contigs . good . names , group = stability . contigs . good . groups ) summary . seqs ( count = stability . trim . contigs . good . count_table ) Using stability . trim . contigs . good . unique . fasta as input file for the fasta parameter . Using 8 processors . Start End NBases Ambigs Polymer NumSeqs Minimum : 1 250 250 0 3 1 2 . 5 %- tile : 1 252 252 0 3 3227 25 %- tile : 1 252 252 0 4 32265 Median : 1 252 252 0 4 64530 75 %- tile : 1 253 253 0 5 96794 97 . 5 %- tile : 1 253 253 0 6 125832 Maximum : 1 270 270 0 12 129058 Mean : 1 252 . 462 252 . 462 0 4 . 36663 # of unique seqs : 16477 total # of seqs : 129058 Now we need to align our sequences to the reference alignment. First we need to download the SILVA database. # This step should be done outside mothur wget http : // www . mothur . org / w / images / 9 / 98 / Silva . bacteria . zip unzip Silva . bacteria . zip If you have quit mothur to download the database, rerun the mothur command, then take a look at the database you have downloaded: summary . seqs ( fasta = silva . bacteria / silva . bacteria . fasta , processors = 8 ) Now do the alignment using align.seqs : align . seqs ( fasta = stability . trim . contigs . good . unique . fasta , reference = silva . bacteria / silva . bacteria . fasta ) We can then run summary.seqs again to get a summary of our alignment: summary . seqs ( fasta = stability . trim . contigs . good . unique . align , count = stability . trim . contigs . good . count_table ) You'll see that the bulk of the sequences start at position 13862 and end at position 23444. Some sequences start at position 13144 or 13876 and end at 22587 or 25294. These deviants from the mode positions are likely due to an insertion or deletion at the terminal ends of the aliignments. Sometimes you'll see sequences that start and end at the same position indicating a very poor alignment, which is generally due to non-specific amplification. To make sure that everything overlaps the same region we'll re-run screen.seqs to get sequences that start at or before position 1968 and end at or after position 11550. We'll also set the maximum homopolymer length to 8 since there's nothing in the database with a stretch of 9 or more of the same base in a row (this really could have been done in the first execution of screen.seqs above). Note that we need the count table so that we can update the table for the sequences we're removing and we're also using the summary file so we don't have to figure out again all the start and stop positions: screen . seqs ( fasta = stability . trim . contigs . good . unique . align , count = stability . trim . contigs . good . count_table , summary = stability . trim . contigs . good . unique . summary , start = 13862 , end = 23444 , maxhomop = 8 ) summary . seqs ( fasta = current , count = current ) No we can trim both ends of the aligned reads to be sure the all overlap exactly the same region. We can do this with fliter.seqs filter . seqs ( fasta = stability . trim . contigs . good . unique . good . align , vertical = T , trump = .) We may have introduced redundancy by trimming the ends of the sequences, so we will re-run unique.seqs unique . seqs ( fasta = stability . trim . contigs . good . unique . good . filter . fasta , count = stability . trim . contigs . good . good . count_table ) This identified 3 duplicate sequences that we've now merged with previous unique sequences. The next thing we want to do to further de-noise our sequences is to pre-cluster the sequences using the pre.cluster command allowing for up to 2 differences between sequences. This command will split the sequences by group and then sort them by abundance and go from most abundant to least and identify sequences that are within 2 nt of each other. If they are then they get merged. We generally favor allowing 1 difference for every 100 bp of sequence: pre . cluster ( fasta = stability . trim . contigs . good . unique . good . filter . unique . fasta , count = stability . trim . contigs . good . unique . good . filter . count_table , diffs = 2 ) At this point we have removed as much sequencing error as we can and it is time to turn our attention to removing chimeras. We'll do this using the UCHIME algorithm that is called within mothur using the chimera.uchime command. Again, this command will split the data by sample and check for chimeras. Our preferred way of doing this is to use the abundant sequences as our reference. In addition, if a sequence is flagged as chimeric in one sample, the the default (dereplicate=F) is to remove it from all samples. Our experience suggests that this is a bit aggressive since we've seen rare sequences get flagged as chimeric when they're the most abundant sequence in another sample. This is how we do it: chimera . uchime ( fasta = stability . trim . contigs . good . unique . good . filter . unique . precluster . fasta , count = stability . trim . contigs . good . unique . good . filter . unique . precluster . count_table , dereplicate = t ) Running chimera.uchime with the count file will remove the chimeric sequences from the count file. But you still need to remove those sequences from the fasta file. We do this using remove.seqs : remove . seqs ( fasta = stability . trim . contigs . good . unique . good . filter . unique . precluster . fasta , accnos = stability . trim . contigs . good . unique . good . filter . unique . precluster . denovo . uchime . accnos ) As a final quality control step, we need to see if there are any \"undesirables\" in our dataset. Sometimes when we pick a primer set they will amplify other stuff that gets to this point in the pipeline such as 18S rRNA gene fragments or 16S rRNA from Archaea, chloroplasts, and mitochondira. There's also just the random stuff that we want to get rid of. Let's go ahead and classify those sequences using the Bayesian classifier with the classify.seqs command: classify . seqs ( fasta = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . fasta , count = stability . trim . contigs . good . unique . good . filter . unique . precluster . denovo . uchime . pick . count_table , reference = silva . bacteria / silva . bacteria . fasta , taxonomy = silva . bacteria / silva . bacteria . rdp . tax , cutoff = 80 ) Now that everything is classified we want to remove our undesirables. We do this with the remove.lineage command: remove . lineage ( fasta = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . fasta , count = stability . trim . contigs . good . unique . good . filter . unique . precluster . denovo . uchime . pick . count_table , taxonomy = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . rdp . wang . taxonomy , taxon = Chloroplast - Mitochondria - unknown - Archaea - Eukaryota )","title":"Processing Improved Sequences"},{"location":"16S_mothur/#analysis","text":"","title":"Analysis"},{"location":"16S_mothur/#otus","text":"We will use cluster.split for clustering sequences into OTUs cluster . split ( fasta = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . pick . fasta , count = stability . trim . contigs . good . unique . good . filter . unique . precluster . denovo . uchime . pick . pick . count_table , taxonomy = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . rdp . wang . pick . taxonomy , splitmethod = classify , taxlevel = 4 , cutoff = 0 . 15 ) We used taxlevel=4 , which corresponds to the level of Order Next we want to know how many sequences are in each OTU from each group and we can do this using the make.shared command . Here we tell mothur that we're really only interested in the 0.03 cutoff level: make . shared ( list = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . pick . an . unique_list . list , count = stability . trim . contigs . good . unique . good . filter . unique . precluster . denovo . uchime . pick . pick . count_table , label = 0 . 03 ) We also want to know the taxonomy for each of our OTUs. We can get the consensus taxonomy for each OTU using the classify.otu command classify . otu ( list = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . pick . an . unique_list . list , count = stability . trim . contigs . good . unique . good . filter . unique . precluster . denovo . uchime . pick . pick . count_table , taxonomy = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . rdp . wang . pick . taxonomy , label = 0.03 ) If you open the file stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.an.unique_list.0.03.cons.taxonomy , you can get information about your OTUs. OTU Size Taxonomy Otu0001 12328 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Porphyromonadaceae ( 100 ); Barnesiella ( 100 ); Barnesiella_unclassified ( 100 ); Otu0002 8918 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Porphyromonadaceae ( 100 ); Barnesiella ( 100 ); Barnesiella_unclassified ( 100 ); Otu0003 7850 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Porphyromonadaceae ( 100 ); Barnesiella ( 100 ); Barnesiella_unclassified ( 100 ); Otu0004 7478 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Porphyromonadaceae ( 100 ); Barnesiella ( 100 ); Barnesiella_unclassified ( 100 ); Otu0005 7478 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Porphyromonadaceae ( 100 ); Barnesiella ( 100 ); Barnesiella_unclassified ( 100 ); Otu0006 6650 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Porphyromonadaceae ( 100 ); Barnesiella ( 100 ); Barnesiella_unclassified ( 100 ); Otu0007 6341 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Bacteroidaceae ( 100 ); Bacteroides ( 100 ); Bacteroides_unclassified ( 100 ); Otu0008 5374 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Rikenellaceae ( 100 ); Alistipes ( 100 ); Alistipes_unclassified ( 100 ); Otu0009 3618 Bacteria ( 100 ); Bacteroidetes ( 100 ); Bacteroidia ( 100 ); Bacteroidales ( 100 ); Porphyromonadaceae ( 100 ); Barnesiella ( 100 ); Barnesiella_unclassified ( 100 ); This is telling you that Otu0001 was observed 12328 times in your sample and that 100% of the sequences were from Barnesiella In order to vizualise the composition of our datasets, we'll use phyloseq, a R package to work with microbiom data. Most of the phyloseq functionalities require aand a tree file. We need to generate it with mothur: dist . seqs ( fasta = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . pick . fasta , output = lt , processors = 8 ) clearcut ( phylip = stability . trim . contigs . good . unique . good . filter . unique . precluster . pick . pick . phylip . dist )","title":"OTUs"},{"location":"16S_mothur/#batch-mode","text":"It is perfectly acceptable to enter the commands for your analysis from within mothur. We call this the interactive mode. If you are doing a lot these types of analysis or you want to use this SOP on your own data without thinking too much, you can run mothur in batch mode using ./mothur script.batch where script.batch (or whatever name you want, really) is a text file containing all the commands that you previously entered in interactive mode. If you have time, copy all the commands from this tutorial in a file, a try to make mothur work in batch mode!","title":"Batch Mode"},{"location":"16S_mothur/#phyloseq-analysis","text":"First, install and load the phyloseq package: source ( 'http://bioconductor.org/biocLite.R' ) biocLite ( 'phyloseq' ) library ( \"phyloseq\" ) library ( \"ggplot2\" ) library ( \"plyr\" ) theme_set ( theme_bw ()) # set the ggplot theme The PhyloSeq package has an import_mothur function that you can use to import the files you generated with mothur. As an example, import the example mothur data provided by phyloseq as an example: mothlist <- system.file ( \"extdata\" , \"esophagus.fn.list.gz\" , package = \"phyloseq\" ) mothgroup <- system.file ( \"extdata\" , \"esophagus.good.groups.gz\" , package = \"phyloseq\" ) mothtree <- system.file ( \"extdata\" , \"esophagus.tree.gz\" , package = \"phyloseq\" ) show_mothur_cutoffs ( mothlist ) cutoff <- '0.10' x <- import_mothur ( mothlist , mothgroup , mothtree , cutoff ) x Note: If if you ever work with 16s data and decide to use QIIME instead of mothur, phyloseq also has an import_qiime function. Also, newer version of qiime and mothur have the ability to produce a .biom file. \u201cThe biom file format (canonically pronounced \u2018biome\u2019) is designed to be a general-use format for representing counts of observations in one or more biological samples. BIOM is a recognized standard for the Earth Microbiome Project and is a Genomics Standards Consortium candidate project.\u201d More info on http://biom-format.org/ For the rest of this tutorial, we will work with an example dataset provided by the phyloseq package. Load the data with the following command: data ( enterotype ) data ( \"GlobalPatterns\" )","title":"PhyloSeq Analysis"},{"location":"16S_mothur/#ordination-and-distance-based-analysis","text":"Let's do some preliminary filtering. Remove the OTUs that included all unassigned sequences (\"-1\") enterotype <- subset_species ( enterotype , Genus != \"-1\" ) The available distance methods coded in the phyloseq package: dist_methods <- unlist ( distanceMethodList ) print ( dist_methods ) ## UniFrac1 UniFrac2 DPCoA JSD vegdist1 ## \"unifrac\" \"wunifrac\" \"dpcoa\" \"jsd\" \"manhattan\" ## vegdist2 vegdist3 vegdist4 vegdist5 vegdist6 ## \"euclidean\" \"canberra\" \"bray\" \"kulczynski\" \"jaccard\" ## vegdist7 vegdist8 vegdist9 vegdist10 vegdist11 ## \"gower\" \"altGower\" \"morisita\" \"horn\" \"mountford\" ## vegdist12 vegdist13 vegdist14 vegdist15 betadiver1 ## \"raup\" \"binomial\" \"chao\" \"cao\" \"w\" ## betadiver2 betadiver3 betadiver4 betadiver5 betadiver6 ## \"-1\" \"c\" \"wb\" \"r\" \"I\" ## betadiver7 betadiver8 betadiver9 betadiver10 betadiver11 ## \"e\" \"t\" \"me\" \"j\" \"sor\" ## betadiver12 betadiver13 betadiver14 betadiver15 betadiver16 ## \"m\" \"-2\" \"co\" \"cc\" \"g\" ## betadiver17 betadiver18 betadiver19 betadiver20 betadiver21 ## \"-3\" \"l\" \"19\" \"hk\" \"rlb\" ## betadiver22 betadiver23 betadiver24 dist1 dist2 ## \"sim\" \"gl\" \"z\" \"maximum\" \"binary\" ## dist3 designdist ## \"minkowski\" \"ANY\" Remove the two distance-methods that require a tree, and the generic custom method that requires user-defined distance arguments. # These require tree dist_methods [ ( 1 : 3 ) ] # Remove them from the vector dist_methods <- dist_methods[ - ( 1 : 3 ) ] # This is the user-defined method: dist_methods[ \"designdist\" ] # Remove the user-defined distance dist_methods = dist_methods[ - which ( dist_methods == \"ANY\" ) ] Loop through each distance method, save each plot to a list, called plist. plist <- vector ( \"list\" , length ( dist_methods )) names ( plist ) = dist_methods for ( i in dist_methods ){ # Calculate distance matrix iDist <- distance ( enterotype , method = i ) # Calculate ordination iMDS <- ordinate ( enterotype , \"MDS\" , distance = iDist ) ## Make plot # Don't carry over previous plot (if error, p will be blank) p <- NULL # Create plot, store as temp variable, p p <- plot_ordination ( enterotype , iMDS , color = \"SeqTech\" , shape = \"Enterotype\" ) # Add title to each plot p <- p + ggtitle ( paste ( \"MDS using distance method \" , i , sep = \"\" )) # Save the graphic to file. plist[[i]] = p } Combine results and shade according to Sequencing technology: df = ldply ( plist , function ( x ) x $ data ) names ( df ) [1] <- \"distance\" p = ggplot ( df , aes ( Axis.1 , Axis.2 , color = SeqTech , shape = Enterotype )) p = p + geom_point ( size = 3 , alpha = 0.5 ) p = p + facet_wrap ( ~ distance , scales = \"free\" ) p = p + ggtitle ( \"MDS on various distance metrics for Enterotype dataset\" ) p Print individual plots: print ( plist[[ \"jsd\" ]] ) print ( plist[[ \"jaccard\" ]] ) print ( plist[[ \"bray\" ]] ) print ( plist[[ \"euclidean\" ]] )","title":"Ordination and distance-based analysis"},{"location":"16S_mothur/#alpha-diversity-graphics","text":"Here is the default graphic produced by the plot_richness function on the GP example dataset: GP <- prune_species ( speciesSums ( GlobalPatterns ) > 0 , GlobalPatterns ) plot_richness ( GP ) Note that in this case, the Fisher calculation results in a warning (but still plots). We can avoid this by specifying a measures argument to plot_richness, which will include just the alpha-diversity measures that we want. plot_richness ( GP , measures = c ( \"Chao1\" , \"Shannon\" )) We can specify a sample variable on which to group/organize samples along the horizontal (x) axis. An experimentally meaningful categorical variable is usually a good choice \u2013 in this case, the \"SampleType\" variable works much better than attempting to interpret the sample names directly (as in the previous plot): plot_richness ( GP , x = \"SampleType\" , measures = c ( \"Chao1\" , \"Shannon\" )) Now suppose we wanted to use an external variable in the plot that isn\u2019t in the GP dataset already \u2013 for example, a logical that indicated whether or not the samples are human-associated. First, define this new variable, human, as a factor (other vectors could also work; or other data you might have describing the samples). sampleData ( GP ) $ human <- getVariable ( GP , \"SampleType\" ) %in% c ( \"Feces\" , \"Mock\" , \"Skin\" , \"Tongue\" ) Now tell plot_richness to map the new human variable on the horizontal axis, and shade the points in different color groups, according to which \"SampleType\" they belong. plot_richness ( GP , x = \"human\" , color = \"SampleType\" , measures = c ( \"Chao1\" , \"Shannon\" )) We can merge samples that are from the environment (SampleType), and make the points bigger with a ggplot2 layer. First, merge the samples. GPst = merge_samples ( GP , \"SampleType\" ) # repair variables that were damaged during merge (coerced to numeric) sample_data ( GPst ) $ SampleType <- factor ( sample_names ( GPst )) sample_data ( GPst ) $ human <- as.logical ( sample_data ( GPst ) $ human ) p = plot_richness ( GPst , x = \"human\" , color = \"SampleType\" , measures = c ( \"Chao1\" , \"Shannon\" )) p + geom_point ( size = 5 , alpha = 0.7 )","title":"Alpha diversity graphics"},{"location":"16S_mothur/#trees","text":"head ( phy_tree ( GlobalPatterns ) $ node.label , 10 ) The node data from the GlobalPatterns dataset are strange. They look like they might be bootstrap values, but they sometimes have two decimals. phy_tree ( GlobalPatterns ) $ node.label = substr ( phy_tree ( GlobalPatterns ) $ node.label , 1 , 4 ) Additionally, the dataset has many OTUs, too many to fit them all on a tree. Let's take the 50 more abundant and plot a basic tree: physeq = prune_taxa ( taxa_names ( GlobalPatterns ) [1 : 50 ] , GlobalPatterns ) plot_tree ( physeq ) dots are annotated next to tips (OTUs) in the tree, one for each sample in which that OTU was observed. Let's color the dots by taxonomic ranks, and sample covariates: plot_tree ( physeq , nodelabf = nodeplotboot (), ladderize = \"left\" , color = \"SampleType\" ) by taxonomic class: plot_tree ( physeq , nodelabf = nodeplotboot (), ladderize = \"left\" , color = \"Class\" ) It can be useful to label the tips: plot_tree ( physeq , color = \"SampleType\" , label . tips = \"Genus\" ) Making a radial tree is easy with ggplot2, simply recognizing that our vertically-oriented tree is a cartesian mapping of the data to a graphic \u2013 and that a radial tree is the same mapping, but with polar coordinates instead. plot_tree ( physeq , nodelabf = nodeplotboot ( 60 , 60 , 3 ), color = \"SampleType\" , shape = \"Class\" , ladderize = \"left\" ) + coord_polar ( theta = \"y\" )","title":"Trees"},{"location":"16S_mothur/#bar-plots","text":"Bar plots are one of the easiest way to vizualize your data. But be careful, they can be misleading if grouping sample! Let's take a subset of the GlobalPatterns dataset, and produce a basic bar plot: gp.ch = subset_taxa ( GlobalPatterns , Phylum == \"Chlamydiae\" ) plot_bar ( gp.ch ) The dataset is plotted with every sample mapped individually to the horizontal (x) axis, and abundance values mapped to the veritcal (y) axis. At each sample\u2019s horizontal position, the abundance values for each OTU are stacked in order from greatest to least, separate by a thin horizontal line. As long as the parameters you choose to separate the data result in more than one OTU abundance value at the respective position in the plot, the values will be stacked in order as a means of displaying both the sum total value while still representing the individual OTU abundances. The bar plot will be clearer with color to represent the Genus to which each OTU belongs. plot_bar ( gp.ch , fill = \"Genus\" ) Now keep the same fill color, and group the samples together by the SampleType variable; essentially, the environment from which the sample was taken and sequenced. plot_bar ( gp.ch , x = \"SampleType\" , fill = \"Genus\" ) A more complex example using facets: plot_bar ( gp.ch , \"Family\" , fill = \"Genus\" , facet_grid =~ SampleType )","title":"Bar plots"},{"location":"16S_mothur/#heatmaps","text":"The following two lines subset the dataset to just the top 300 most abundant Bacteria taxa across all samples (in this case, with no prior preprocessing. Not recommended, but quick). data ( \"GlobalPatterns\" ) gpt <- subset_taxa ( GlobalPatterns , Kingdom == \"Bacteria\" ) gpt <- prune_taxa ( names ( sort ( taxa_sums ( gpt ), TRUE ) [1 : 300 ] ), gpt ) plot_heatmap ( gpt , sample.label = \"SampleType\" ) subset a smaller dataset based on an Archaeal phylum gpac <- subset_taxa ( GlobalPatterns , Phylum == \"Crenarchaeota\" ) plot_heatmap ( gpac )","title":"Heatmaps"},{"location":"16S_mothur/#plot-microbiome-network","text":"There is a random aspect to some of the network layout methods. For complete reproducibility of the images produced later in this tutorial, it is possible to set the random number generator seed explicitly: set.seed(711L) Because we want to use the enterotype designations as a plot feature in these plots, we need to remove the 9 samples for which no enterotype designation was assigned (this will save us the hassle of some pesky warning messages, but everything still works; the offending samples are anyway omitted). enterotype = subset_samples ( enterotype , ! is.na ( Enterotype )) Create an igraph-based network based on the default distance method, \u201cJaccard\u201d, and a maximum distance between connected nodes of 0.3. ig <- make_network ( enterotype , max.dist = 0.3 ) plot_network ( ig , enterotype ) The previous graphic displayed some interesting structure, with one or two major subgraphs comprising a majority of samples. Furthermore, there seemed to be a correlation in the sample naming scheme and position within the network. Instead of trying to read all of the sample names to understand the pattern, let\u2019s map some of the sample variables onto this graphic as color and shape: plot_network ( ig , enterotype , color = \"SeqTech\" , shape = \"Enterotype\" , line_weight = 0.4 , label = NULL ) In the previous examples, the choice of maximum-distance and distance method were informed, but arbitrary. Let\u2019s see what happens when the maximum distance is lowered, decreasing the number of edges in the network ig <- make_network ( enterotype , max.dist = 0.2 ) plot_network ( ig , enterotype , color = \"SeqTech\" , shape = \"Enterotype\" , line_weight = 0.4 , label = NULL ) Let\u2019s repeat the previous exercise, but replace the Jaccard (default) distance method with Bray-Curtis ig <- make_network ( enterotype , dist.fun = \"bray\" , max.dist = 0.3 ) plot_network ( ig , enterotype , color = \"SeqTech\" , shape = \"Enterotype\" , line_weight = 0.4 , label = NULL )","title":"Plot microbiome network"},{"location":"annotation/","text":"Genome Annotation \u00b6 Lecture \u00b6 After you have de novo assembled your genome sequencing reads into contigs, it is useful to know what genomic features are on those contigs. The process of identifying and labelling those features is called genome annotation. Prokka is a \"wrapper\"; it collects together several pieces of software (from various authors), and so avoids \"re-inventing the wheel\". Prokka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using Prodigal ; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found here . Input data \u00b6 Prokka requires assembled contigs. You can prepare you working directory for this annotation tutorial. mkdir ~/annotation cd ~/annotation You will download an improved assembly of Mycoplasma genitalium into you data directory: curl -O -J -L https://osf.io/7eaky/download You will also need a proteins set specific of Mycoplasma for the annotation. Here is a file containing the Mycoplasma proteins retrieved from Swiss-Prot database (3041 sequences) curl -O -J -L https://osf.io/xjm3n/download Running prokka \u00b6 prokka --outdir annotation --kingdom Bacteria \\ --proteins uniprot_mycoplasma_reviewed.faa m_genetalium_improved.fasta Once Prokka has finished, examine each of its output files. The GFF and GBK files contain all of the information about the features annotated (in different formats.) The .txt file contains a summary of the number of features annotated. The .faa file contains the protein sequences of the genes annotated. The .ffn file contains the nucleotide sequences of the genes annotated. Visualising the annotation \u00b6 Artemis is a graphical Java program to browse annotated genomes. Download it here and install it on your local computer. Copy the .gff file produced by prokka on your computer, and open it with artemis. You will be overwhelmed and/or confused at first, and possibly permanently. Here are some tips: There are 3 panels: feature map (top), sequence (middle), feature list (bottom) Click right-mouse-button on bottom panel and select Show products Zooming is done via the verrtical scroll bars in the two top panels","title":"Genome Annotation"},{"location":"annotation/#genome-annotation","text":"","title":"Genome Annotation"},{"location":"annotation/#lecture","text":"After you have de novo assembled your genome sequencing reads into contigs, it is useful to know what genomic features are on those contigs. The process of identifying and labelling those features is called genome annotation. Prokka is a \"wrapper\"; it collects together several pieces of software (from various authors), and so avoids \"re-inventing the wheel\". Prokka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using Prodigal ; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found here .","title":"Lecture"},{"location":"annotation/#input-data","text":"Prokka requires assembled contigs. You can prepare you working directory for this annotation tutorial. mkdir ~/annotation cd ~/annotation You will download an improved assembly of Mycoplasma genitalium into you data directory: curl -O -J -L https://osf.io/7eaky/download You will also need a proteins set specific of Mycoplasma for the annotation. Here is a file containing the Mycoplasma proteins retrieved from Swiss-Prot database (3041 sequences) curl -O -J -L https://osf.io/xjm3n/download","title":"Input data"},{"location":"annotation/#running-prokka","text":"prokka --outdir annotation --kingdom Bacteria \\ --proteins uniprot_mycoplasma_reviewed.faa m_genetalium_improved.fasta Once Prokka has finished, examine each of its output files. The GFF and GBK files contain all of the information about the features annotated (in different formats.) The .txt file contains a summary of the number of features annotated. The .faa file contains the protein sequences of the genes annotated. The .ffn file contains the nucleotide sequences of the genes annotated.","title":"Running prokka"},{"location":"annotation/#visualising-the-annotation","text":"Artemis is a graphical Java program to browse annotated genomes. Download it here and install it on your local computer. Copy the .gff file produced by prokka on your computer, and open it with artemis. You will be overwhelmed and/or confused at first, and possibly permanently. Here are some tips: There are 3 panels: feature map (top), sequence (middle), feature list (bottom) Click right-mouse-button on bottom panel and select Show products Zooming is done via the verrtical scroll bars in the two top panels","title":"Visualising the annotation"},{"location":"assembly/","text":"De-novo Genome Assembly \u00b6 Lecture \u00b6 Practical \u00b6 In this practical we will perform the assembly of M. genitalium , a bacterium published in 1995 by Fraser et al in Science ( abstract link ). Getting the data \u00b6 M. genitalium was sequenced using the MiSeq platform (2 * 150bp). The reads were deposited in the ENA Short Read Archive under the accession ERR486840 Download the 2 fastq files associated with the run. wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_1.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_2.fastq.gz The files that were deposited in ENA were already trimmed, so we do not have to trim ourselves! Question How many reads are in the files? De-novo assembly \u00b6 We will be using the MEGAHIT assembler to assemble our bacterium megahit -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz -o m_genitalium This will take a few minutes. The result of the assembly is in the directory m_genitalium under the name final.contigs.fa Let's make a copy of it cp m_genitalium/final.contigs.fa m_genitalium.fasta and look at it head m_genitalium.fasta Quality of the Assembly \u00b6 QUAST is a software evaluating the quality of genome assemblies by computing various metrics, including Run Quast on your assembly quast.py m_genitalium.fasta -o m_genitalium_report and take a look at the text report cat m_genitalium_report/report.txt You should see something like All statistics are based on contigs of size >= 500 bp , unless otherwise noted ( e . g ., \" # contigs (>= 0 bp) \" and \" Total length (>= 0 bp) \" include all contigs ) . Assembly m_genitalium # contigs ( >= 0 bp ) 17 # contigs ( >= 1000 bp ) 8 # contigs ( >= 5000 bp ) 7 # contigs ( >= 10000 bp ) 6 # contigs ( >= 25000 bp ) 5 # contigs ( >= 50000 bp ) 2 Total length ( >= 0 bp ) 584267 Total length ( >= 1000 bp ) 580160 Total length ( >= 5000 bp ) 577000 Total length ( >= 10000 bp ) 570240 Total length ( >= 25000 bp ) 554043 Total length ( >= 50000 bp ) 446481 # contigs 11 Largest contig 368542 Total length 582257 GC ( % ) 31 . 71 N50 368542 N75 77939 L50 1 L75 2 # N ' s per 100 kbp 0.00 which is a summary stats about our assembly. Additionally, the file m_genitalium_report/report.html You can either download it and open it in your own web browser, or we make it available for your convenience: m_genitalium_report/report.html Note N50: length for which the collection of all contigs of that length or longer covers at least 50% of assembly length Question How well does the assembly total consensus size and coverage correspond to your earlier estimation? Question How many contigs in total did the assembly produce? Question What is the N50 of the assembly? What does this mean? Fixing misassemblies \u00b6 Pilon is a software tool which can be used to automatically improve draft assemblies. It attempts to make improvements to the input genome, including: Single base differences Small Indels Larger Indels or block substitution events Gap filling Identification of local misassemblies, including optional opening of new gaps Pilon then outputs a FASTA file containing an improved representation of the genome from the read data and an optional VCF file detailing variation seen between the read data and the input genome. Before running Pilon itself, we have to align our reads against the assembly bowtie2 - build m_genitalium . fasta m_genitalium bowtie2 - x m_genitalium - 1 ERR486840_1 . fastq . gz - 2 ERR486840_2 . fastq . gz | \\ samtools view - bS - o m_genitalium . bam samtools sort m_genitalium . bam - o m_genitalium . sorted . bam samtools index m_genitalium . sorted . bam then we run Pilon pilon --genome m_genitalium.fasta --frags m_genitalium.sorted.bam --output m_genitalium_improved which will correct eventual mismatches in our assembly and write the new improved assembly to m_genitalium_improved.fasta Assembly Completeness \u00b6 Although quast output a range of metric to assess how contiguous our assembly is, having a long N50 does not guarantee a good assembly: it could be riddled by misassemblies! We will run busco to try to find marker genes in our assembly. Marker genes are conserved across a range of species and finding intact conserved genes in our assembly would be a good indication of its quality First we need to download and unpack the bacterial datasets used by busco wget http://busco.ezlab.org/datasets/bacteria_odb9.tar.gz tar xzf bacteria_odb9.tar.gz then we can run busco with BUSCO.py -i m_genitalium.fasta -l bacteria_odb9 -o busco_genitalium -m genome Question How many marker genes has busco found? Course literature \u00b6 Course litteraturer for today is: Next-Generation Sequence Assembly: Four Stages of Data Processing and Computational Challenges: https://doi.org/10.1371/journal.pcbi.1003345","title":"De-novo Genome Assembly"},{"location":"assembly/#de-novo-genome-assembly","text":"","title":"De-novo Genome Assembly"},{"location":"assembly/#lecture","text":"","title":"Lecture"},{"location":"assembly/#practical","text":"In this practical we will perform the assembly of M. genitalium , a bacterium published in 1995 by Fraser et al in Science ( abstract link ).","title":"Practical"},{"location":"assembly/#getting-the-data","text":"M. genitalium was sequenced using the MiSeq platform (2 * 150bp). The reads were deposited in the ENA Short Read Archive under the accession ERR486840 Download the 2 fastq files associated with the run. wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_1.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_2.fastq.gz The files that were deposited in ENA were already trimmed, so we do not have to trim ourselves! Question How many reads are in the files?","title":"Getting the data"},{"location":"assembly/#de-novo-assembly","text":"We will be using the MEGAHIT assembler to assemble our bacterium megahit -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz -o m_genitalium This will take a few minutes. The result of the assembly is in the directory m_genitalium under the name final.contigs.fa Let's make a copy of it cp m_genitalium/final.contigs.fa m_genitalium.fasta and look at it head m_genitalium.fasta","title":"De-novo assembly"},{"location":"assembly/#quality-of-the-assembly","text":"QUAST is a software evaluating the quality of genome assemblies by computing various metrics, including Run Quast on your assembly quast.py m_genitalium.fasta -o m_genitalium_report and take a look at the text report cat m_genitalium_report/report.txt You should see something like All statistics are based on contigs of size >= 500 bp , unless otherwise noted ( e . g ., \" # contigs (>= 0 bp) \" and \" Total length (>= 0 bp) \" include all contigs ) . Assembly m_genitalium # contigs ( >= 0 bp ) 17 # contigs ( >= 1000 bp ) 8 # contigs ( >= 5000 bp ) 7 # contigs ( >= 10000 bp ) 6 # contigs ( >= 25000 bp ) 5 # contigs ( >= 50000 bp ) 2 Total length ( >= 0 bp ) 584267 Total length ( >= 1000 bp ) 580160 Total length ( >= 5000 bp ) 577000 Total length ( >= 10000 bp ) 570240 Total length ( >= 25000 bp ) 554043 Total length ( >= 50000 bp ) 446481 # contigs 11 Largest contig 368542 Total length 582257 GC ( % ) 31 . 71 N50 368542 N75 77939 L50 1 L75 2 # N ' s per 100 kbp 0.00 which is a summary stats about our assembly. Additionally, the file m_genitalium_report/report.html You can either download it and open it in your own web browser, or we make it available for your convenience: m_genitalium_report/report.html Note N50: length for which the collection of all contigs of that length or longer covers at least 50% of assembly length Question How well does the assembly total consensus size and coverage correspond to your earlier estimation? Question How many contigs in total did the assembly produce? Question What is the N50 of the assembly? What does this mean?","title":"Quality of the Assembly"},{"location":"assembly/#fixing-misassemblies","text":"Pilon is a software tool which can be used to automatically improve draft assemblies. It attempts to make improvements to the input genome, including: Single base differences Small Indels Larger Indels or block substitution events Gap filling Identification of local misassemblies, including optional opening of new gaps Pilon then outputs a FASTA file containing an improved representation of the genome from the read data and an optional VCF file detailing variation seen between the read data and the input genome. Before running Pilon itself, we have to align our reads against the assembly bowtie2 - build m_genitalium . fasta m_genitalium bowtie2 - x m_genitalium - 1 ERR486840_1 . fastq . gz - 2 ERR486840_2 . fastq . gz | \\ samtools view - bS - o m_genitalium . bam samtools sort m_genitalium . bam - o m_genitalium . sorted . bam samtools index m_genitalium . sorted . bam then we run Pilon pilon --genome m_genitalium.fasta --frags m_genitalium.sorted.bam --output m_genitalium_improved which will correct eventual mismatches in our assembly and write the new improved assembly to m_genitalium_improved.fasta","title":"Fixing misassemblies"},{"location":"assembly/#assembly-completeness","text":"Although quast output a range of metric to assess how contiguous our assembly is, having a long N50 does not guarantee a good assembly: it could be riddled by misassemblies! We will run busco to try to find marker genes in our assembly. Marker genes are conserved across a range of species and finding intact conserved genes in our assembly would be a good indication of its quality First we need to download and unpack the bacterial datasets used by busco wget http://busco.ezlab.org/datasets/bacteria_odb9.tar.gz tar xzf bacteria_odb9.tar.gz then we can run busco with BUSCO.py -i m_genitalium.fasta -l bacteria_odb9 -o busco_genitalium -m genome Question How many marker genes has busco found?","title":"Assembly Completeness"},{"location":"assembly/#course-literature","text":"Course litteraturer for today is: Next-Generation Sequence Assembly: Four Stages of Data Processing and Computational Challenges: https://doi.org/10.1371/journal.pcbi.1003345","title":"Course literature"},{"location":"command_line/","text":"The command-line \u00b6 Warning This lesson has been deprecated. Please refer to http://swcarpentry.github.io/shell-novice/ for a better, up-to-date lesson This tutorial is largely inspired of the Introduction to UNIX course from the Sanger Institute. The aim of this module is to introduce Unix and cover some of the basics that will allow you to be more comfortable with the command-line. Several of the programs that you are going to use during this course are useful for bioinformatics analyses. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry. Introduction \u00b6 Unix is the standard operating system on most large computer systems in scientific research, in the same way that Microsoft Windows is the dominant operating system on desktop PCs. Unix and MS Windows both perform the important job of managing the computer\u2019s hardware (screen, keyboard, mouse, hard disks, network connections, etc...) on your behalf. They also provide you with tools to manage your files and to run application software. They both offer a graphical user interface (desktop). The desktops look different, call things by different names but they mostly can do the same things. Unix is a powerful, secure, robust and stable operating system that allows dozens of people to run programs on the same computer at the same time. This is why it is the preferred operating system for large-scale scientific computing. It is run on all kind of machines, like mobile phones (Android), desktop PCs, kitchen appliances,... all the way up to supercomputers. Unix powers the majority of the Internet. Aims \u00b6 The aim of this course is to introduce Unix and cover the basics. The programs that you are going to use during the courses, plus many others that are useful for bioinformatics analyses, are run in Unix. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry. Why use Unix? \u00b6 Unix is a well established, very widespread operating system. You probably have a device running on Unix in your home without realising it (e.g. playstation, TV box, wireless router, android tablets/phones,... Command line driven, with a huge number of often terse, but powerful commands. In contrast to Windows, it is designed to allow many users to run their programs simultaneously on the same computer. Designed to work in computer networks - for example, most of the Internet is Unix based. It is used on many of the powerful computers at bioinformatics centres and also on many desktops and laptops (MacOS is largely UNIX compatible). The major difference between Unix and Windows is that it is free (as in freedom) and you can modify it to work however you want. This same principle of freedom is also used in most bioinformatics software. There are many distributions of Unix such as Ubuntu, RedHat, Fedora, Mint,...). These are all Unix, but they bundle up extra software in a different way or combinations. Some are known for being conservative and reliable; whilst others are know for being cutting-edge (and less reliable). The MacOSX operating system used by the eBioKit is also based on Unix. Getting started \u00b6 For this course, you will have to connect to the eBiokit using SSH. SSH stands for Secure Shell and is a network protocol used to securely connect to a server. To do so, you will need an SSH client: On Linux: it is included by default, named Terminal. On MacOS: it is included by default, also named Terminal. On Windows: you'll have to download and install MobaXterm , a terminal emulator. Once you've opened your terminal (or terminal emulator), type ssh username@ip_address replacing username and ip_address with your username and the ip address of the server you are connecting to. Type your password when prompted. As you type, nothing will show on screen. No stars, no dots. It is supposed to be that way. Just type the password and press enter! You can type commands directly into the terminal at the \u2018$' prompt. A list of useful commands can be found on the next page. Many of them are two- or three-letter abbreviations. The earliest Unix systems ( circa 1970) only had slow Teletype terminals, so it was faster to type 'rm' to remove a file than 'delete' or 'erase'. This terseness is a feature of Unix that still survives. The command line \u00b6 All Unix programs may be run by typing commands at the Unix prompt. The command line tells the computer what to do. You may subtly alter these commands by specifying certain options when typing in the command line. Command line Arguments \u00b6 Typing any Unix command for example ls , mv or cd at the Unix prompt with the appropriate variables such as files names or directories will result in the tasks being performed on pressing the enter key. The command is separated from the options and arguments by a space. Additional options and/or arguments can be added to the commands to affect the way the command works. Options usually have one dash and a letter (e.g. -h) or two dashes and a word (--help) with no space between the dash and the letter/word. Arguments are usually filenames or directories. For example: List the contents of a directory ls List the contents of a directoryList the contents of a directory with extra information about the files ls \u2013l List the contents of a directory with extra information about the files ls \u2013a List all contents including hidden files & directories ls -al List all contents including hidden files & directories, with extra information about the files ls \u2013l /usr/ List the contents of the directory /usr/, with extra information about the files Files and Directories \u00b6 Directories are the Unix equivalent of folders on a PC or Mac. They are organised in a hierarchy, so directories can have sub-directories. Directories are very useful for organising your work and keeping your account tidy - for example, if you have more than one project, you can organise the files for each project into different directories to keep them separate. You can think of directories as rooms in a house. You can only be in one room (directory) at a time. When you are in a room you can see everything in that room easily. To see things in other rooms, you have to go to the appropriate door and crane your head around. Unix works in a similar manner, moving from directory to directory to access files. The location or directory that you are in is referred to as the current working directory. Directory structure example Therefore if there is a file called genome.seq in the dna directory its location or full pathname can be expressed as /nfs/dna/genome.seq. General Points \u00b6 Unix is pretty straightforward, but there are some general points to remember that will make your life easier: most flavors of UNIX are case sensitive - typing ls is generally not the same as typing LS . You need to put a space between a command and its argument - for example, less my_file will show you the contents of the file called my_file; lessmyfile will just give you an error! Unix is not psychic: If you misspell the name of a command or the name of a file, it will not understand you. Many of the commands are only a few letters long; this can be confusing until you start to think logically about why those letters were chosen - ls for list, rm for remove and so on. Often when you have problems with Unix, it is due to a spelling mistake, or perhaps you have omitted a space. If you want to know more about Unix and its commands there are plenty of resources available that provide a more comprehensive guide (including a cheat sheet at the end of this chapter. http://unix.t-a-y-l-o-r.com/ In what follows, we shall use the following typographical conventions: Characters written in bold typewriter font are commands to be typed into the computer as they stand. Characters written in italic typewriter font indicate non-specific file or directory names. Words inserted within square brackets [Ctrl] indicate keys to be pressed. So, for example, $ **ls** *any_directory* [Enter] means \"at the Unix prompt $, type ls followed by the name of some directory, then press Enter\" Don't forget to press the [Enter] key: commands are not sent to the computer until this is done. Some useful Unix commands Command and What it does \u00b6 Command What it does ls Lists the contents of the current directory mkdir Creates a new directory mv Moves or renames a file cp Copies a file rm Removes a file cat Concatenates files less Displays the contents of a file one page at a time head Displays the first ten lines of a file tail Displays the last ten lines of a file cd Changes current working directory pwd Prints working directory find Finds files matching an expression grep Searches a file for patterns wc Counts the lines, words, characters, and bytes in a file kill Stops a process jobs Lists the processes that are running Firts steps \u00b6 The following exercise introduces a few useful Unix commands and provides examples of how they can be used. Many people panic when they are confronted with an Unix prompt! Don\u2019t! The exercise is designed to be step-by-step, so all the commands you need are provided in the text. If you get lost ask a demonstrator. If you are a person skilled at Unix, be patient it is only a short exercise. Finding where you are and what you\u2019ve got pwd Print the working directory As seen previously directories are arranged in a hierarchical structure. To determine where you are in the hierarchy you can use the pwd command to display the name of the current working directory. The current working directory may be thought of as the directory you are in, i.e. your current position in the file-system tree To find out where you are type pwd [enter] You will see that you are in your home directory. We need to move into the ngs_course_data directory. Remember, Unix is case sensitive PWD is not the same as pwd cd Change current working directory The cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the \"ngs_course_data\" directory below. To do this type: cd ngs_course_data [enter] Now use the pwd command to check your location in the directory hierarchy. Change again the directory to Module_Unix ls List the contents of a directory To find out what are the contents of the current directory type ls [enter] The ls command lists the contents of your current directory, this includes files and directories You should see that there are several other directories. Now use the cd command again to change to the Module_Unix directory. Changing and moving what you\u2019ve got \u00b6 cp Copy a file. cp file1 file2 is the command which makes a copy of file1 in the current working directory and calls it file2! What you are going to do is make a copy of AL513382.embl. This file contains the genome of Salmonella typhi strain CT18 in EMBL format (we'll learn more about file formats later during the course). The new file will be called S_typhi.embl. cp AL513382.embl S_typhi.embl [enter] If you use the ls command to check the contents of the current directory you will see that there is an extra file called S_typhi.embl. rm Delete a file. This command removes a file permanently, so be careful! You are now going to remove the old version of the S. typhi genome file, AL513382.embl rm AL513382.embl [enter] The file will be removed. Use the ls command to check the contents of the current directory to see that AL513382.embl has been removed. Unix, as a general rule does exactly what you ask, and does not ask for confirmation. Unfortunately there is no \"recycle bin\" on the command line to recover the file from, so you have to be careful. cd Change current working directory. As before the cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the directory above, type: cd .. [enter] Now use the pwd command to check your location in the directory hierarchy. Next, we are going to move into the Module_Artemis directory. To change to the Module_Artemis directory type: cd Module_Artemis [enter] use the ls command to check the contents of the directory. Tips \u00b6 There are some short cuts for referring to directories: . Current directory ( one full stop ) .. Directory above ( two full stops ) ~ Home directory ( tilde ) / Root of the file system ( like C : \\ in Windows ) Pressing the tab key twice will try and autocomplete what you\u2019ve started typing or give you a list of all possible completions. This saves a lot of typing and typos. Pressing the up/down arrows will let you scroll through the previous commands. If you highlight some text, middle clicking will paste it on the command line. mv Move a file. To move a file from one place to another use the mv command. This moves the file rather than copies it, therefore you end up with only one file rather than two. When using the command the path or pathname is used to tell Unix where to find the file. You refer to files in other directories by using the list of hierarchical names separated by slashes. For example, the file bases in the directory genome has the path genome/bases If no path is specified Unix assumes that the file is in the current working directory. What you are going to do is move the file S_typhi.embl from the Module_Unix directory, to the current working directory. mv ../Module_Unix/S_typhi.embl . [enter] Use the ls command to check the contents of the current directory to see that S_typhi.embl has been moved. ../Module_Unix/S_typhi.embl specifies that S_typhi.embl is in the Module_Unix directory. If the file was in the directory above, the path would change to: ../ S_typhi.embl The command can also be used to rename a file in the current working directory. Previously we used the cp command, but mv provides an alternative without the need to delete the original file. Therefore we could have used: mv AL513382.embl S_typhi.embl [enter] instead of: cp AL513382 . embl S_typhi . embl [ enter ] rm AL513382 . embl [ enter ] Viewing what you\u2019ve got \u00b6 less Display file contents. This command displays the contents of a specified file one screen at a time. You are now going to look at the contents of S_typhi.embl. less S_typhi.embl [enter] The contents of S_typhi.embl will be displayed one screen at a time, to view the next screen press the space bar. less can also scroll backwards if you hit the b key. Another useful feature is the slash key, /, to search for a word in the file. You type the word you are looking for and press enter. The screen will jump to the next occurrence and highlight it. As S_typhi.embl is a large file this will take a while, therefore you may want to escape or exit from this command. To exit press the letter \u2018q\u2019. If you really need to exit from a program and it isn\u2019t responding press \u2018control\u2019 and the letter \u2018c\u2019 at the same time. head Display the first ten lines of a file tail Display the last ten lines of a file Sometimes you may just want to view the text at the beginning or the end of a file, without having to display all of the file. The head and tail commands can be used to do this. You are now going to look at the beginning of S_typhi.embl. head S_typhi.embl [enter] To look at the end of S_typhi.embl type: tail S_typhi.embl [enter] The number of lines that are displayed can be increased by adding extra arguments. To increase the number of lines viewed from 10 to 100 add the \u2013100 argument to the command. For example to view the last 100 lines of S_typhi.embl type: tail -100 S_typhi.embl [enter] Do this for both head and tail commands. What type of information is at the beginning and end of the EMBL format file? cat Join files together. Having looked at the beginning and end of the S_typhi.embl file you should notice that in EMBL format files the annotation comes first, then the DNA sequence at the end. If you had two separate files containing the annotation and the DNA sequence, both in EMBL format, it is possible to concatenate or join the two together to make a single file like the S_typhi.embl file you have just looked at. The Unix command cat can be used to join two or more files into a single file. The order in which the files are joined is determined by the order in which they appear in the command line. For example, we have two separate files, MAL13P1.dna and MAL13P1.tab, that contain the DNA and annotation, respectively, from the P. falciparum genome. Return to the Module_Unix directory using the cd command: cd ../Module_Unix [enter] and type cat MAL13P1.tab MAL13P1.dna > MAL13P1.embl [enter] MAL13P1.tab and MAL13P1.dna will be joined together and written to a file called MAL13P1.embl The > symbol in the command line directs the output of the cat program to the designated file MAL13P1.embl wc Counts the lines, words or characters of files. By typing the command line: ls | wc -l [enter] The above command uses wc to count the number of files that are listed by ls. The \u2018-l\u2019 option tells wc to return a count of the number of lines. The | symbol (known as the \u2018pipe\u2019 character) in the command line connects the two commands into a single operation for simplicity. You can connect as many commands as you want: ls | grep \".embl\" | wc -l This command will list out all of the files in the current directory, then send the results to the grep command which searches for all filenames containing the \u2018embl\u2019, then sends the results to wc which counts the number of lines (which corresponds to the number of files). grep Searches a file for patterns. grep is a powerful tool to search for patterns in a file. In the examples below, we are going to use the file called Malaria.fasta that contains the set of P. falciparum chromosomes in FASTA format. A FASTA file has the following format: Sequence Header CTAAACCTAAACCTAAACCCTGAACCCTAA... Therefore if we want to get the sequence headers, we can extract the lines that match the \u2018>\u2019 symbol: grep \u2018>\u2019 Malaria.fasta [enter] By typing the command line: grep -B 1 -A 1 'aagtagggttca' Malaria.fasta [enter] This command will search for a nucliotide sequence and print 1 line before and after any match. It won\u2019t find the pattern if it spans more than 1 line. find Finds files matching an expression. The find command is similar to ls but in many ways it is more powerful. It can be used to recursively search the directory tree for a specified path name, seeking files that match a given Boolean expression (a test which returns true or false) find . -name \u201c*.embl\u201d This command will return the files which name has the .embl suffix. mkdir test_directory find . -type d This command will return all the subdirectories contained in the current directory. These are just two basic examples but it is possible to search in many other ways: -mtime search files by modifying date -atime search files by last access date -size search files by file size -user search files by user they belong to. Tips \u00b6 You need to be careful with quoting when using wildcards! The wildcard * symbol represents a string of any character and of any length. For more information on Unix command see EMBNet UNIX Quick Guide. End of the module # Introduction to Unix (continued) In this part of the Unix tutorial, you will learn to download files, compress and decompress them, and combine commands. ## Download files `wget` can be used to download files from the internet and store them. `wget https://raw.githubusercontent.com/HadrienG/tutorials/master/LICENSE` will download the file that is located at the above URL on the internet, and put it **in the current directory**. This is the license under which this course is released. Open it and read it if you like! The `-O` option can be used to change the output file name. `wget -O GNU_FDL.txt https://raw.githubusercontent.com/HadrienG/tutorials/master/LICENSE` You can also use wget to download a file list using -i option and giving a text file containing file URLs. The following cat > download - file - list . txt url_1 url_2 url_3 url_4 [ CTRL - C ] ( to exit cat ) `wget -i download-file-list.txt` ## Compressing and decompressing files ### Compressing files with gzip gzip is a utility for compressing and decompressing individual files. To compress files, use: `gzip filename` The filename will be deleted and replaced by a compressed file called filename.gz. To reverse the compression process, use: `gzip -d filename.gz` Try it on the License you just downloaded! ### Tar archives Quite often, you don't want to compress just one file, but rather a bunch of them, or a directory. tar backs up entire directories and files as an archive. An archive is a file that contains other files plus information about them, such as their filename, owner, timestamps, and access permissions. tar does not perform any compression by default. To create a gzipped disk file tar archive, use `tar -czvf archivename filenames` where archivename will usually have a .tar.gz extension The c option means create, the v option means verbose (output filenames as they are archived), option f means file, and z means that the tar archive should be gzip compressed. To list the contents of a gzipped tar archive, use `tar -tzvf archivename` To unpack files from a tar archive, use `tar -xzvf archivename` Try to archive the folder `Module_Unix` from the previous exercise! You will notice a file called tutorials.tar.bz2 in your home directory. This is also a compressed archive, but compressed in the bzip format. Read the tar manual and find a way to decompress it. Hint: you can read the manual for any command using `man` `man tar` ### Redirection Some commands give you an output to your screen, but you would have preferred it to go into another program or into a file. For those cases you have some redirection characters. #### Output redirection The output from a command normally intended for standard output (that is, your screen) can be easily diverted to a file instead. This capability is known as output redirection: If the notation `> file` is appended to any command that normally writes its output to standard output, the output of that command will be written to file instead of your terminal. I.e, the following who command: `who > users.txt` No output appears at the terminal. This is because the output has been redirected into the specified file. `less users.txt` Be careful, if a command has its output redirected to a file and the file already contains some data, that data will be lost. Consider this example: `echo Hello > users.txt` `less users.txt` You can use the `>>` operator to append the output in an existing file as follows: who > users . txt echo \" This goes at the end of the file \" >> users . txt `less users.txt` #### Piping You can connect two commands together so that the output from one program becomes the input of the next program. Two or more commands connected in this way form a pipe. To make a pipe, put a vertical bar `|` on the command line between two commands. Remember the command `grep`? We can pipe other commands to it, to refine searches per example: `ls -l ngs_course_data | grep \"Jan\"` will only give you the files and directories created in January. Tip: There are various options you can use with the grep command, look at the manual! Pipes are extremely useful to connect various bioinformatics software together. We'll use them extensively later. # Introduction to Unix (continued) In this part of the tutorial, we'll learn how to install programs in a Unix system ## Using a package manager This is the most straight-forward way, and the way used by most of the people using unix at home, or administrating their own machine. This course is aimed at giving you a working knowledge of linux for bioinformatics, and in that setting, you will rarely, if ever, be the administrator of your own machine. The methods below are here as an information ### On Ubuntu and Debian: Apt To install a software: `apt-get install name_of_the_software` to uninstall: `apt-get remove name_of_the_software` to update all installed softwares: apt - get update apt - get upgrade ### On Fedora, CentOS and RedHat: yum To install a software: `yum install name_of_the_software` to uninstall: `yum remove name_of_the_software` to update: `yum update` ### MacOS: brew Although there are no official package managers on MacOS, two popular, community-driven alternatives exist: macports and brew. Brew is particularly pupular within the bioinformatics community, and allows easy installation of many bioinformatics softwares on MacOS To install brew on your mac: `/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"` To install a software: `brew install name_of_the_software` To uninstall: `brew uninstall name_of_the_software` To update all brew-installed softwares: brew update brew upgrade More info on [brew.sh](http://brew.sh) and [brew.sh/homebrew-science/](http://brew.sh/homebrew-science/) ## Downloading binaries In a university setting, you will rarely by administrator of your own machine. This is a very good thing for one reason: it's harder for you to break something! The downside is that it makes installing softwares more complicated. We'll start wit simply downloading the software and executing it, then we'll learn how to obtain packages from source code. for example, we'll install the blast binaries: First, download the archive: `wget ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.6.0+-x64-linux.tar.gz` then unpack it and go to the newly created directory tar xzf ncbi - blast - 2 . 6 . 0 +- x64 - linux . tar . gz cd ncbi - blast - 2 . 6 . 0 + you should have a `bin` directory, go inside and look at the files. You have a bunch of executable files. ### Execute a file Most of the lunix commands that you execute on a regular basis (ls, cp, mkdir) are located in `/usr/bin`, but you don't have to invoke them with their full path: i.e. you dont type `/usr/bin/ls` but just `ls`. This is because `/usr/bin/ls` is in your $PATH. to execute a file that you just downloaded, and is therefore not in your path, you have to type the absolute or relative path to that file. Meaning, for the blast program suite that we just downloaded: `bin/blastn -help` or cd bin . / blastn - help and that's it! But it is not very convenient. You want to be able to execute blast without having to remember where it is. If you have administrator rights (sudo), you can move the software in `/usr/bin`. If you don't you can modify your $PATH in a configuration file called `.bash_profile` that is located in your home. More information on how to correctly modify your PATH [here](http://unix.stackexchange.com/a/26059) ## Compiling from source Sometimes pre-compiled binaries are not available. You then have to compile from source: transforming the human-readable code (written in one or another programming language) into machine-readable code (binary) The most common way to do so, if a software package has its source coud available online is . / configure make make install If you don't have the administrator rights, you'll often have to pass an extra argument to ./configure: . / configure --prefix=\"/where_i_want_to_install\" make make install Most of the softwares come with instructions on how to install them. Always read the file called README or INSTALL in the package directory before installing! ### Exercice The most popular unix distributions come with a version of python (a programming language) that is not the most recent one. Install from source the most recent version of python in a folder called `bin` in your home directory. You can download the python source code at https://www.python.org/ftp/python/3.6.0/Python-3.6.0.tgz ## Install python packages Python is a really popular programming language in the world of bioinformatics. Python has a package manager called `pip` that you can use to install softwares written in python. Please us the python executable you installed in the above exercise! Firstly, get pip: `wget https://bootstrap.pypa.io/get-pip.py` then execute the script `python get-pip.py` Thenm you can use pip to install package, either globally (if you're an administrator): `pip install youtube_dl` or just for you: `pip install --user youtube_dl` ## Final exercise One of the oldest and most famous bioinformatics package is called EMBOSS. Install EMBOSS in the bin directory of your home. Good luck! # Introduction to UNIX (continued) In the 4th and last module of your unix course, we'll how to write small programs, or scripts. Shell scripts allow us to program commands in chains and have the system execute them as a scripted chain of events. They also allow for far more useful functions, such as command substitution. You can invoke a command, like date, and use it\u2019s output as part of a file-naming scheme. You can automate backups and each copied file can have the current date appended to the end of its name. You can automate a bioinformatics analysis pipeline. Before we begin our scripting tutorial, let\u2019s cover some basic information. We\u2019ll be using the bash shell, which most Linux distributions use natively. Bash is available for Mac OS users and Cygwin on Windows (which you are using with MobaXterm). Since it\u2019s so universal, you should be able to script regardless of your platform. At their core, scripts are just plain text files. You can use nano (or any other text editor) to write them. ## Permissions Scripts are executed like programs. For this to happen, you need to have the proper permissions. You can make the script executable for you by running the following command: `chmod u+x my_script.sh` by convention, bash script are saved with the .sh extension. Linux doesn't really care about file extension, but it is easier for the user to use the \"proper\" extensions! ## executing a script You have to cd in the proper directory, then run the script like this: `./my_script.sh` To make things more convenient, you can place scripts in a \u201cbin\u201d folder in your home directory and add it to your path `mkdir -p ~/bin` More information on how to correctly modify your PATH [here](http://unix.stackexchange.com/a/26059) ## Getting started As previously said, every script is a text file. Still, there are rules and conventions to follow in order of you file being recognized as a script If you juste write a few command and try to execute it as is, with `./my_script`, it will not work. You can invoke `sh my_script`, but it is not very convenient. `./` tries to find out which interpreter to use (e.g. which programming language and how to execute your script). It does so by looking at the first line: The first line of your bash scripts should be: `#!/bin/bash` or `#!/usr/bin/env bash` The second version being better and more portable. Ask your teacher why! This line will have the same syntax for every interpreted language. If you are programming in python: `#!/usr/bin/env python` ### New line = new command After the firstline, every line of your script will be a new command. Your first scripts will essentially be a succession of terminal commands. We'll learn about flow control (if, for, while, ...) later on. ### Comments It is good practise to comment your scripts, i.e give some explanation of what is does, and explain a particularly arcane method that you wrote. Comments start with a `#` and are snippets of texts that are ignored by the interpreter. ### Your first script Let's start with a simple script, that copy files and append today's date to the end of the file name. We'll call it `datecp.sh` In your `~/bin` folder: touch datecp . sh chmod u + x datecp . sh and let's start writing our script `nano datecp.sh` 1 2 3 4 #!/usr/bin/env bash # this script will copy a file, appending the data and time to # the end of the file name Next, we need to declare a variable. A variable allows us to store and reuse information (characters, the date or the command `date`). Variables have a name, but can **expend** to their content when referenced if they contain a command. Variables can hold strings and characers, like this: `my_variable=\"hippopotamus\"` or a command. In bash, the correct way to store a command in a variable is within the syntax `$()`: `variable=$(command \u2013options arguments)` Store the date and time in a variable. Test the date command first in your terminal, then when you got the right format, store it in a variable in your script. It is generally bad practice to put spaces in file names in unix, so we'll want the following date format: `date +%m_%d_%y-%H.%M.%S` and for putting it into a variable: date_formatted=$(date +%m_%d_%y-%H.%M.%S) Your script now can print thedate without too much more coding: 1 2 3 4 5 6 7 #!/usr/bin/env bash # this script will copy a file, appending the data and time to # the end of the file name date_formatted = $( date +%m_%d_%y-%H.%M.%S ) echo \"This is the date and time:\" $date_formatted Now we need to add the copying part: `cp \u2013iv $1 $2.$date_formatted` This will invoke the copy command, with two options: -i for asking for permission before overwriting a file, and -v for verbose. You can also notice two variables: $1 and $2. When scripting in bash, a dollar sign ($) followed by a number will denote an argument of the script. For example in the following command: `cp \u2013iv a_file a_file_copy` the first argument ($1) is `a_file` and the second argument ($2) is `a_file_copy` What our script will do is a simple copy of a file, but with adding the date to the end of the file name. Save it and try it out! ### Exercise Write a script that backs itself up, that is, copies itself to a file named backup.sh. Hint: Use the cat command and the appropriate positional parameter.","title":"The command-line"},{"location":"command_line/#the-command-line","text":"Warning This lesson has been deprecated. Please refer to http://swcarpentry.github.io/shell-novice/ for a better, up-to-date lesson This tutorial is largely inspired of the Introduction to UNIX course from the Sanger Institute. The aim of this module is to introduce Unix and cover some of the basics that will allow you to be more comfortable with the command-line. Several of the programs that you are going to use during this course are useful for bioinformatics analyses. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry.","title":"The command-line"},{"location":"command_line/#introduction","text":"Unix is the standard operating system on most large computer systems in scientific research, in the same way that Microsoft Windows is the dominant operating system on desktop PCs. Unix and MS Windows both perform the important job of managing the computer\u2019s hardware (screen, keyboard, mouse, hard disks, network connections, etc...) on your behalf. They also provide you with tools to manage your files and to run application software. They both offer a graphical user interface (desktop). The desktops look different, call things by different names but they mostly can do the same things. Unix is a powerful, secure, robust and stable operating system that allows dozens of people to run programs on the same computer at the same time. This is why it is the preferred operating system for large-scale scientific computing. It is run on all kind of machines, like mobile phones (Android), desktop PCs, kitchen appliances,... all the way up to supercomputers. Unix powers the majority of the Internet.","title":"Introduction"},{"location":"command_line/#aims","text":"The aim of this course is to introduce Unix and cover the basics. The programs that you are going to use during the courses, plus many others that are useful for bioinformatics analyses, are run in Unix. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry.","title":"Aims"},{"location":"command_line/#why-use-unix","text":"Unix is a well established, very widespread operating system. You probably have a device running on Unix in your home without realising it (e.g. playstation, TV box, wireless router, android tablets/phones,... Command line driven, with a huge number of often terse, but powerful commands. In contrast to Windows, it is designed to allow many users to run their programs simultaneously on the same computer. Designed to work in computer networks - for example, most of the Internet is Unix based. It is used on many of the powerful computers at bioinformatics centres and also on many desktops and laptops (MacOS is largely UNIX compatible). The major difference between Unix and Windows is that it is free (as in freedom) and you can modify it to work however you want. This same principle of freedom is also used in most bioinformatics software. There are many distributions of Unix such as Ubuntu, RedHat, Fedora, Mint,...). These are all Unix, but they bundle up extra software in a different way or combinations. Some are known for being conservative and reliable; whilst others are know for being cutting-edge (and less reliable). The MacOSX operating system used by the eBioKit is also based on Unix.","title":"Why use Unix?"},{"location":"command_line/#getting-started","text":"For this course, you will have to connect to the eBiokit using SSH. SSH stands for Secure Shell and is a network protocol used to securely connect to a server. To do so, you will need an SSH client: On Linux: it is included by default, named Terminal. On MacOS: it is included by default, also named Terminal. On Windows: you'll have to download and install MobaXterm , a terminal emulator. Once you've opened your terminal (or terminal emulator), type ssh username@ip_address replacing username and ip_address with your username and the ip address of the server you are connecting to. Type your password when prompted. As you type, nothing will show on screen. No stars, no dots. It is supposed to be that way. Just type the password and press enter! You can type commands directly into the terminal at the \u2018$' prompt. A list of useful commands can be found on the next page. Many of them are two- or three-letter abbreviations. The earliest Unix systems ( circa 1970) only had slow Teletype terminals, so it was faster to type 'rm' to remove a file than 'delete' or 'erase'. This terseness is a feature of Unix that still survives.","title":"Getting started"},{"location":"command_line/#the-command-line_1","text":"All Unix programs may be run by typing commands at the Unix prompt. The command line tells the computer what to do. You may subtly alter these commands by specifying certain options when typing in the command line.","title":"The command line"},{"location":"command_line/#command-line-arguments","text":"Typing any Unix command for example ls , mv or cd at the Unix prompt with the appropriate variables such as files names or directories will result in the tasks being performed on pressing the enter key. The command is separated from the options and arguments by a space. Additional options and/or arguments can be added to the commands to affect the way the command works. Options usually have one dash and a letter (e.g. -h) or two dashes and a word (--help) with no space between the dash and the letter/word. Arguments are usually filenames or directories. For example: List the contents of a directory ls List the contents of a directoryList the contents of a directory with extra information about the files ls \u2013l List the contents of a directory with extra information about the files ls \u2013a List all contents including hidden files & directories ls -al List all contents including hidden files & directories, with extra information about the files ls \u2013l /usr/ List the contents of the directory /usr/, with extra information about the files","title":"Command line Arguments"},{"location":"command_line/#files-and-directories","text":"Directories are the Unix equivalent of folders on a PC or Mac. They are organised in a hierarchy, so directories can have sub-directories. Directories are very useful for organising your work and keeping your account tidy - for example, if you have more than one project, you can organise the files for each project into different directories to keep them separate. You can think of directories as rooms in a house. You can only be in one room (directory) at a time. When you are in a room you can see everything in that room easily. To see things in other rooms, you have to go to the appropriate door and crane your head around. Unix works in a similar manner, moving from directory to directory to access files. The location or directory that you are in is referred to as the current working directory. Directory structure example Therefore if there is a file called genome.seq in the dna directory its location or full pathname can be expressed as /nfs/dna/genome.seq.","title":"Files and Directories"},{"location":"command_line/#general-points","text":"Unix is pretty straightforward, but there are some general points to remember that will make your life easier: most flavors of UNIX are case sensitive - typing ls is generally not the same as typing LS . You need to put a space between a command and its argument - for example, less my_file will show you the contents of the file called my_file; lessmyfile will just give you an error! Unix is not psychic: If you misspell the name of a command or the name of a file, it will not understand you. Many of the commands are only a few letters long; this can be confusing until you start to think logically about why those letters were chosen - ls for list, rm for remove and so on. Often when you have problems with Unix, it is due to a spelling mistake, or perhaps you have omitted a space. If you want to know more about Unix and its commands there are plenty of resources available that provide a more comprehensive guide (including a cheat sheet at the end of this chapter. http://unix.t-a-y-l-o-r.com/ In what follows, we shall use the following typographical conventions: Characters written in bold typewriter font are commands to be typed into the computer as they stand. Characters written in italic typewriter font indicate non-specific file or directory names. Words inserted within square brackets [Ctrl] indicate keys to be pressed. So, for example, $ **ls** *any_directory* [Enter] means \"at the Unix prompt $, type ls followed by the name of some directory, then press Enter\" Don't forget to press the [Enter] key: commands are not sent to the computer until this is done.","title":"General Points"},{"location":"command_line/#some-useful-unix-commands-command-and-what-it-does","text":"Command What it does ls Lists the contents of the current directory mkdir Creates a new directory mv Moves or renames a file cp Copies a file rm Removes a file cat Concatenates files less Displays the contents of a file one page at a time head Displays the first ten lines of a file tail Displays the last ten lines of a file cd Changes current working directory pwd Prints working directory find Finds files matching an expression grep Searches a file for patterns wc Counts the lines, words, characters, and bytes in a file kill Stops a process jobs Lists the processes that are running","title":"Some useful Unix commands Command\u00a0and What it does"},{"location":"command_line/#firts-steps","text":"The following exercise introduces a few useful Unix commands and provides examples of how they can be used. Many people panic when they are confronted with an Unix prompt! Don\u2019t! The exercise is designed to be step-by-step, so all the commands you need are provided in the text. If you get lost ask a demonstrator. If you are a person skilled at Unix, be patient it is only a short exercise. Finding where you are and what you\u2019ve got pwd Print the working directory As seen previously directories are arranged in a hierarchical structure. To determine where you are in the hierarchy you can use the pwd command to display the name of the current working directory. The current working directory may be thought of as the directory you are in, i.e. your current position in the file-system tree To find out where you are type pwd [enter] You will see that you are in your home directory. We need to move into the ngs_course_data directory. Remember, Unix is case sensitive PWD is not the same as pwd cd Change current working directory The cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the \"ngs_course_data\" directory below. To do this type: cd ngs_course_data [enter] Now use the pwd command to check your location in the directory hierarchy. Change again the directory to Module_Unix ls List the contents of a directory To find out what are the contents of the current directory type ls [enter] The ls command lists the contents of your current directory, this includes files and directories You should see that there are several other directories. Now use the cd command again to change to the Module_Unix directory.","title":"Firts steps"},{"location":"command_line/#changing-and-moving-what-youve-got","text":"cp Copy a file. cp file1 file2 is the command which makes a copy of file1 in the current working directory and calls it file2! What you are going to do is make a copy of AL513382.embl. This file contains the genome of Salmonella typhi strain CT18 in EMBL format (we'll learn more about file formats later during the course). The new file will be called S_typhi.embl. cp AL513382.embl S_typhi.embl [enter] If you use the ls command to check the contents of the current directory you will see that there is an extra file called S_typhi.embl. rm Delete a file. This command removes a file permanently, so be careful! You are now going to remove the old version of the S. typhi genome file, AL513382.embl rm AL513382.embl [enter] The file will be removed. Use the ls command to check the contents of the current directory to see that AL513382.embl has been removed. Unix, as a general rule does exactly what you ask, and does not ask for confirmation. Unfortunately there is no \"recycle bin\" on the command line to recover the file from, so you have to be careful. cd Change current working directory. As before the cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the directory above, type: cd .. [enter] Now use the pwd command to check your location in the directory hierarchy. Next, we are going to move into the Module_Artemis directory. To change to the Module_Artemis directory type: cd Module_Artemis [enter] use the ls command to check the contents of the directory.","title":"Changing and moving what you\u2019ve got"},{"location":"command_line/#tips","text":"There are some short cuts for referring to directories: . Current directory ( one full stop ) .. Directory above ( two full stops ) ~ Home directory ( tilde ) / Root of the file system ( like C : \\ in Windows ) Pressing the tab key twice will try and autocomplete what you\u2019ve started typing or give you a list of all possible completions. This saves a lot of typing and typos. Pressing the up/down arrows will let you scroll through the previous commands. If you highlight some text, middle clicking will paste it on the command line. mv Move a file. To move a file from one place to another use the mv command. This moves the file rather than copies it, therefore you end up with only one file rather than two. When using the command the path or pathname is used to tell Unix where to find the file. You refer to files in other directories by using the list of hierarchical names separated by slashes. For example, the file bases in the directory genome has the path genome/bases If no path is specified Unix assumes that the file is in the current working directory. What you are going to do is move the file S_typhi.embl from the Module_Unix directory, to the current working directory. mv ../Module_Unix/S_typhi.embl . [enter] Use the ls command to check the contents of the current directory to see that S_typhi.embl has been moved. ../Module_Unix/S_typhi.embl specifies that S_typhi.embl is in the Module_Unix directory. If the file was in the directory above, the path would change to: ../ S_typhi.embl The command can also be used to rename a file in the current working directory. Previously we used the cp command, but mv provides an alternative without the need to delete the original file. Therefore we could have used: mv AL513382.embl S_typhi.embl [enter] instead of: cp AL513382 . embl S_typhi . embl [ enter ] rm AL513382 . embl [ enter ]","title":"Tips"},{"location":"command_line/#viewing-what-youve-got","text":"less Display file contents. This command displays the contents of a specified file one screen at a time. You are now going to look at the contents of S_typhi.embl. less S_typhi.embl [enter] The contents of S_typhi.embl will be displayed one screen at a time, to view the next screen press the space bar. less can also scroll backwards if you hit the b key. Another useful feature is the slash key, /, to search for a word in the file. You type the word you are looking for and press enter. The screen will jump to the next occurrence and highlight it. As S_typhi.embl is a large file this will take a while, therefore you may want to escape or exit from this command. To exit press the letter \u2018q\u2019. If you really need to exit from a program and it isn\u2019t responding press \u2018control\u2019 and the letter \u2018c\u2019 at the same time. head Display the first ten lines of a file tail Display the last ten lines of a file Sometimes you may just want to view the text at the beginning or the end of a file, without having to display all of the file. The head and tail commands can be used to do this. You are now going to look at the beginning of S_typhi.embl. head S_typhi.embl [enter] To look at the end of S_typhi.embl type: tail S_typhi.embl [enter] The number of lines that are displayed can be increased by adding extra arguments. To increase the number of lines viewed from 10 to 100 add the \u2013100 argument to the command. For example to view the last 100 lines of S_typhi.embl type: tail -100 S_typhi.embl [enter] Do this for both head and tail commands. What type of information is at the beginning and end of the EMBL format file? cat Join files together. Having looked at the beginning and end of the S_typhi.embl file you should notice that in EMBL format files the annotation comes first, then the DNA sequence at the end. If you had two separate files containing the annotation and the DNA sequence, both in EMBL format, it is possible to concatenate or join the two together to make a single file like the S_typhi.embl file you have just looked at. The Unix command cat can be used to join two or more files into a single file. The order in which the files are joined is determined by the order in which they appear in the command line. For example, we have two separate files, MAL13P1.dna and MAL13P1.tab, that contain the DNA and annotation, respectively, from the P. falciparum genome. Return to the Module_Unix directory using the cd command: cd ../Module_Unix [enter] and type cat MAL13P1.tab MAL13P1.dna > MAL13P1.embl [enter] MAL13P1.tab and MAL13P1.dna will be joined together and written to a file called MAL13P1.embl The > symbol in the command line directs the output of the cat program to the designated file MAL13P1.embl wc Counts the lines, words or characters of files. By typing the command line: ls | wc -l [enter] The above command uses wc to count the number of files that are listed by ls. The \u2018-l\u2019 option tells wc to return a count of the number of lines. The | symbol (known as the \u2018pipe\u2019 character) in the command line connects the two commands into a single operation for simplicity. You can connect as many commands as you want: ls | grep \".embl\" | wc -l This command will list out all of the files in the current directory, then send the results to the grep command which searches for all filenames containing the \u2018embl\u2019, then sends the results to wc which counts the number of lines (which corresponds to the number of files). grep Searches a file for patterns. grep is a powerful tool to search for patterns in a file. In the examples below, we are going to use the file called Malaria.fasta that contains the set of P. falciparum chromosomes in FASTA format. A FASTA file has the following format: Sequence Header CTAAACCTAAACCTAAACCCTGAACCCTAA... Therefore if we want to get the sequence headers, we can extract the lines that match the \u2018>\u2019 symbol: grep \u2018>\u2019 Malaria.fasta [enter] By typing the command line: grep -B 1 -A 1 'aagtagggttca' Malaria.fasta [enter] This command will search for a nucliotide sequence and print 1 line before and after any match. It won\u2019t find the pattern if it spans more than 1 line. find Finds files matching an expression. The find command is similar to ls but in many ways it is more powerful. It can be used to recursively search the directory tree for a specified path name, seeking files that match a given Boolean expression (a test which returns true or false) find . -name \u201c*.embl\u201d This command will return the files which name has the .embl suffix. mkdir test_directory find . -type d This command will return all the subdirectories contained in the current directory. These are just two basic examples but it is possible to search in many other ways: -mtime search files by modifying date -atime search files by last access date -size search files by file size -user search files by user they belong to.","title":"Viewing what you\u2019ve got"},{"location":"command_line/#tips_1","text":"You need to be careful with quoting when using wildcards! The wildcard * symbol represents a string of any character and of any length. For more information on Unix command see EMBNet UNIX Quick Guide.","title":"Tips"},{"location":"file_formats/","text":"File Formats \u00b6 This lecture is aimed at making you discover the most popular file formats used in bioinformatics. You're expected to have basic working knowledge of Linux to be able to follow the lesson. Table of Contents \u00b6 The fasta format The fastq format The sam/bam format The vcf format The gff format The fasta format \u00b6 The fasta format was invented in 1988 and designed to represent nucleotide or peptide sequences. It originates from the FASTA software package, but is now a standard in the world of bioinformatics. The first line in a FASTA file starts with a \">\" (greater-than) symbol followed by the description or identifier of the sequence. Following the initial line (used for a unique description of the sequence) is the actual sequence itself in standard one-letter code. A few sample sequences: > KX580312 . 1 Homo sapiens truncated breast cancer 1 ( BRCA1 ) gene , exon 15 and partial cds GTCATCCCCTTCTAAATGCCCATCATTAGATGATAGGTGGTACATGCACAGTTGCTCTGGGAGTCTTCAG AATAGAAACTACCCATCTCAAGAGGAGCTCATTAAGGTTGTTGATGTGGAGGAGTAACAGCTGGAAGAGT CTGGGCCACACGATTTGACGGAAACATCTTACTTGCCAAGGCAAGATCTAG > KRN06561 . 1 heat shock [ Lactobacillus sucicola DSM 21376 = JCM 15457 ] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE A fasta file can contain multiple sequence. Each sequence will be separated by their \"header\" line, starting by \">\". Example: > KRN06561 . 1 heat shock [ Lactobacillus sucicola DSM 21376 = JCM 15457 ] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE > 3 HHU_A Chain A , Human Heat - Shock Protein 90 ( Hsp90 ) MPEETQTQDQPMEEEEVETFAFQAEIAQLMSLIINTFYSNKEIFLRELISNSSDALDKIRYESLTDPSKL DSGKELHINLIPNKQDRTLTIVDTGIGMTKADLINNLGTIAKSGTKAFMEALQAGADISMIGQFGVGFYS AYLVAEKVTVITKHNDDEQYAWESSAGGSFTVRTDTGEPMGRGTKVILHLKEDQTEYLEERRIKEIVKKH SQFIGYPITLFVEK The fastq format \u00b6 The fastq format is also a text based format to represent nucleotide sequences, but also contains the corresponding quality of each nucleotide. It is the standard for storing the output of high-throughput sequencing instruments such as the Illumina machines. A fastq file uses four lines per sequence: Line 1 begins with a '@' character and is followed by a sequence identifier and an optional description (like a FASTA title line). Line 2 is the raw sequence letters. Line 3 begins with a '+' character and is optionally followed by the same sequence identifier (and any description) again. Line 4 encodes the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence. An example sequence in fastq format: @SEQ_ID GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT + ! '' * (((( ***+ )) %%%++ )( %%%% ) .1 ***-+* '' )) ** 55 CCF >>>>>> CCCCCCC65 Quality \u00b6 The quality, also called phred score, is the probability that the corresponding basecall is incorrect. Phred scores use a logarithmic scale, and are represented by ASCII characters, mapping to a quality usually going from 0 to 40. Phred Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% 50 1 in 100,000 99.999% 60 1 in 1,000,000 99.9999% the sam/bam format \u00b6 From Wikipedia : SAM (file format) is a text-based format for storing biological sequences aligned to a reference sequence developed by Heng Li. The acronym SAM stands for Sequence Alignment/Map. It is widely used for storing data, such as nucleotide sequences, generated by Next generation sequencing technologies and usually mapped to a reference. The SAM format consists of a header and an alignment section. The binary representation of a SAM file is a BAM file, which is a compressed SAM file.[1] SAM files can be analysed and edited with the software SAMtools. The SAM format has a really extensive and complex specification that you can find here . In brief it consists of a header section and reads (with other information) in tab delimited format. Example header section \u00b6 @HD VN : 1.0 SO : unsorted @SQ SN : O_volvulusOVOC_OM1a LN : 2816604 @SQ SN : O_volvulusOVOC_OM1b LN : 28345163 @SQ SN : O_volvulusOVOC_OM2 LN : 25485961 Example read \u00b6 M01137 : 130 : 00 - A : 17009 : 1352 / 14 * 0 0 * * 0 0 AGCAAAATACAACGATCTGGATGGTAGCATTAGCGATGCGACACTGCTTGAACCGTCAAAG FGGFGCFGFFGC8 ,, @ D ? E6EFCF ,= AEFFGGDGGGADFGG @ > FFEGGG :+< 7 D > AFCFGG YT : Z : UU the vcf format \u00b6 The vcf format is also a text-based file format. VCF stands for Variant Call Format and is used to store gene sequence variations (SNVs, indels). The format has been developped for genotyping projects, and is the standard to represent variations in the genome of a species. A vcf is a tab-delimited file, described here . VCF Example \u00b6 ## fileformat = VCFv4 . 0 ## fileDate = 20110705 ## reference = 1000 GenomesPilot - NCBI37 ## phasing = partial ## INFO =< ID = NS , Number = 1 , Type = Integer , Description = \"Number of Samples With Data\" > ## INFO =< ID = DP , Number = 1 , Type = Integer , Description = \"Total Depth\" > ## INFO =< ID = AF , Number = ., Type = Float , Description = \"Allele Frequency\" > ## INFO =< ID = AA , Number = 1 , Type = String , Description = \"Ancestral Allele\" > ## INFO =< ID = DB , Number = 0 , Type = Flag , Description = \"dbSNP membership, build 129\" > ## INFO =< ID = H2 , Number = 0 , Type = Flag , Description = \"HapMap2 membership\" > ## FILTER =< ID = q10 , Description = \"Quality below 10\" > ## FILTER =< ID = s50 , Description = \"Less than 50% of samples have data\" > ## FORMAT =< ID = GQ , Number = 1 , Type = Integer , Description = \"Genotype Quality\" > ## FORMAT =< ID = GT , Number = 1 , Type = String , Description = \"Genotype\" > ## FORMAT =< ID = DP , Number = 1 , Type = Integer , Description = \"Read Depth\" > ## FORMAT =< ID = HQ , Number = 2 , Type = Integer , Description = \"Haplotype Quality\" > # CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample1 Sample2 Sample3 2 4370 rs6057 G A 29 . NS = 2 ; DP = 13 ; AF = 0 . 5 ; DB ; H2 GT : GQ : DP : HQ 0 | 0 : 48 : 1 : 52 , 51 1 | 0 : 48 : 8 : 51 , 51 1 / 1 : 43 : 5 :.,. 2 7330 . T A 3 q10 NS = 5 ; DP = 12 ; AF = 0 . 017 GT : GQ : DP : HQ 0 | 0 : 46 : 3 : 58 , 50 0 | 1 : 3 : 5 : 65 , 3 0 / 0 : 41 : 3 2 110696 rs6055 A G , T 67 PASS NS = 2 ; DP = 10 ; AF = 0 . 333 , 0 . 667 ; AA = T ; DB GT : GQ : DP : HQ 1 | 2 : 21 : 6 : 23 , 27 2 | 1 : 2 : 0 : 18 , 2 2 / 2 : 35 : 4 2 130237 . T . 47 . NS = 2 ; DP = 16 ; AA = T GT : GQ : DP : HQ 0 | 0 : 54 : 7 : 56 , 60 0 | 0 : 48 : 4 : 56 , 51 0 / 0 : 61 : 2 2 134567 microsat1 GTCT G , GTACT 50 PASS NS = 2 ; DP = 9 ; AA = G GT : GQ : DP 0 / 1 : 35 : 4 0 / 2 : 17 : 2 1 / 1 : 40 : 3 chr1 45796269 . G C chr1 45797505 . C G chr1 45798555 . T C chr1 45798901 . C T chr1 45805566 . G C chr2 47703379 . C T chr2 48010488 . G A chr2 48030838 . A T chr2 48032875 . CTAT - chr2 48032937 . T C chr2 48033273 . TTTTTGTTTTAATTCCT - chr2 48033551 . C G chr2 48033910 . A T chr2 215632048 . G T chr2 215632125 . TT - chr2 215632155 . T C chr2 215632192 . G A chr2 215632255 . CA TG chr2 215634055 . C T the gff format \u00b6 The general feature format (gff) is another text file format, used for describing genes and other features of DNA, RNA and protein sequences. It is the standard for annotation of genomes. A gff file should contain 9 columns, described here Example gff \u00b6 ##description : evidence - based annotation of the human genome ( GRCh38 ), version 25 ( Ensembl 85 ) ##provider : GENCODE ##contact : gencode - help @sanger . ac . uk ##format : gtf ##date : 2016 - 07 - 15 chr1 HAVANA gene 11869 14409 . + . gene_id \"ENSG00000223972.5\" ; gene_type \"transcribed_unprocessed_pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; level 2 ; havana_gene \"OTTHUMG00000000961.2\" ; chr1 HAVANA transcript 11869 14409 . + . gene_id \"ENSG00000223972.5\" ; transcript_id \"ENST00000456328.2\" ; gene_type \"transcribed_unprocessed_pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"processed_transcript\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1-002\" ; level 2 ; transcript_support_level \"1\" ; tag \"basic\" ; havana_gene \"OTTHUMG00000000961.2\" ; havana_transcript \"OTTHUMT00000362751.1\" ; chr1 HAVANA exon 11869 12227 . + . gene_id \"ENSG00000223972.5\" ; transcript_id \"ENST00000456328.2\" ; gene_type \"transcribed_unprocessed_pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"processed_transcript\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1-002\" ; exon_number 1 ; exon_id \"ENSE00002234944.1\" ; level 2 ; transcript_support_level \"1\" ; tag \"basic\" ; havana_gene \"OTTHUMG00000000961.2\" ; havana_transcript \"OTTHUMT00000362751.1\" ; chr1 HAVANA exon 12613 12721 . + . gene_id \"ENSG00000223972.5\" ; transcript_id \"ENST00000456328.2\" ; gene_type \"transcribed_unprocessed_pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"processed_transcript\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1-002\" ; exon_number 2 ; exon_id \"ENSE00003582793.1\" ; level 2 ; transcript_support_level \"1\" ; tag \"basic\" ; havana_gene \"OTTHUMG00000000961.2\" ; havana_transcript \"OTTHUMT00000362751.1\" ; chr1 HAVANA exon 13221 14409 . + . gene_id \"ENSG00000223972.5\" ; transcript_id \"ENST00000456328.2\" ; gene_type \"transcribed_unprocessed_pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"processed_transcript\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1-002\" ; exon_number 3 ; exon_id \"ENSE00002312635.1\" ; level 2 ; transcript_support_level \"1\" ; tag \"basic\" ; havana_gene \"OTTHUMG00000000961.2\" ; havana_transcript \"OTTHUMT00000362751.1\" ;","title":"File Formats"},{"location":"file_formats/#file-formats","text":"This lecture is aimed at making you discover the most popular file formats used in bioinformatics. You're expected to have basic working knowledge of Linux to be able to follow the lesson.","title":"File Formats"},{"location":"file_formats/#table-of-contents","text":"The fasta format The fastq format The sam/bam format The vcf format The gff format","title":"Table of Contents"},{"location":"file_formats/#the-fasta-format","text":"The fasta format was invented in 1988 and designed to represent nucleotide or peptide sequences. It originates from the FASTA software package, but is now a standard in the world of bioinformatics. The first line in a FASTA file starts with a \">\" (greater-than) symbol followed by the description or identifier of the sequence. Following the initial line (used for a unique description of the sequence) is the actual sequence itself in standard one-letter code. A few sample sequences: > KX580312 . 1 Homo sapiens truncated breast cancer 1 ( BRCA1 ) gene , exon 15 and partial cds GTCATCCCCTTCTAAATGCCCATCATTAGATGATAGGTGGTACATGCACAGTTGCTCTGGGAGTCTTCAG AATAGAAACTACCCATCTCAAGAGGAGCTCATTAAGGTTGTTGATGTGGAGGAGTAACAGCTGGAAGAGT CTGGGCCACACGATTTGACGGAAACATCTTACTTGCCAAGGCAAGATCTAG > KRN06561 . 1 heat shock [ Lactobacillus sucicola DSM 21376 = JCM 15457 ] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE A fasta file can contain multiple sequence. Each sequence will be separated by their \"header\" line, starting by \">\". Example: > KRN06561 . 1 heat shock [ Lactobacillus sucicola DSM 21376 = JCM 15457 ] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE > 3 HHU_A Chain A , Human Heat - Shock Protein 90 ( Hsp90 ) MPEETQTQDQPMEEEEVETFAFQAEIAQLMSLIINTFYSNKEIFLRELISNSSDALDKIRYESLTDPSKL DSGKELHINLIPNKQDRTLTIVDTGIGMTKADLINNLGTIAKSGTKAFMEALQAGADISMIGQFGVGFYS AYLVAEKVTVITKHNDDEQYAWESSAGGSFTVRTDTGEPMGRGTKVILHLKEDQTEYLEERRIKEIVKKH SQFIGYPITLFVEK","title":"The fasta format"},{"location":"file_formats/#the-fastq-format","text":"The fastq format is also a text based format to represent nucleotide sequences, but also contains the corresponding quality of each nucleotide. It is the standard for storing the output of high-throughput sequencing instruments such as the Illumina machines. A fastq file uses four lines per sequence: Line 1 begins with a '@' character and is followed by a sequence identifier and an optional description (like a FASTA title line). Line 2 is the raw sequence letters. Line 3 begins with a '+' character and is optionally followed by the same sequence identifier (and any description) again. Line 4 encodes the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence. An example sequence in fastq format: @SEQ_ID GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT + ! '' * (((( ***+ )) %%%++ )( %%%% ) .1 ***-+* '' )) ** 55 CCF >>>>>> CCCCCCC65","title":"The fastq format"},{"location":"file_formats/#quality","text":"The quality, also called phred score, is the probability that the corresponding basecall is incorrect. Phred scores use a logarithmic scale, and are represented by ASCII characters, mapping to a quality usually going from 0 to 40. Phred Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% 50 1 in 100,000 99.999% 60 1 in 1,000,000 99.9999%","title":"Quality"},{"location":"file_formats/#the-sambam-format","text":"From Wikipedia : SAM (file format) is a text-based format for storing biological sequences aligned to a reference sequence developed by Heng Li. The acronym SAM stands for Sequence Alignment/Map. It is widely used for storing data, such as nucleotide sequences, generated by Next generation sequencing technologies and usually mapped to a reference. The SAM format consists of a header and an alignment section. The binary representation of a SAM file is a BAM file, which is a compressed SAM file.[1] SAM files can be analysed and edited with the software SAMtools. The SAM format has a really extensive and complex specification that you can find here . In brief it consists of a header section and reads (with other information) in tab delimited format.","title":"the sam/bam format"},{"location":"file_formats/#example-header-section","text":"@HD VN : 1.0 SO : unsorted @SQ SN : O_volvulusOVOC_OM1a LN : 2816604 @SQ SN : O_volvulusOVOC_OM1b LN : 28345163 @SQ SN : O_volvulusOVOC_OM2 LN : 25485961","title":"Example header section"},{"location":"file_formats/#example-read","text":"M01137 : 130 : 00 - A : 17009 : 1352 / 14 * 0 0 * * 0 0 AGCAAAATACAACGATCTGGATGGTAGCATTAGCGATGCGACACTGCTTGAACCGTCAAAG FGGFGCFGFFGC8 ,, @ D ? E6EFCF ,= AEFFGGDGGGADFGG @ > FFEGGG :+< 7 D > AFCFGG YT : Z : UU","title":"Example read"},{"location":"file_formats/#the-vcf-format","text":"The vcf format is also a text-based file format. VCF stands for Variant Call Format and is used to store gene sequence variations (SNVs, indels). The format has been developped for genotyping projects, and is the standard to represent variations in the genome of a species. A vcf is a tab-delimited file, described here .","title":"the vcf format"},{"location":"file_formats/#vcf-example","text":"## fileformat = VCFv4 . 0 ## fileDate = 20110705 ## reference = 1000 GenomesPilot - NCBI37 ## phasing = partial ## INFO =< ID = NS , Number = 1 , Type = Integer , Description = \"Number of Samples With Data\" > ## INFO =< ID = DP , Number = 1 , Type = Integer , Description = \"Total Depth\" > ## INFO =< ID = AF , Number = ., Type = Float , Description = \"Allele Frequency\" > ## INFO =< ID = AA , Number = 1 , Type = String , Description = \"Ancestral Allele\" > ## INFO =< ID = DB , Number = 0 , Type = Flag , Description = \"dbSNP membership, build 129\" > ## INFO =< ID = H2 , Number = 0 , Type = Flag , Description = \"HapMap2 membership\" > ## FILTER =< ID = q10 , Description = \"Quality below 10\" > ## FILTER =< ID = s50 , Description = \"Less than 50% of samples have data\" > ## FORMAT =< ID = GQ , Number = 1 , Type = Integer , Description = \"Genotype Quality\" > ## FORMAT =< ID = GT , Number = 1 , Type = String , Description = \"Genotype\" > ## FORMAT =< ID = DP , Number = 1 , Type = Integer , Description = \"Read Depth\" > ## FORMAT =< ID = HQ , Number = 2 , Type = Integer , Description = \"Haplotype Quality\" > # CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample1 Sample2 Sample3 2 4370 rs6057 G A 29 . NS = 2 ; DP = 13 ; AF = 0 . 5 ; DB ; H2 GT : GQ : DP : HQ 0 | 0 : 48 : 1 : 52 , 51 1 | 0 : 48 : 8 : 51 , 51 1 / 1 : 43 : 5 :.,. 2 7330 . T A 3 q10 NS = 5 ; DP = 12 ; AF = 0 . 017 GT : GQ : DP : HQ 0 | 0 : 46 : 3 : 58 , 50 0 | 1 : 3 : 5 : 65 , 3 0 / 0 : 41 : 3 2 110696 rs6055 A G , T 67 PASS NS = 2 ; DP = 10 ; AF = 0 . 333 , 0 . 667 ; AA = T ; DB GT : GQ : DP : HQ 1 | 2 : 21 : 6 : 23 , 27 2 | 1 : 2 : 0 : 18 , 2 2 / 2 : 35 : 4 2 130237 . T . 47 . NS = 2 ; DP = 16 ; AA = T GT : GQ : DP : HQ 0 | 0 : 54 : 7 : 56 , 60 0 | 0 : 48 : 4 : 56 , 51 0 / 0 : 61 : 2 2 134567 microsat1 GTCT G , GTACT 50 PASS NS = 2 ; DP = 9 ; AA = G GT : GQ : DP 0 / 1 : 35 : 4 0 / 2 : 17 : 2 1 / 1 : 40 : 3 chr1 45796269 . G C chr1 45797505 . C G chr1 45798555 . T C chr1 45798901 . C T chr1 45805566 . G C chr2 47703379 . C T chr2 48010488 . G A chr2 48030838 . A T chr2 48032875 . CTAT - chr2 48032937 . T C chr2 48033273 . TTTTTGTTTTAATTCCT - chr2 48033551 . C G chr2 48033910 . A T chr2 215632048 . G T chr2 215632125 . TT - chr2 215632155 . T C chr2 215632192 . G A chr2 215632255 . CA TG chr2 215634055 . C T","title":"VCF Example"},{"location":"file_formats/#the-gff-format","text":"The general feature format (gff) is another text file format, used for describing genes and other features of DNA, RNA and protein sequences. It is the standard for annotation of genomes. A gff file should contain 9 columns, described here","title":"the gff format"},{"location":"file_formats/#example-gff","text":"##description : evidence - based annotation of the human genome ( GRCh38 ), version 25 ( Ensembl 85 ) ##provider : GENCODE ##contact : gencode - help @sanger . ac . uk ##format : gtf ##date : 2016 - 07 - 15 chr1 HAVANA gene 11869 14409 . + . gene_id \"ENSG00000223972.5\" ; gene_type \"transcribed_unprocessed_pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; level 2 ; havana_gene \"OTTHUMG00000000961.2\" ; chr1 HAVANA transcript 11869 14409 . + . gene_id \"ENSG00000223972.5\" ; transcript_id \"ENST00000456328.2\" ; gene_type \"transcribed_unprocessed_pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"processed_transcript\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1-002\" ; level 2 ; transcript_support_level \"1\" ; tag \"basic\" ; havana_gene \"OTTHUMG00000000961.2\" ; havana_transcript \"OTTHUMT00000362751.1\" ; chr1 HAVANA exon 11869 12227 . + . gene_id \"ENSG00000223972.5\" ; transcript_id \"ENST00000456328.2\" ; gene_type \"transcribed_unprocessed_pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"processed_transcript\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1-002\" ; exon_number 1 ; exon_id \"ENSE00002234944.1\" ; level 2 ; transcript_support_level \"1\" ; tag \"basic\" ; havana_gene \"OTTHUMG00000000961.2\" ; havana_transcript \"OTTHUMT00000362751.1\" ; chr1 HAVANA exon 12613 12721 . + . gene_id \"ENSG00000223972.5\" ; transcript_id \"ENST00000456328.2\" ; gene_type \"transcribed_unprocessed_pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"processed_transcript\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1-002\" ; exon_number 2 ; exon_id \"ENSE00003582793.1\" ; level 2 ; transcript_support_level \"1\" ; tag \"basic\" ; havana_gene \"OTTHUMG00000000961.2\" ; havana_transcript \"OTTHUMT00000362751.1\" ; chr1 HAVANA exon 13221 14409 . + . gene_id \"ENSG00000223972.5\" ; transcript_id \"ENST00000456328.2\" ; gene_type \"transcribed_unprocessed_pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"processed_transcript\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1-002\" ; exon_number 3 ; exon_id \"ENSE00002312635.1\" ; level 2 ; transcript_support_level \"1\" ; tag \"basic\" ; havana_gene \"OTTHUMG00000000961.2\" ; havana_transcript \"OTTHUMT00000362751.1\" ;","title":"Example gff"},{"location":"mapping/","text":"Mapping and Variant Calling \u00b6 In this practical you will learn to map NGS reads to a reference sequence, check the output using a viewer software and investigate some aspects of the results. You will be using the read data from the Quality Control practical. EHEC O157 strains generally carry a large virulence plasmid, pO157. Plasmids are circular genetic elements that many bacteria carry in addition to their chromosomes. This particular plasmid encodes a number of proteins which are known or suspected to be involved in the ability to cause severe disease in infected humans. Your task in this practical is to map your prepared read set to a reference sequence of the virulence plasmid, to determine if the pO157 plasmid is present in the St. Louis outbreak strain. Illustration of plasmid integration into a host bacteria Downloading a Reference \u00b6 You will need a reference sequence to map your reads to. cd ~/work curl -O -J -L https://osf.io/rnzbe/download This file contains the sequence of the pO157 plasmid from the Sakai outbreak strain of E. coli O157. In contrast to the strain we are working on, this strain is available as a finished genome, i.e. the whole sequence of both the single chromosome and the large virulence plasmid are known. Indexing the reference \u00b6 Before aligning the reads against a reference, it is necessary to build an index of that reference bowtie2-build pO157_Sakai.fasta.gz pO157_Sakai Note Indexing the reference is a necessary pre-processing step that makes searching for patterns much much faster. Many popular aligners such as Bowtie and BWA use an algorithm called the Burrows\u2013Wheeler transform to build the index. Aligning reads \u00b6 Now we are ready to map our reads bowtie2 -x pO157_Sakai -1 SRR957824_trimmed_R1.fastq.gz \\ -2 SRR957824_trimmed_R2.fastq.gz -S SRR957824.sam The output of the mapping will be in the SAM format. You can find a brief explanation of the SAM format here Note In this tutorial as well as many other places, you'll often see the terms mapping and alignment being used interchangeably. If you want to read more about the difference between the two, I invite you to read this excellent Biostars discussion Visualising with tview \u00b6 head SRR957824.sam But it is not very informative. We'll use samtools to visualise our data Before downloading the data in tablet, we have to convert our SAM file into BAM, a compressed version of SAM that can be indexed. samtools view -hSbo SRR957824.bam SRR957824.sam Sort the bam file per position in the genome and index it samtools sort SRR957824.bam SRR2584857.sorted.bam samtools index SRR2584857.sorted.bam Finally we can visualise with samtools tview samtools tview SRR2584857.sorted.bam pO157_Sakai.fasta.gz Tip navigate in tview: - left and right arrows scroll - q to quit - CTRL-h and CTRL-l scrolls more - g gi|10955266|ref|NC_002128.1|:8000 will take you to a specific location. Variant Calling \u00b6 A frequent application for mapping reads is variant calling, i.e. finding positions where the reads are systematically different from the reference genome. Single nucleotide polymorphism (SNP)-based typing is particularly popular and used for a broad range of applications. For an EHEC O157 outbreak you could use it to identify the source, for instance. We can call the variants using samtools mpileup samtools mpileup -uD -f pO157_Sakai.fasta.gz SRR2584857.sorted.bam | \\ bcftools view - > variants.vcf You can read about the structure of vcf files here . The documentation is quite painful to read and take a look at the file Look at the non-commented lines grep -v ^## variants.vcf The first five columns are CHROM POS ID REF ALT . Use grep -v ^## variants.vcf | less -S for a better view. Tip Use your left and right arrows to scroll horizontally, and q to quit. Question How many SNPs did the variant caller find? Did you find any indels? Examine one of the variants with tview samtools tview SRR2584857.sorted.bam pO157_Sakai.fasta.gz \\ -p 'gi|10955266|ref|NC_002128.1|:43071' That seems very real! Question Where do reference genomes come from?","title":"Mapping and Variant Calling"},{"location":"mapping/#mapping-and-variant-calling","text":"In this practical you will learn to map NGS reads to a reference sequence, check the output using a viewer software and investigate some aspects of the results. You will be using the read data from the Quality Control practical. EHEC O157 strains generally carry a large virulence plasmid, pO157. Plasmids are circular genetic elements that many bacteria carry in addition to their chromosomes. This particular plasmid encodes a number of proteins which are known or suspected to be involved in the ability to cause severe disease in infected humans. Your task in this practical is to map your prepared read set to a reference sequence of the virulence plasmid, to determine if the pO157 plasmid is present in the St. Louis outbreak strain. Illustration of plasmid integration into a host bacteria","title":"Mapping and Variant Calling"},{"location":"mapping/#downloading-a-reference","text":"You will need a reference sequence to map your reads to. cd ~/work curl -O -J -L https://osf.io/rnzbe/download This file contains the sequence of the pO157 plasmid from the Sakai outbreak strain of E. coli O157. In contrast to the strain we are working on, this strain is available as a finished genome, i.e. the whole sequence of both the single chromosome and the large virulence plasmid are known.","title":"Downloading a Reference"},{"location":"mapping/#indexing-the-reference","text":"Before aligning the reads against a reference, it is necessary to build an index of that reference bowtie2-build pO157_Sakai.fasta.gz pO157_Sakai Note Indexing the reference is a necessary pre-processing step that makes searching for patterns much much faster. Many popular aligners such as Bowtie and BWA use an algorithm called the Burrows\u2013Wheeler transform to build the index.","title":"Indexing the reference"},{"location":"mapping/#aligning-reads","text":"Now we are ready to map our reads bowtie2 -x pO157_Sakai -1 SRR957824_trimmed_R1.fastq.gz \\ -2 SRR957824_trimmed_R2.fastq.gz -S SRR957824.sam The output of the mapping will be in the SAM format. You can find a brief explanation of the SAM format here Note In this tutorial as well as many other places, you'll often see the terms mapping and alignment being used interchangeably. If you want to read more about the difference between the two, I invite you to read this excellent Biostars discussion","title":"Aligning reads"},{"location":"mapping/#visualising-with-tview","text":"head SRR957824.sam But it is not very informative. We'll use samtools to visualise our data Before downloading the data in tablet, we have to convert our SAM file into BAM, a compressed version of SAM that can be indexed. samtools view -hSbo SRR957824.bam SRR957824.sam Sort the bam file per position in the genome and index it samtools sort SRR957824.bam SRR2584857.sorted.bam samtools index SRR2584857.sorted.bam Finally we can visualise with samtools tview samtools tview SRR2584857.sorted.bam pO157_Sakai.fasta.gz Tip navigate in tview: - left and right arrows scroll - q to quit - CTRL-h and CTRL-l scrolls more - g gi|10955266|ref|NC_002128.1|:8000 will take you to a specific location.","title":"Visualising with tview"},{"location":"mapping/#variant-calling","text":"A frequent application for mapping reads is variant calling, i.e. finding positions where the reads are systematically different from the reference genome. Single nucleotide polymorphism (SNP)-based typing is particularly popular and used for a broad range of applications. For an EHEC O157 outbreak you could use it to identify the source, for instance. We can call the variants using samtools mpileup samtools mpileup -uD -f pO157_Sakai.fasta.gz SRR2584857.sorted.bam | \\ bcftools view - > variants.vcf You can read about the structure of vcf files here . The documentation is quite painful to read and take a look at the file Look at the non-commented lines grep -v ^## variants.vcf The first five columns are CHROM POS ID REF ALT . Use grep -v ^## variants.vcf | less -S for a better view. Tip Use your left and right arrows to scroll horizontally, and q to quit. Question How many SNPs did the variant caller find? Did you find any indels? Examine one of the variants with tview samtools tview SRR2584857.sorted.bam pO157_Sakai.fasta.gz \\ -p 'gi|10955266|ref|NC_002128.1|:43071' That seems very real! Question Where do reference genomes come from?","title":"Variant Calling"},{"location":"meta_assembly/","text":"Metagenome assembly and binning \u00b6 In this tutorial you'll learn how to inspect assemble metagenomic data and retrieve draft genomes from assembled metagenomes We'll use a mock community of 20 bacteria sequenced using the Illumina HiSeq. In reality the data were simulated using InSilicoSeq . The 20 bacteria in the dataset were selected from the Tara Ocean study that recovered 957 distinct Metagenome-assembled-genomes (or MAGs) that were previsouly unknown! (full list on figshare ) Getting the Data \u00b6 mkdir -p ~/data cd ~/data curl -O -J -L https://osf.io/th9z6/download curl -O -J -L https://osf.io/k6vme/download chmod -w tara_reads_R* Quality Control \u00b6 we'll use FastQC to check the quality of our data, as well as sickle for trimming the bad quality part of the reads. If you need a refresher on how and why to check the quality of sequence data, please check the Quality Control and Trimming tutorial mkdir -p ~/results cd ~/results ln -s ~/data/tara_reads_* . fastqc tara_reads_*.fastq.gz Question What is the average read length? The average quality? Question Compared to single genome sequencing, what graphs differ? Now we'll trim the reads using sickle sickle pe - f tara_reads_R1 . fastq . gz - r tara_reads_R2 . fastq . gz - t sanger \\ - o tara_trimmed_R1 . fastq - p tara_trimmed_R2 . fastq - s / dev / null Question How many reads were trimmed? Assembly \u00b6 Megahit will be used for the assembly. megahit - 1 tara_trimmed_R1 . fastq - 2 tara_trimmed_R2 . fastq - o tara_assembly the resulting assenmbly can be found under tara_assembly/final.contigs.fa . Question How many contigs does this assembly contain? Binning \u00b6 First we need to map the reads back against the assembly to get coverage information ln -s tara_assembly/final.contigs.fa . bowtie2-build final.contigs.fa final.contigs bowtie2 -x final.contigs -1 tara_reads_R1.fastq.gz -2 tara_reads_R2.fastq.gz | \\ samtools view -bS -o tara_to_sort.bam samtools sort tara_to_sort.bam -o tara.bam samtools index tara.bam then we run metabat runMetaBat.sh -m 1500 final.contigs.fa tara.bam mv final.contigs.fa.metabat-bins1500 metabat Question How many bins did we obtain? Checking the quality of the bins \u00b6 The first time you run checkm you have to create the database sudo checkm data setRoot ~/.local/data/checkm checkm lineage_wf -x fa metabat checkm/ checkm bin_qa_plot -x fa checkm metabat plots Question Which bins should we keep for downstream analysis? Note checkm can plot a lot of metrics. If you have time, check the manual and try to produce different plots Warning if checkm fails at the phylogeny step, it is likely that your vm doesn't have enough RAM. pplacer requires about 35G of RAM to place the bins in the tree of life. In that case, execute the following cd ~/results curl -O -J -L https://osf.io/xuzhn/download tar xzf checkm.tar.gz checkm qa checkm/lineage.ms checkm then plot the completeness checkm bin_qa_plot -x fa checkm metabat plots and take a look at plots/bin_qa_plot.png Further reading \u00b6 Recovery of nearly 8,000 metagenome-assembled genomes substantially expands the tree of life The reconstruction of 2,631 draft metagenome-assembled genomes from the global oceans","title":"Metagenome assembly"},{"location":"meta_assembly/#metagenome-assembly-and-binning","text":"In this tutorial you'll learn how to inspect assemble metagenomic data and retrieve draft genomes from assembled metagenomes We'll use a mock community of 20 bacteria sequenced using the Illumina HiSeq. In reality the data were simulated using InSilicoSeq . The 20 bacteria in the dataset were selected from the Tara Ocean study that recovered 957 distinct Metagenome-assembled-genomes (or MAGs) that were previsouly unknown! (full list on figshare )","title":"Metagenome assembly and binning"},{"location":"meta_assembly/#getting-the-data","text":"mkdir -p ~/data cd ~/data curl -O -J -L https://osf.io/th9z6/download curl -O -J -L https://osf.io/k6vme/download chmod -w tara_reads_R*","title":"Getting the Data"},{"location":"meta_assembly/#quality-control","text":"we'll use FastQC to check the quality of our data, as well as sickle for trimming the bad quality part of the reads. If you need a refresher on how and why to check the quality of sequence data, please check the Quality Control and Trimming tutorial mkdir -p ~/results cd ~/results ln -s ~/data/tara_reads_* . fastqc tara_reads_*.fastq.gz Question What is the average read length? The average quality? Question Compared to single genome sequencing, what graphs differ? Now we'll trim the reads using sickle sickle pe - f tara_reads_R1 . fastq . gz - r tara_reads_R2 . fastq . gz - t sanger \\ - o tara_trimmed_R1 . fastq - p tara_trimmed_R2 . fastq - s / dev / null Question How many reads were trimmed?","title":"Quality Control"},{"location":"meta_assembly/#assembly","text":"Megahit will be used for the assembly. megahit - 1 tara_trimmed_R1 . fastq - 2 tara_trimmed_R2 . fastq - o tara_assembly the resulting assenmbly can be found under tara_assembly/final.contigs.fa . Question How many contigs does this assembly contain?","title":"Assembly"},{"location":"meta_assembly/#binning","text":"First we need to map the reads back against the assembly to get coverage information ln -s tara_assembly/final.contigs.fa . bowtie2-build final.contigs.fa final.contigs bowtie2 -x final.contigs -1 tara_reads_R1.fastq.gz -2 tara_reads_R2.fastq.gz | \\ samtools view -bS -o tara_to_sort.bam samtools sort tara_to_sort.bam -o tara.bam samtools index tara.bam then we run metabat runMetaBat.sh -m 1500 final.contigs.fa tara.bam mv final.contigs.fa.metabat-bins1500 metabat Question How many bins did we obtain?","title":"Binning"},{"location":"meta_assembly/#checking-the-quality-of-the-bins","text":"The first time you run checkm you have to create the database sudo checkm data setRoot ~/.local/data/checkm checkm lineage_wf -x fa metabat checkm/ checkm bin_qa_plot -x fa checkm metabat plots Question Which bins should we keep for downstream analysis? Note checkm can plot a lot of metrics. If you have time, check the manual and try to produce different plots Warning if checkm fails at the phylogeny step, it is likely that your vm doesn't have enough RAM. pplacer requires about 35G of RAM to place the bins in the tree of life. In that case, execute the following cd ~/results curl -O -J -L https://osf.io/xuzhn/download tar xzf checkm.tar.gz checkm qa checkm/lineage.ms checkm then plot the completeness checkm bin_qa_plot -x fa checkm metabat plots and take a look at plots/bin_qa_plot.png","title":"Checking the quality of the bins"},{"location":"meta_assembly/#further-reading","text":"Recovery of nearly 8,000 metagenome-assembled genomes substantially expands the tree of life The reconstruction of 2,631 draft metagenome-assembled genomes from the global oceans","title":"Further reading"},{"location":"meta_assembly_2/","text":"Metagenome assembly and binning (continued) \u00b6 In the previous tutorial we have seen how to recover draft genomes from raw metagenomic data. But we do not know what kind of organisms we have yet! Identyfying an unknown species from a genome is not always easy, especially if the species has not been described before. In this tutorial, you'll learn a few ways of identyfying (or attempting to identidy) an unkown genome Getting the Data \u00b6 This tutorial focuses on one particular genome bin but you can download the necessary data by executing the code block below. mkdir -p ~/mag/data cd ~/mag/data curl -O -J -L https://osf.io/d65jm/download chmod -w *.fa Ribosomal RNA \u00b6 The simplest thing we can do is search our draft genome for rRNA genes. Since these genes are usually quite conserved across species/genera, it could give us a broad idea of our organism barrnap -o bin2_rrna.fa bin.2.fa cat bin2_rrna.fa Now, Use the online Blast service to search similar sequences to the rRNA we obtained. Assigning taxonomy to each contig \u00b6 We'll use diamond against the swissprot database for quickly assigning taxonomy to our contigs. First, we download and build the database curl -O ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz diamond makedb --in uniprot_sprot.fasta.gz --db uniprot_sprot -p 4 then we run diamond diamond blastx -p 4 -q bin.2.fa -f 6 -d uniprot_sprot.dmnd -o bin2_diamond.txt Question How did that go? Did we find anything meaningful? It is very possible the swissprot database is too small for finding meaningful hits for undersequenced / poorly known organisms. Let us try with another piece of software, kraken2 sudo mkdir /mnt/kraken2 cd /mnt/kraken2 sudo curl -O https://ccb.jhu.edu/software/kraken2/dl/minikraken2_v1_8GB.tgz sudo tar xf minikraken2_v1_8GB.tgz sudo chmod +r * cd - and then (don't forget to move back to your results directory if you haven't) kraken2 --memory-mapping --db /mnt/kraken2 --threads 4 --output bin2_kraken.txt --report bin2_kraken_report.txt metabat/bin.2.fa Question Does this confirm our initial diagnostic? What kind of organism do we have? Functional annotation \u00b6 Now that we have at least a genus for our organism, let us try to look at what it does: curl -O -J -L https://osf.io/7k5tv/download then we add functional categories to our genes using eggnog-mapper: emapper.py --data_dir /mnt/nog/ -d actNOG -i annotation/bin.2/bin.2.faa --no_refine -o bin2_NOG and we download bin2_NOG.emapper.annotations to our own computers for downstream analysis rpoB phylogeny \u00b6 Warning We're about to produce a gene tree. Phylogenetic trees based on one gene must be interpreted very carefully, and may not reflect the actual phylogeny of the species Coming soon","title":"Metagenome assembly (continued)"},{"location":"meta_assembly_2/#metagenome-assembly-and-binning-continued","text":"In the previous tutorial we have seen how to recover draft genomes from raw metagenomic data. But we do not know what kind of organisms we have yet! Identyfying an unknown species from a genome is not always easy, especially if the species has not been described before. In this tutorial, you'll learn a few ways of identyfying (or attempting to identidy) an unkown genome","title":"Metagenome assembly and binning (continued)"},{"location":"meta_assembly_2/#getting-the-data","text":"This tutorial focuses on one particular genome bin but you can download the necessary data by executing the code block below. mkdir -p ~/mag/data cd ~/mag/data curl -O -J -L https://osf.io/d65jm/download chmod -w *.fa","title":"Getting the Data"},{"location":"meta_assembly_2/#ribosomal-rna","text":"The simplest thing we can do is search our draft genome for rRNA genes. Since these genes are usually quite conserved across species/genera, it could give us a broad idea of our organism barrnap -o bin2_rrna.fa bin.2.fa cat bin2_rrna.fa Now, Use the online Blast service to search similar sequences to the rRNA we obtained.","title":"Ribosomal RNA"},{"location":"meta_assembly_2/#assigning-taxonomy-to-each-contig","text":"We'll use diamond against the swissprot database for quickly assigning taxonomy to our contigs. First, we download and build the database curl -O ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz diamond makedb --in uniprot_sprot.fasta.gz --db uniprot_sprot -p 4 then we run diamond diamond blastx -p 4 -q bin.2.fa -f 6 -d uniprot_sprot.dmnd -o bin2_diamond.txt Question How did that go? Did we find anything meaningful? It is very possible the swissprot database is too small for finding meaningful hits for undersequenced / poorly known organisms. Let us try with another piece of software, kraken2 sudo mkdir /mnt/kraken2 cd /mnt/kraken2 sudo curl -O https://ccb.jhu.edu/software/kraken2/dl/minikraken2_v1_8GB.tgz sudo tar xf minikraken2_v1_8GB.tgz sudo chmod +r * cd - and then (don't forget to move back to your results directory if you haven't) kraken2 --memory-mapping --db /mnt/kraken2 --threads 4 --output bin2_kraken.txt --report bin2_kraken_report.txt metabat/bin.2.fa Question Does this confirm our initial diagnostic? What kind of organism do we have?","title":"Assigning taxonomy to each contig"},{"location":"meta_assembly_2/#functional-annotation","text":"Now that we have at least a genus for our organism, let us try to look at what it does: curl -O -J -L https://osf.io/7k5tv/download then we add functional categories to our genes using eggnog-mapper: emapper.py --data_dir /mnt/nog/ -d actNOG -i annotation/bin.2/bin.2.faa --no_refine -o bin2_NOG and we download bin2_NOG.emapper.annotations to our own computers for downstream analysis","title":"Functional annotation"},{"location":"meta_assembly_2/#rpob-phylogeny","text":"Warning We're about to produce a gene tree. Phylogenetic trees based on one gene must be interpreted very carefully, and may not reflect the actual phylogeny of the species Coming soon","title":"rpoB phylogeny"},{"location":"metavir/","text":"Viral Metagenome from a dolphin sample: hunting for a disease causing virus \u00b6 In this tutorial you will learn how to investigate metagenomics data and retrieve draft genome from an assembled metagenome. We will use a real dataset published in 2017 in a study in dolphins, where fecal samples where prepared for viral metagenomics study. The dolphin had a self-limiting gastroenteritis of suspected viral origin. Getting the Data \u00b6 First, create an appropriate directory to put the data: mkdir -p ~/dolphin/data cd ~/dolphin/data You can download them from here: curl - O - J - L https : // osf . io / 4 x6qs / download curl - O - J - L https : // osf . io / z2xed / download Alternatively, your instructor will let you know where to get the dataset from. You should get 2 compressed files: Dol1_S19_L001_R1_001 . fastq . gz Dol1_S19_L001_R2_001 . fastq . gz Quality Control \u00b6 We will use FastQC to check the quality of our data, as well as fastp for trimming the bad quality part of the reads. If you need a refresher on how and why to check the quality of sequence data, please check the Quality Control and Trimming tutorial mkdir -p ~/dolphin/results cd ~/dolphin/results ln -s ~/dolphin/data/Dol1* . fastqc Dol1_*.fastq.gz Question What is the average read length? The average quality? Question Compared to single genome sequencing, which graphs differ? Quality control with Fastp \u00b6 We will removing the adapters and trim by quality. Now we run fastp our read files fastp -i Dol1_S19_L001_R1_001.fastq.gz -o Dol1_trimmed_R1.fastq \\ -I Dol1_S19_L001_R2_001.fastq.gz -O Dol1_trimmed_R2.fastq \\ --detect_adapter_for_pe --length_required 30 \\ --cut_front --cut_tail --cut_mean_quality 10 Check the html report produced. Question How many reads were trimmed? Removing the host sequences by mapping/aligning on the dolphin genome \u00b6 For this we will use Bowtie2. We have downloaded the genome of Tursiops truncatus from Ensembl (fasta file). Then we have run the following command to produce the indexes of the dolphin genome for Bowtie2 (do not run it, we have pre-calculated the results for you): bowtie2 - build Tursiops_truncatus . turTru1 . dna . toplevel . fa Tursiops_truncatus Because this step takes a while, we have precomputed the index files, you can get them from here: curl - O - J - L https : // osf . io / wfk9t / download First we will extract the bowtie indexes of the dolphin genome into our results directory: tar - xzvf host_genome . tar . gz Now we are ready to map our sequencing reads on the dolphin genome: bowtie2 - x host_genome / Tursiops_truncatus \\ - 1 Dol1_trimmed_R1 . fastq - 2 Dol1_trimmed_R2 . fastq \\ - S dol_map . sam --un-conc Dol_reads_unmapped.fastq --threads 4 Question How many reads mapped on the dolphin genome? Taxonomic classification of the trimmed reads \u00b6 We will use Kaiju for the classification of the produced contigs. As we are mainly interested in detecting the viral sequences in our dataset and we want to reduce the computing time and the memory needed, we will build a viruses-only database. # Kaiju needs to be installed cd mkdir -p databases/kaijudb cd databases/kaijudb makeDB.sh -v Once the database built, Kaiju tells us that we need only 3 files: Then other files and directories can be removed # Removing un-needed things rm -r genomes/ rm kaiju_db.bwt rm kaiju_db.sa rm merged.dmp rm kaiju_db.faa We are now ready to run Kaiju on our trimmed reads cd dolphin/results kaiju -t ~/databases/kaijudb/nodes.dmp -f ~/databases/kaijudb/kaiju_db.fmi \\ -i Dol_reads_unmapped.1.fastq -j Dol_reads_unmapped.2.fastq \\ -o Dol1_reads_kaiju.out In order to visualise the results, we will produce a Krona chart. This step requires to have KronaTools installed: conda install - c bioconda krona kaiju2krona -t ~/databases/kaijudb/nodes.dmp -n ~/databases/kaijudb/names.dmp \\ -i Dol1_reads_kaiju.out -o Dol1_reads_kaiju.krona -u ktImportText -o Dol1_reads_kaiju.krona.html Dol1_reads_kaiju.krona Note If we were interested in bacteria and had a databases containing them, we could also use the command kaijuReport to get a text summary. Unfortunately, it does not provide taxonomic levels for viruses. Then we copy the produced html file locally to visualise in our web browser. Assembly \u00b6 Megahit will be used for the de novo assembly of the metagenome. megahit - 1 Dol_reads_unmapped . 1 . fastq - 2 Dol_reads_unmapped . 2 . fastq - o assembly The resulting assembly can be found under assembly/final.contigs.fa . Question How many contigs does this assembly contain? Is there any long contig? Taxonomic classification of contigs \u00b6 We will use Kaiju again with the same viruses-only database for the classification of the produced contigs. cd assembly kaiju -t ~/databases/kaijudb/nodes.dmp -f ~/databases/kaijudb/kaiju_db.fmi \\ -i final.contigs.fa -o Dol1_contigs_kaiju.out Then we produce the Krona chart: kaiju2krona -t ~/databases/kaijudb/nodes.dmp -n ~/databases/kaijudb/names.dmp \\ -i Dol1_contigs_kaiju.out -o Dol1_contigs_kaiju.krona -u ktImportText -o Dol1_contigs_kaiju.krona.html Dol1_contigs_kaiju.krona Question Does the classification of contigs produce different results than the classification of reads? Extraction of the contig of interest \u00b6 Let's go for a little practice of your Unix skills! Question Find a way to to find the longest contig. They look like this: > k141_1 flag = 1 multi = 1 . 0000 len = 301 > k141_2 flag = 1 multi = 1 . 0000 len = 303 hint 1 : all lines containing '>' hint 2 : use grep and sed Once we have this file, we want to sort all the sequences headers by the sequence length (len=X): cat final.contigs.fa | grep \">\" | sed s/len = // | sort -k4n | tail -1 Question What is the size of the longest contig? Now that you have identified the sequence header or id of the longest contig, you want to save it to a fasta file. grep -i '>k141_XXX' -A 1 final.contigs.fa > longest_contig.fasta Note You need to replace the XXX by the correct header. The option -A 1 of grep allows to print 1 line additionally to the matching line (which enables to print the full sequence that corresponds to one line). Test with -A 2 (without the redirection to longest_contig.fasta) to see what happens. Now that you have identified the longest contig, you will check in Kaiju results what was the taxon assigned to this contig. Have a look at the file Dol1_contigs_kaiju.out . It is structured in 3 columns: classification status (C/U), sequence id, assigned TaxID. Question Identify the TaxID of the longest contig and search on NCBI Taxonomy database to which species it corresponds to. Genome annotation of the contig of interest \u00b6 Once the contig to annotate is extracted and saved in the file longest_contig.fasta, we will use Prokka to detect ORFs (Open Reading Frames) in order to predict genes and their resulting proteins. First, go to Uniprot database and retrieve a set of protein sequences belonging to adenoviruses. Save the file as adenovirus.faa and copy it in your results directory. prokka --outdir annotation --kingdom Viruses \\ --proteins adenovirus.faa longest_contig.fasta Question How many genes and proteins were predicted? Visualization and manual curation. \u00b6 If there is some time left, you can visualise the produced annotation (gff file) in Ugene or Artemis for example. Go further with proteins functions \u00b6 For the predicted proteins that are left \"hypotetical\", you can try running Interproscan on them to get more information on domains and motifs.","title":"Viral metagenomics"},{"location":"metavir/#viral-metagenome-from-a-dolphin-sample-hunting-for-a-disease-causing-virus","text":"In this tutorial you will learn how to investigate metagenomics data and retrieve draft genome from an assembled metagenome. We will use a real dataset published in 2017 in a study in dolphins, where fecal samples where prepared for viral metagenomics study. The dolphin had a self-limiting gastroenteritis of suspected viral origin.","title":"Viral Metagenome from a dolphin sample: hunting for a disease causing virus"},{"location":"metavir/#getting-the-data","text":"First, create an appropriate directory to put the data: mkdir -p ~/dolphin/data cd ~/dolphin/data You can download them from here: curl - O - J - L https : // osf . io / 4 x6qs / download curl - O - J - L https : // osf . io / z2xed / download Alternatively, your instructor will let you know where to get the dataset from. You should get 2 compressed files: Dol1_S19_L001_R1_001 . fastq . gz Dol1_S19_L001_R2_001 . fastq . gz","title":"Getting the Data"},{"location":"metavir/#quality-control","text":"We will use FastQC to check the quality of our data, as well as fastp for trimming the bad quality part of the reads. If you need a refresher on how and why to check the quality of sequence data, please check the Quality Control and Trimming tutorial mkdir -p ~/dolphin/results cd ~/dolphin/results ln -s ~/dolphin/data/Dol1* . fastqc Dol1_*.fastq.gz Question What is the average read length? The average quality? Question Compared to single genome sequencing, which graphs differ?","title":"Quality Control"},{"location":"metavir/#quality-control-with-fastp","text":"We will removing the adapters and trim by quality. Now we run fastp our read files fastp -i Dol1_S19_L001_R1_001.fastq.gz -o Dol1_trimmed_R1.fastq \\ -I Dol1_S19_L001_R2_001.fastq.gz -O Dol1_trimmed_R2.fastq \\ --detect_adapter_for_pe --length_required 30 \\ --cut_front --cut_tail --cut_mean_quality 10 Check the html report produced. Question How many reads were trimmed?","title":"Quality control with Fastp"},{"location":"metavir/#removing-the-host-sequences-by-mappingaligning-on-the-dolphin-genome","text":"For this we will use Bowtie2. We have downloaded the genome of Tursiops truncatus from Ensembl (fasta file). Then we have run the following command to produce the indexes of the dolphin genome for Bowtie2 (do not run it, we have pre-calculated the results for you): bowtie2 - build Tursiops_truncatus . turTru1 . dna . toplevel . fa Tursiops_truncatus Because this step takes a while, we have precomputed the index files, you can get them from here: curl - O - J - L https : // osf . io / wfk9t / download First we will extract the bowtie indexes of the dolphin genome into our results directory: tar - xzvf host_genome . tar . gz Now we are ready to map our sequencing reads on the dolphin genome: bowtie2 - x host_genome / Tursiops_truncatus \\ - 1 Dol1_trimmed_R1 . fastq - 2 Dol1_trimmed_R2 . fastq \\ - S dol_map . sam --un-conc Dol_reads_unmapped.fastq --threads 4 Question How many reads mapped on the dolphin genome?","title":"Removing the host sequences by mapping/aligning on the dolphin genome"},{"location":"metavir/#taxonomic-classification-of-the-trimmed-reads","text":"We will use Kaiju for the classification of the produced contigs. As we are mainly interested in detecting the viral sequences in our dataset and we want to reduce the computing time and the memory needed, we will build a viruses-only database. # Kaiju needs to be installed cd mkdir -p databases/kaijudb cd databases/kaijudb makeDB.sh -v Once the database built, Kaiju tells us that we need only 3 files: Then other files and directories can be removed # Removing un-needed things rm -r genomes/ rm kaiju_db.bwt rm kaiju_db.sa rm merged.dmp rm kaiju_db.faa We are now ready to run Kaiju on our trimmed reads cd dolphin/results kaiju -t ~/databases/kaijudb/nodes.dmp -f ~/databases/kaijudb/kaiju_db.fmi \\ -i Dol_reads_unmapped.1.fastq -j Dol_reads_unmapped.2.fastq \\ -o Dol1_reads_kaiju.out In order to visualise the results, we will produce a Krona chart. This step requires to have KronaTools installed: conda install - c bioconda krona kaiju2krona -t ~/databases/kaijudb/nodes.dmp -n ~/databases/kaijudb/names.dmp \\ -i Dol1_reads_kaiju.out -o Dol1_reads_kaiju.krona -u ktImportText -o Dol1_reads_kaiju.krona.html Dol1_reads_kaiju.krona Note If we were interested in bacteria and had a databases containing them, we could also use the command kaijuReport to get a text summary. Unfortunately, it does not provide taxonomic levels for viruses. Then we copy the produced html file locally to visualise in our web browser.","title":"Taxonomic classification of the trimmed reads"},{"location":"metavir/#assembly","text":"Megahit will be used for the de novo assembly of the metagenome. megahit - 1 Dol_reads_unmapped . 1 . fastq - 2 Dol_reads_unmapped . 2 . fastq - o assembly The resulting assembly can be found under assembly/final.contigs.fa . Question How many contigs does this assembly contain? Is there any long contig?","title":"Assembly"},{"location":"metavir/#taxonomic-classification-of-contigs","text":"We will use Kaiju again with the same viruses-only database for the classification of the produced contigs. cd assembly kaiju -t ~/databases/kaijudb/nodes.dmp -f ~/databases/kaijudb/kaiju_db.fmi \\ -i final.contigs.fa -o Dol1_contigs_kaiju.out Then we produce the Krona chart: kaiju2krona -t ~/databases/kaijudb/nodes.dmp -n ~/databases/kaijudb/names.dmp \\ -i Dol1_contigs_kaiju.out -o Dol1_contigs_kaiju.krona -u ktImportText -o Dol1_contigs_kaiju.krona.html Dol1_contigs_kaiju.krona Question Does the classification of contigs produce different results than the classification of reads?","title":"Taxonomic classification of contigs"},{"location":"metavir/#extraction-of-the-contig-of-interest","text":"Let's go for a little practice of your Unix skills! Question Find a way to to find the longest contig. They look like this: > k141_1 flag = 1 multi = 1 . 0000 len = 301 > k141_2 flag = 1 multi = 1 . 0000 len = 303 hint 1 : all lines containing '>' hint 2 : use grep and sed Once we have this file, we want to sort all the sequences headers by the sequence length (len=X): cat final.contigs.fa | grep \">\" | sed s/len = // | sort -k4n | tail -1 Question What is the size of the longest contig? Now that you have identified the sequence header or id of the longest contig, you want to save it to a fasta file. grep -i '>k141_XXX' -A 1 final.contigs.fa > longest_contig.fasta Note You need to replace the XXX by the correct header. The option -A 1 of grep allows to print 1 line additionally to the matching line (which enables to print the full sequence that corresponds to one line). Test with -A 2 (without the redirection to longest_contig.fasta) to see what happens. Now that you have identified the longest contig, you will check in Kaiju results what was the taxon assigned to this contig. Have a look at the file Dol1_contigs_kaiju.out . It is structured in 3 columns: classification status (C/U), sequence id, assigned TaxID. Question Identify the TaxID of the longest contig and search on NCBI Taxonomy database to which species it corresponds to.","title":"Extraction of the contig of interest"},{"location":"metavir/#genome-annotation-of-the-contig-of-interest","text":"Once the contig to annotate is extracted and saved in the file longest_contig.fasta, we will use Prokka to detect ORFs (Open Reading Frames) in order to predict genes and their resulting proteins. First, go to Uniprot database and retrieve a set of protein sequences belonging to adenoviruses. Save the file as adenovirus.faa and copy it in your results directory. prokka --outdir annotation --kingdom Viruses \\ --proteins adenovirus.faa longest_contig.fasta Question How many genes and proteins were predicted?","title":"Genome annotation of the contig of interest"},{"location":"metavir/#visualization-and-manual-curation","text":"If there is some time left, you can visualise the produced annotation (gff file) in Ugene or Artemis for example.","title":"Visualization and manual curation."},{"location":"metavir/#go-further-with-proteins-functions","text":"For the predicted proteins that are left \"hypotetical\", you can try running Interproscan on them to get more information on domains and motifs.","title":"Go further with proteins functions"},{"location":"nanopore/","text":"Introduction to Nanopore Sequencing \u00b6 In this tutorial we will assemble the E. coli genome using a mix of long, error-prone reads from the MinION (Oxford Nanopore) and short reads from a HiSeq instrument (Illumina). The MinION data used in this tutorial come a test run by the Loman lab . The Illumina data were simulated using InSilicoSeq Get the Data \u00b6 First download the nanopore data wget http://s3.climb.ac.uk/nanopore/ecoli_allreads.fasta You will not need the HiSeq data right away, but you can start the download in another window curl -O -J -L https://osf.io/pxk7f/download curl -O -J -L https://osf.io/zax3c/download look at basic stats of the nanopore reads assembly-stats ecoli_allreads.fasta Question How many nanopore reads do we have? Question How long is the longest read? Question What is the average read length? Adapter trimming \u00b6 The guppy basecaller, i.e. the program that transform raw electrical signal in fastq files, already demultiplex and trim for us. Assembly \u00b6 We assemble the reads using wtdbg2 (version > 2.3) head -n 20000 ecoli_allreads.fasta > subset.fasta wtdbg2 -x ont -i subset.fasta -fo assembly wtpoa-cns -i assembly.ctg.lay.gz -fo assembly.ctg.fa Polishing \u00b6 Since the assembly likely contains a lot of errors, we correct it with Illumina reads. First we map the short reads against the assembly bowtie2 - build assembly . ctg . fa assembly bowtie2 - x assembly - 1 ecoli_hiseq_R1 . fastq . gz - 2 ecoli_hiseq_R2 . fastq . gz | \\ samtools view - bS - o assembly_short_reads . bam samtools sort assembly_short_reads . bam - o assembly_short_sorted . bam samtools index assembly_short_sorted . bam then we run the consensus step samtools view assembly_short_sorted . bam | wtpoa - cns - t 16 - x sam - sr \\ - d assembly . ctg . fa - i - - fo assembly_polished . fasta which will correct eventual misamatches in our assembly and write the new improved assembly to assembly_polished.fasta For better results we should perform more than one round of polishing. Compare with the existing assembly and an illumina only assembly \u00b6 an existing assembly \u00b6 Go to https://www.ncbi.nlm.nih.gov and search for NC_000913. Download the associated genome in fasta format and rename it to ecoli_ref.fasta nucmer --maxmatch -c 100 -p ecoli assembly_polished.fasta ecoli_ref.fasta mummerplot --fat --filter --png --large -p ecoli ecoli.delta then take a look at ecoli.png compare metrics \u00b6 Note First you need to assemble the illumina data Then run busco and quast on the 3 assemblies Question which assembly would you say is the best? Annotation \u00b6 If you have time, train your annotation skills by running prokka on your genome! prokka --outdir annotation --kingdom Bacteria assembly_polished.fasta You can open the output to see how it went cat annotation/*.txt Question Does it fit your expectations? How many genes were you expecting?","title":"Introduction to Nanopore Sequencing"},{"location":"nanopore/#introduction-to-nanopore-sequencing","text":"In this tutorial we will assemble the E. coli genome using a mix of long, error-prone reads from the MinION (Oxford Nanopore) and short reads from a HiSeq instrument (Illumina). The MinION data used in this tutorial come a test run by the Loman lab . The Illumina data were simulated using InSilicoSeq","title":"Introduction to Nanopore Sequencing"},{"location":"nanopore/#get-the-data","text":"First download the nanopore data wget http://s3.climb.ac.uk/nanopore/ecoli_allreads.fasta You will not need the HiSeq data right away, but you can start the download in another window curl -O -J -L https://osf.io/pxk7f/download curl -O -J -L https://osf.io/zax3c/download look at basic stats of the nanopore reads assembly-stats ecoli_allreads.fasta Question How many nanopore reads do we have? Question How long is the longest read? Question What is the average read length?","title":"Get the Data"},{"location":"nanopore/#adapter-trimming","text":"The guppy basecaller, i.e. the program that transform raw electrical signal in fastq files, already demultiplex and trim for us.","title":"Adapter trimming"},{"location":"nanopore/#assembly","text":"We assemble the reads using wtdbg2 (version > 2.3) head -n 20000 ecoli_allreads.fasta > subset.fasta wtdbg2 -x ont -i subset.fasta -fo assembly wtpoa-cns -i assembly.ctg.lay.gz -fo assembly.ctg.fa","title":"Assembly"},{"location":"nanopore/#polishing","text":"Since the assembly likely contains a lot of errors, we correct it with Illumina reads. First we map the short reads against the assembly bowtie2 - build assembly . ctg . fa assembly bowtie2 - x assembly - 1 ecoli_hiseq_R1 . fastq . gz - 2 ecoli_hiseq_R2 . fastq . gz | \\ samtools view - bS - o assembly_short_reads . bam samtools sort assembly_short_reads . bam - o assembly_short_sorted . bam samtools index assembly_short_sorted . bam then we run the consensus step samtools view assembly_short_sorted . bam | wtpoa - cns - t 16 - x sam - sr \\ - d assembly . ctg . fa - i - - fo assembly_polished . fasta which will correct eventual misamatches in our assembly and write the new improved assembly to assembly_polished.fasta For better results we should perform more than one round of polishing.","title":"Polishing"},{"location":"nanopore/#compare-with-the-existing-assembly-and-an-illumina-only-assembly","text":"","title":"Compare with the existing assembly and an illumina only assembly"},{"location":"nanopore/#an-existing-assembly","text":"Go to https://www.ncbi.nlm.nih.gov and search for NC_000913. Download the associated genome in fasta format and rename it to ecoli_ref.fasta nucmer --maxmatch -c 100 -p ecoli assembly_polished.fasta ecoli_ref.fasta mummerplot --fat --filter --png --large -p ecoli ecoli.delta then take a look at ecoli.png","title":"an existing assembly"},{"location":"nanopore/#compare-metrics","text":"Note First you need to assemble the illumina data Then run busco and quast on the 3 assemblies Question which assembly would you say is the best?","title":"compare metrics"},{"location":"nanopore/#annotation","text":"If you have time, train your annotation skills by running prokka on your genome! prokka --outdir annotation --kingdom Bacteria assembly_polished.fasta You can open the output to see how it went cat annotation/*.txt Question Does it fit your expectations? How many genes were you expecting?","title":"Annotation"},{"location":"pan_genome/","text":"Pan-Genome Analysis \u00b6 Lecture \u00b6 In this tutorial we will learn how to determine a pan-genome from a collection of isolate genomes. This tutorial is inspired from Genome annotation and Pangenome Analysis from the CBIB in Santiago, Chile. Don't forget to activate the conda environment! \u00b6 conda activate tutorials Getting the data \u00b6 We'll get data from this article and analyse the core and accessory genomes of E. coli . You can download raw data (paired-end reads) used to assemble a selection of the above-mentioned strains: curl -OL https://www.dropbox.com/s/ef4tuc2qby9tcmn/all_datasets.tar.gz # uncompress folder tar -xvzf all_datasets.tar.gz and then put all the reads in the same directory mkdir reads mv ERR*.fastq.gz reads/ rm -r ERR* Assemble and Annotate the strains \u00b6 You'll assemble your strains with megahit. mkdir assemblies for r1 in reads/*_1.fastq.gz do prefix = $( basename $r1 _1.fastq.gz ) r2 = reads/ ${ prefix } _2.fastq.gz megahit -1 $r1 -2 $r2 -o ${ prefix } --out-prefix ${ prefix } mv ${ prefix } / ${ prefix } .contigs.fa assemblies/ rm -r ${ prefix } done and use prokka to annotate mkdir annotation for assembly in assemblies/*.fa do prefix = $( basename $assembly .contigs.fa ) prokka --usegenus --genus Escherichia --species coli --strain ${ prefix } \\ --outdir ${ prefix } --prefix ${ prefix } ${ assembly } mv ${ prefix } / ${ prefix } .gff annotation/ rm -r ${ prefix } done Pan-genome analysis \u00b6 We will use Roary to run pan-genome analysis. Our analysis will also include running a tool to build trees, Fasttree. If running roary and FastTree gives an error, please install the two tools within your conda environment with conda install -c bioconda roary cpanm -f Bio::Roary conda install -c bioconda fasttree Run Roary roary -f roary -e -n -v annotation/*.gff Roary will get all the coding sequences, convert them into protein, and create pre-clusters. Then, using BLASTP and MCL, Roary will create clusters, and check for paralogs. Finally, Roary will take every isolate and order them by presence/absence of orthologs. The summary output is present in the summary_statistics.txt file. Additionally, Roary produces a gene_presence_absence.csv file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not. Plotting the result \u00b6 Roary comes with a python script that allows you to generate a few plots to graphically assess your analysis output. First, we need to generate a tree file from the alignment generated by Roary: cd roary FastTreeMP -nt -gtr core_gene_alignment.aln > my_tree.newick Then we can plot the Roary results with roary_plots.py , a community contributed python script to visualise roary results: wget https://raw.githubusercontent.com/sanger-pathogens/Roary/master/contrib/roary_plots/roary_plots.py python roary_plots.py roary_plots.py my_tree.newick gene_presence_absence.csv then look at the 3 /png files that have been generated","title":"Pan-Genome Analysis"},{"location":"pan_genome/#pan-genome-analysis","text":"","title":"Pan-Genome Analysis"},{"location":"pan_genome/#lecture","text":"In this tutorial we will learn how to determine a pan-genome from a collection of isolate genomes. This tutorial is inspired from Genome annotation and Pangenome Analysis from the CBIB in Santiago, Chile.","title":"Lecture"},{"location":"pan_genome/#dont-forget-to-activate-the-conda-environment","text":"conda activate tutorials","title":"Don't forget to activate the conda environment!"},{"location":"pan_genome/#getting-the-data","text":"We'll get data from this article and analyse the core and accessory genomes of E. coli . You can download raw data (paired-end reads) used to assemble a selection of the above-mentioned strains: curl -OL https://www.dropbox.com/s/ef4tuc2qby9tcmn/all_datasets.tar.gz # uncompress folder tar -xvzf all_datasets.tar.gz and then put all the reads in the same directory mkdir reads mv ERR*.fastq.gz reads/ rm -r ERR*","title":"Getting the data"},{"location":"pan_genome/#assemble-and-annotate-the-strains","text":"You'll assemble your strains with megahit. mkdir assemblies for r1 in reads/*_1.fastq.gz do prefix = $( basename $r1 _1.fastq.gz ) r2 = reads/ ${ prefix } _2.fastq.gz megahit -1 $r1 -2 $r2 -o ${ prefix } --out-prefix ${ prefix } mv ${ prefix } / ${ prefix } .contigs.fa assemblies/ rm -r ${ prefix } done and use prokka to annotate mkdir annotation for assembly in assemblies/*.fa do prefix = $( basename $assembly .contigs.fa ) prokka --usegenus --genus Escherichia --species coli --strain ${ prefix } \\ --outdir ${ prefix } --prefix ${ prefix } ${ assembly } mv ${ prefix } / ${ prefix } .gff annotation/ rm -r ${ prefix } done","title":"Assemble and Annotate the strains"},{"location":"pan_genome/#pan-genome-analysis_1","text":"We will use Roary to run pan-genome analysis. Our analysis will also include running a tool to build trees, Fasttree. If running roary and FastTree gives an error, please install the two tools within your conda environment with conda install -c bioconda roary cpanm -f Bio::Roary conda install -c bioconda fasttree Run Roary roary -f roary -e -n -v annotation/*.gff Roary will get all the coding sequences, convert them into protein, and create pre-clusters. Then, using BLASTP and MCL, Roary will create clusters, and check for paralogs. Finally, Roary will take every isolate and order them by presence/absence of orthologs. The summary output is present in the summary_statistics.txt file. Additionally, Roary produces a gene_presence_absence.csv file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not.","title":"Pan-genome analysis"},{"location":"pan_genome/#plotting-the-result","text":"Roary comes with a python script that allows you to generate a few plots to graphically assess your analysis output. First, we need to generate a tree file from the alignment generated by Roary: cd roary FastTreeMP -nt -gtr core_gene_alignment.aln > my_tree.newick Then we can plot the Roary results with roary_plots.py , a community contributed python script to visualise roary results: wget https://raw.githubusercontent.com/sanger-pathogens/Roary/master/contrib/roary_plots/roary_plots.py python roary_plots.py roary_plots.py my_tree.newick gene_presence_absence.csv then look at the 3 /png files that have been generated","title":"Plotting the result"},{"location":"qc/","text":"Quality Control and Trimming \u00b6 Lecture \u00b6 Practical \u00b6 In this practical you will learn to import, view and check the quality of raw high thoughput sequencing sequencing data. The first dataset you will be working with is from an Illumina MiSeq dataset. The sequenced organism is an enterohaemorrhagic E. coli (EHEC) of the serotype O157, a potentially fatal gastrointestinal pathogen. The sequenced bacterium was part of an outbreak investigation in the St. Louis area, USA in 2011. The sequencing was done as paired-end 2x150bp. Downloading the data \u00b6 The raw data were deposited at the European Nucleotide Archive, under the accession number SRR957824. You could go to the ENA website and search for the run with the accession SRR957824. However these files contain about 3 million reads and are therefore quite big. We are only gonna use a subset of the original dataset for this tutorial. First create a data/ directory in your home folder mkdir ~/data now let's download the subset cd ~/data curl -O -J -L https://osf.io/shqpv/download curl -O -J -L https://osf.io/9m3ch/download Let\u2019s make sure we downloaded all of our data using md5sum. md5sum SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz you should see this 1 e8cf249e3217a5a0bcc0d8a654585fb SRR957824_500K_R1 . fastq . gz 70 c726a31f05f856fe942d727613adb7 SRR957824_500K_R2 . fastq . gz and now look at the file names and their size ls -l total 97 M - rw - r --r-- 1 hadrien 48M Nov 19 18:44 SRR957824_500K_R1.fastq.gz - rw - r --r-- 1 hadrien 50M Nov 19 18:53 SRR957824_500K_R2.fastq.gz There are 500 000 paired-end reads taken randomly from the original data One last thing before we get to the quality control: those files are writeable. By default, UNIX makes things writeable by the file owner. This poses an issue with creating typos or errors in raw data. We fix that before going further chmod u-w * Working Directory \u00b6 First we make a work directory: a directory where we can play around with a copy of the data without messing with the original mkdir ~/work cd ~/work Now we make a link of the data in our working directory ln -s ~/data/* . The files that we've downloaded are FASTQ files. Take a look at one of them with zless SRR957824_500K_R1.fastq.gz Tip Use the spacebar to scroll down, and type \u2018q\u2019 to exit \u2018less\u2019 You can read more on the FASTQ format in the File Formats lesson. Question Where does the filename come from? Question Why are there 1 and 2 in the file names? FastQC \u00b6 To check the quality of the sequence data we will use a tool called FastQC. FastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation. It is available here . However, FastQC is also available as a command line utility on the training server you are using. To run FastQC on our two files fastqc SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz and look what FastQC has produced ls * fastqc * For each file, FastQC has produced both a .zip archive containing all the plots, and a html report. Download and open the html files with your favourite web browser. Alternatively you can look a these copies of them: SRR957824_500K_R1_fastqc.html SRR957824_500K_R2_fastqc.html Question What should you pay attention to in the FastQC report? Question Which file is of better quality? Pay special attention to the per base sequence quality and sequence length distribution. Explanations for the various quality modules can be found here . Also, have a look at examples of a good and a bad illumina read set for comparison. You will note that the reads in your uploaded dataset have fairly poor quality (<20) towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads. Scythe \u00b6 Now we'll do some trimming! Scythe uses a Naive Bayesian approach to classify contaminant substrings in sequence reads. It considers quality information, which can make it robust in picking out 3'-end adapters, which often include poor quality bases. The first thing we need is the adapters to trim off curl -O -J -L https://osf.io/v24pt/download Now we run scythe on both our read files scythe -a adapters.fasta -o SRR957824_adapt_R1.fastq SRR957824_500K_R1.fastq.gz scythe -a adapters.fasta -o SRR957824_adapt_R2.fastq SRR957824_500K_R2.fastq.gz Question What adapters do you use? Sickle \u00b6 Most modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well. Incorrectly called bases in both regions negatively impact assembles, mapping, and downstream bioinformatics analyses. We will trim each read individually down to the good quality part to keep the bad part from interfering with downstream applications. To do so, we will use sickle. Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It will also discard reads based upon a length threshold. To run sickle sickle pe -f SRR957824_adapt_R1.fastq -r SRR957824_adapt_R2.fastq \\ -t sanger -o SRR957824_trimmed_R1.fastq -p SRR957824_trimmed_R2.fastq \\ -s /dev/null -q 25 which should output something like PE forward file : SRR957824_trimmed_R1 . fastq PE reverse file : SRR957824_trimmed_R2 . fastq Total input FastQ records : 1000000 ( 500000 pairs ) FastQ paired records kept : 834570 ( 417285 pairs ) FastQ single records kept : 13263 ( from PE1 : 11094 , from PE2 : 2169 ) FastQ paired records discarded : 138904 ( 69452 pairs ) FastQ single records discarded : 13263 ( from PE1 : 2169 , from PE2 : 11094 ) FastQC again \u00b6 Run fastqc again on the filtered reads fastqc SRR957824_trimmed_R1.fastq SRR957824_trimmed_R2.fastq and look at the reports SRR957824_trimmed_R1_fastqc.html SRR957824_trimmed_R2_fastqc.html MultiQC \u00b6 MultiQC is a tool that aggreagtes results from several popular QC bioinformatics software into one html report. Let's run MultiQC in our current directory multiqc . You can download the report or view it by clickinh on the link below multiqc_report.html Question What did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution?","title":"Quality Control and Trimming"},{"location":"qc/#quality-control-and-trimming","text":"","title":"Quality Control and Trimming"},{"location":"qc/#lecture","text":"","title":"Lecture"},{"location":"qc/#practical","text":"In this practical you will learn to import, view and check the quality of raw high thoughput sequencing sequencing data. The first dataset you will be working with is from an Illumina MiSeq dataset. The sequenced organism is an enterohaemorrhagic E. coli (EHEC) of the serotype O157, a potentially fatal gastrointestinal pathogen. The sequenced bacterium was part of an outbreak investigation in the St. Louis area, USA in 2011. The sequencing was done as paired-end 2x150bp.","title":"Practical"},{"location":"qc/#downloading-the-data","text":"The raw data were deposited at the European Nucleotide Archive, under the accession number SRR957824. You could go to the ENA website and search for the run with the accession SRR957824. However these files contain about 3 million reads and are therefore quite big. We are only gonna use a subset of the original dataset for this tutorial. First create a data/ directory in your home folder mkdir ~/data now let's download the subset cd ~/data curl -O -J -L https://osf.io/shqpv/download curl -O -J -L https://osf.io/9m3ch/download Let\u2019s make sure we downloaded all of our data using md5sum. md5sum SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz you should see this 1 e8cf249e3217a5a0bcc0d8a654585fb SRR957824_500K_R1 . fastq . gz 70 c726a31f05f856fe942d727613adb7 SRR957824_500K_R2 . fastq . gz and now look at the file names and their size ls -l total 97 M - rw - r --r-- 1 hadrien 48M Nov 19 18:44 SRR957824_500K_R1.fastq.gz - rw - r --r-- 1 hadrien 50M Nov 19 18:53 SRR957824_500K_R2.fastq.gz There are 500 000 paired-end reads taken randomly from the original data One last thing before we get to the quality control: those files are writeable. By default, UNIX makes things writeable by the file owner. This poses an issue with creating typos or errors in raw data. We fix that before going further chmod u-w *","title":"Downloading the data"},{"location":"qc/#working-directory","text":"First we make a work directory: a directory where we can play around with a copy of the data without messing with the original mkdir ~/work cd ~/work Now we make a link of the data in our working directory ln -s ~/data/* . The files that we've downloaded are FASTQ files. Take a look at one of them with zless SRR957824_500K_R1.fastq.gz Tip Use the spacebar to scroll down, and type \u2018q\u2019 to exit \u2018less\u2019 You can read more on the FASTQ format in the File Formats lesson. Question Where does the filename come from? Question Why are there 1 and 2 in the file names?","title":"Working Directory"},{"location":"qc/#fastqc","text":"To check the quality of the sequence data we will use a tool called FastQC. FastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation. It is available here . However, FastQC is also available as a command line utility on the training server you are using. To run FastQC on our two files fastqc SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz and look what FastQC has produced ls * fastqc * For each file, FastQC has produced both a .zip archive containing all the plots, and a html report. Download and open the html files with your favourite web browser. Alternatively you can look a these copies of them: SRR957824_500K_R1_fastqc.html SRR957824_500K_R2_fastqc.html Question What should you pay attention to in the FastQC report? Question Which file is of better quality? Pay special attention to the per base sequence quality and sequence length distribution. Explanations for the various quality modules can be found here . Also, have a look at examples of a good and a bad illumina read set for comparison. You will note that the reads in your uploaded dataset have fairly poor quality (<20) towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads.","title":"FastQC"},{"location":"qc/#scythe","text":"Now we'll do some trimming! Scythe uses a Naive Bayesian approach to classify contaminant substrings in sequence reads. It considers quality information, which can make it robust in picking out 3'-end adapters, which often include poor quality bases. The first thing we need is the adapters to trim off curl -O -J -L https://osf.io/v24pt/download Now we run scythe on both our read files scythe -a adapters.fasta -o SRR957824_adapt_R1.fastq SRR957824_500K_R1.fastq.gz scythe -a adapters.fasta -o SRR957824_adapt_R2.fastq SRR957824_500K_R2.fastq.gz Question What adapters do you use?","title":"Scythe"},{"location":"qc/#sickle","text":"Most modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well. Incorrectly called bases in both regions negatively impact assembles, mapping, and downstream bioinformatics analyses. We will trim each read individually down to the good quality part to keep the bad part from interfering with downstream applications. To do so, we will use sickle. Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It will also discard reads based upon a length threshold. To run sickle sickle pe -f SRR957824_adapt_R1.fastq -r SRR957824_adapt_R2.fastq \\ -t sanger -o SRR957824_trimmed_R1.fastq -p SRR957824_trimmed_R2.fastq \\ -s /dev/null -q 25 which should output something like PE forward file : SRR957824_trimmed_R1 . fastq PE reverse file : SRR957824_trimmed_R2 . fastq Total input FastQ records : 1000000 ( 500000 pairs ) FastQ paired records kept : 834570 ( 417285 pairs ) FastQ single records kept : 13263 ( from PE1 : 11094 , from PE2 : 2169 ) FastQ paired records discarded : 138904 ( 69452 pairs ) FastQ single records discarded : 13263 ( from PE1 : 2169 , from PE2 : 11094 )","title":"Sickle"},{"location":"qc/#fastqc-again","text":"Run fastqc again on the filtered reads fastqc SRR957824_trimmed_R1.fastq SRR957824_trimmed_R2.fastq and look at the reports SRR957824_trimmed_R1_fastqc.html SRR957824_trimmed_R2_fastqc.html","title":"FastQC again"},{"location":"qc/#multiqc","text":"MultiQC is a tool that aggreagtes results from several popular QC bioinformatics software into one html report. Let's run MultiQC in our current directory multiqc . You can download the report or view it by clickinh on the link below multiqc_report.html Question What did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution?","title":"MultiQC"},{"location":"rna/","text":"RNA-Seq \u00b6 Downloading the data \u00b6 For this tutorial we will use the test data from this paper: Malachi Griffith , Jason R. Walker, Nicholas C. Spies, Benjamin J. Ainscough, Obi L. Griffith . 2015. Informatics for RNA-seq: A web resource for analysis on the cloud. PLoS Comp Biol. 11(8):e1004393. The test data consists of two commercially available RNA samples: Universal Human Reference (UHR) and Human Brain Reference (HBR). The UHR is total RNA isolated from a diverse set of 10 cancer cell lines. The HBR is total RNA isolated from the brains of 23 Caucasians, male and female, of varying age but mostly 60-80 years old. In addition, a spike-in control was used. Specifically we added an aliquot of the ERCC ExFold RNA Spike-In Control Mixes to each sample. The spike-in consists of 92 transcripts that are present in known concentrations across a wide abundance range (from very few copies to many copies). This range allows us to test the degree to which the RNA-seq assay (including all laboratory and analysis steps) accurately reflects the relative abundance of transcript species within a sample. There are two 'mixes' of these transcripts to allow an assessment of differential expression output between samples if you put one mix in each of your two comparisons. In our case, Mix1 was added to the UHR sample, and Mix2 was added to the HBR sample. We also have 3 complete experimental replicates for each sample. This allows us to assess the technical variability of our overall process of producing RNA-seq data in the lab. For all libraries we prepared low-throughput (Set A) TruSeq Stranded Total RNA Sample Prep Kit libraries with Ribo-Zero Gold to remove both cytoplasmic and mitochondrial rRNA. Triplicate, indexed libraries were made starting with 100ng Agilent/Strategene Universal Human Reference total RNA and 100ng Ambion Human Brain Reference total RNA. The Universal Human Reference replicates received 2 ul of 1:1000 ERCC Mix 1. The Human Brain Reference replicates received 1:1000 ERCC Mix 2. The libraries were quantified with KAPA Library Quantification qPCR and adjusted to the appropriate concentration for sequencing. The triplicate, indexed libraries were then pooled prior to sequencing. Each pool of three replicate libraries were sequenced across 2 lanes of a HiSeq 2000 using paired-end sequence chemistry with 100bp read lengths. So to summarize we have: UHR + ERCC Spike-In Mix1, Replicate 1 UHR + ERCC Spike-In Mix1, Replicate 2 UHR + ERCC Spike-In Mix1, Replicate 3 HBR + ERCC Spike-In Mix2, Replicate 1 HBR + ERCC Spike-In Mix2, Replicate 2 HBR + ERCC Spike-In Mix2, Replicate 3 You can download the data from here . Download and unpack the data curl -O -J -L https://osf.io/7zepj/download tar xzf toy_rna.tar.gz cd toy_rna Indexing transcriptome \u00b6 salmon index -t chr22_transcripts.fa -i chr22_index Quantify reads using salmon \u00b6 for i in *_R1.fastq.gz do prefix = $( basename $i _R1.fastq.gz ) salmon quant -i chr22_index --libType A \\ -1 ${ prefix } _R1.fastq.gz -2 ${ prefix } _R2.fastq.gz -o quant/ ${ prefix } ; done This loop simply goes through each sample and invokes salmon using fairly basic options: The -i argument tells salmon where to find the index --libType A tells salmon that it should automatically determine the library type of the sequencing reads (e.g. stranded vs. unstranded etc.) The -1 and -2 arguments tell salmon where to find the left and right reads for this sample (notice, salmon will accept gzipped FASTQ files directly). the -o argument specifies the directory where salmon\u2019s quantification results sould be written. Salmon exposes many different options to the user that enable extra features or modify default behavior. However, the purpose and behavior of all of those options is beyond the scope of this introductory tutorial. You can read about salmon\u2019s many options in the documentation . After the salmon commands finish running, you should have a directory named quant , which will have a sub-directory for each sample. These sub-directories contain the quantification results of salmon, as well as a lot of other information salmon records about the sample and the run. The main output file (called quant.sf) is rather self-explanatory. For example, take a peek at the quantification file for sample HBR_Rep1 in quant/HBR_Rep1/quant.sf and you\u2019ll see a simple TSV format file listing the name (Name) of each transcript, its length (Length), effective length (EffectiveLength) (more details on this in the documentation), and its abundance in terms of Transcripts Per Million (TPM) and estimated number of reads (NumReads) originating from this transcript. Import read counts using tximport \u00b6 Using the tximport R package, you can import salmon\u2019s transcript-level quantifications and optionally aggregate them to the gene level for gene-level differential expression analysis. First, go in Rstudio server by typing the address to your server in your browser: http://MY_IP_ADDRESS:8787/ where you replace MY_IP_ADDRESS by the IP address of your Virtual Machine. Note To access Rstudio server on the virtual machine, you'll need a password Ask your instructor for the password! Note If you wish, you may work on Rstudio on your own laptop if it is powerful enough. You will need an up-to-date version of R, and can install the necessary packages using this script You will also need to download the toy_rna directory Once in Rstudio, set your working directory setwd ( '~/toy_rna' ) Then load the modules: library ( tximport ) library ( GenomicFeatures ) library ( readr ) Salmon did the quantifiation of the transcript level. We want to see which genes are differentially expressed, so we need to link the transcript names to the gene names. We can use our .gtf annotation for that, and the GenomicFeatures package: txdb <- makeTxDbFromGFF ( \"chr22_genes.gtf\" ) k <- keys ( txdb , keytype = \"GENEID\" ) tx2gene <- select ( txdb , keys = k , keytype = \"GENEID\" , columns = \"TXNAME\" ) head ( tx2gene ) Now we can import the salmon quantification. samples <- read.table ( \"samples.txt\" , header = TRUE ) files <- file.path ( \"quant\" , samples $ sample , \"quant.sf\" ) names ( files ) <- paste0 ( samples $ sample ) txi.salmon <- tximport ( files , type = \"salmon\" , tx2gene = tx2gene ) Take a look at the data: head ( txi.salmon $ counts ) Differential expression using DESeq2 \u00b6 load DESeq2: library ( DESeq2 ) Instantiate the DESeqDataSet and generate result table. See ?DESeqDataSetFromTximport and ?DESeq for more information about the steps performed by the program. dds <- DESeqDataSetFromTximport ( txi.salmon , samples , ~ condition ) dds <- DESeq ( dds ) res <- results ( dds ) Run the summary command to get an idea of how many genes are up- and downregulated between the two conditions: summary ( res ) DESeq uses a negative binomial distribution. Such distributions have two parameters: mean and dispersion. The dispersion is a parameter describing how much the variance deviates from the mean. You can read more about the methods used by DESeq2 in the paper or the vignette Plot dispersions: plotDispEsts ( dds , main = \"Dispersion plot\" ) For clustering and heatmaps, we need to log transform our data: rld <- rlogTransformation ( dds ) head ( assay ( rld )) Then, we create a sample distance heatmap: library ( RColorBrewer ) library ( gplots ) ( mycols <- brewer.pal ( 8 , \"Dark2\" ) [1 : length ( unique ( samples $ condition )) ] ) sampleDists <- as.matrix ( dist ( t ( assay ( rld )))) heatmap.2 ( as.matrix ( sampleDists ), key = F , trace = \"none\" , col = colorpanel ( 100 , \"black\" , \"white\" ), ColSideColors = mycols[samples $ condition] , RowSideColors = mycols[samples $ condition] , margin = c ( 10 , 10 ), main = \"Sample Distance Matrix\" ) We can also plot a PCA: DESeq2 :: plotPCA ( rld , intgroup = \"condition\" ) It is time to look at some p-values: table ( res $ padj < 0.05 ) res <- res [order ( res $ padj ), ] resdata <- merge ( as.data.frame ( res ), as.data.frame ( counts ( dds , normalized = TRUE )), by = \"row.names\" , sort = FALSE ) names ( resdata ) [1] <- \"Gene\" head ( resdata ) Examine plot of p-values, the MA plot and the Volcano Plot: hist ( res $ pvalue , breaks = 50 , col = \"grey\" ) DESeq2 :: plotMA ( dds , ylim = c ( -1 , 1 ), cex = 1 ) # Volcano plot with ( res , plot ( log2FoldChange , - log10 ( pvalue ), pch = 20 , main = \"Volcano plot\" , xlim = c ( -2.5 , 2 ))) with ( subset ( res , padj < .05 ), points ( log2FoldChange , - log10 ( pvalue ), pch = 20 , col = \"red\" )) KEGG pathway analysis \u00b6 As always, load the necessary packages: library ( AnnotationDbi ) library ( org.Hs.eg.db ) library ( pathview ) library ( gage ) library ( gageData ) Let\u2019s use the mapIds function to add more columns to the results. The row.names of our results table has the Ensembl gene ID (our key), so we need to specify keytype=ENSEMBL . The column argument tells the mapIds function which information we want, and the multiVals argument tells the function what to do if there are multiple possible values for a single input value. Here we ask to just give us back the first one that occurs in the database. Let\u2019s get the Entrez IDs, gene symbols, and full gene names. res $ symbol <- mapIds ( org.Hs.eg.db , keys = row.names ( res ), column = \"SYMBOL\" , keytype = \"ENSEMBL\" , multiVals = \"first\" ) res $ entrez <- mapIds ( org.Hs.eg.db , keys = row.names ( res ), column = \"ENTREZID\" , keytype = \"ENSEMBL\" , multiVals = \"first\" ) res $ name <- mapIds ( org.Hs.eg.db , keys = row.names ( res ), column = \"GENENAME\" , keytype = \"ENSEMBL\" , multiVals = \"first\" ) head ( res ) We\u2019re going to use the gage package for pathway analysis, and the pathview package to draw a pathway diagram. The gageData package has pre-compiled databases mapping genes to KEGG pathways and GO terms for common organisms: data ( kegg.sets.hs ) data ( sigmet.idx.hs ) kegg.sets.hs <- kegg.sets.hs[sigmet.idx.hs] head ( kegg.sets.hs , 3 ) Run the pathway analysis. See help on the gage function with ?gage . Specifically, you might want to try changing the value of same.dir. foldchanges <- res $ log2FoldChange names ( foldchanges ) <- res $ entrez keggres <- gage ( foldchanges , gsets = kegg.sets.hs , same.dir = TRUE ) lapply ( keggres , head ) Pull out the top 5 upregulated pathways, then further process that just to get the IDs. We\u2019ll use these KEGG pathway IDs downstream for plotting. The dplyr package is required to use the pipe ( %>% ) construct. library ( dplyr ) # Get the pathways keggrespathways <- data.frame ( id = rownames ( keggres $ greater ), keggres $ greater ) %>% tbl_df () %>% filter ( row_number () <= 5 ) %>% .$id %>% as.character () keggrespathways # Get the IDs. keggresids <- substr ( keggrespathways , start = 1 , stop = 8 ) keggresids Finally, the pathview() function in the pathview package makes the plots. Let\u2019s write a function so we can loop through and draw plots for the top 5 pathways we created above. # Define plotting function for applying later plot_pathway <- function ( pid ) pathview ( gene.data = foldchanges , pathway.id = pid , species = \"hsa\" , new.signature = FALSE ) # Unload dplyr since it conflicts with the next line detach ( \"package:dplyr\" , unload = T ) # plot multiple pathways (plots saved to disk and returns a throwaway list object) tmp <- sapply ( keggresids , function ( pid ) pathview ( gene.data = foldchanges , pathway.id = pid , species = \"hsa\" )) Thanks \u00b6 This material was inspired by Stephen Turner's blog post: Tutorial: RNA-seq differential expression & pathway analysis with Sailfish, DESeq2, GAGE, and Pathview: http://www.gettinggeneticsdone.com/2015/12/tutorial-rna-seq-differential.html","title":"RNA-Seq"},{"location":"rna/#rna-seq","text":"","title":"RNA-Seq"},{"location":"rna/#downloading-the-data","text":"For this tutorial we will use the test data from this paper: Malachi Griffith , Jason R. Walker, Nicholas C. Spies, Benjamin J. Ainscough, Obi L. Griffith . 2015. Informatics for RNA-seq: A web resource for analysis on the cloud. PLoS Comp Biol. 11(8):e1004393. The test data consists of two commercially available RNA samples: Universal Human Reference (UHR) and Human Brain Reference (HBR). The UHR is total RNA isolated from a diverse set of 10 cancer cell lines. The HBR is total RNA isolated from the brains of 23 Caucasians, male and female, of varying age but mostly 60-80 years old. In addition, a spike-in control was used. Specifically we added an aliquot of the ERCC ExFold RNA Spike-In Control Mixes to each sample. The spike-in consists of 92 transcripts that are present in known concentrations across a wide abundance range (from very few copies to many copies). This range allows us to test the degree to which the RNA-seq assay (including all laboratory and analysis steps) accurately reflects the relative abundance of transcript species within a sample. There are two 'mixes' of these transcripts to allow an assessment of differential expression output between samples if you put one mix in each of your two comparisons. In our case, Mix1 was added to the UHR sample, and Mix2 was added to the HBR sample. We also have 3 complete experimental replicates for each sample. This allows us to assess the technical variability of our overall process of producing RNA-seq data in the lab. For all libraries we prepared low-throughput (Set A) TruSeq Stranded Total RNA Sample Prep Kit libraries with Ribo-Zero Gold to remove both cytoplasmic and mitochondrial rRNA. Triplicate, indexed libraries were made starting with 100ng Agilent/Strategene Universal Human Reference total RNA and 100ng Ambion Human Brain Reference total RNA. The Universal Human Reference replicates received 2 ul of 1:1000 ERCC Mix 1. The Human Brain Reference replicates received 1:1000 ERCC Mix 2. The libraries were quantified with KAPA Library Quantification qPCR and adjusted to the appropriate concentration for sequencing. The triplicate, indexed libraries were then pooled prior to sequencing. Each pool of three replicate libraries were sequenced across 2 lanes of a HiSeq 2000 using paired-end sequence chemistry with 100bp read lengths. So to summarize we have: UHR + ERCC Spike-In Mix1, Replicate 1 UHR + ERCC Spike-In Mix1, Replicate 2 UHR + ERCC Spike-In Mix1, Replicate 3 HBR + ERCC Spike-In Mix2, Replicate 1 HBR + ERCC Spike-In Mix2, Replicate 2 HBR + ERCC Spike-In Mix2, Replicate 3 You can download the data from here . Download and unpack the data curl -O -J -L https://osf.io/7zepj/download tar xzf toy_rna.tar.gz cd toy_rna","title":"Downloading the data"},{"location":"rna/#indexing-transcriptome","text":"salmon index -t chr22_transcripts.fa -i chr22_index","title":"Indexing transcriptome"},{"location":"rna/#quantify-reads-using-salmon","text":"for i in *_R1.fastq.gz do prefix = $( basename $i _R1.fastq.gz ) salmon quant -i chr22_index --libType A \\ -1 ${ prefix } _R1.fastq.gz -2 ${ prefix } _R2.fastq.gz -o quant/ ${ prefix } ; done This loop simply goes through each sample and invokes salmon using fairly basic options: The -i argument tells salmon where to find the index --libType A tells salmon that it should automatically determine the library type of the sequencing reads (e.g. stranded vs. unstranded etc.) The -1 and -2 arguments tell salmon where to find the left and right reads for this sample (notice, salmon will accept gzipped FASTQ files directly). the -o argument specifies the directory where salmon\u2019s quantification results sould be written. Salmon exposes many different options to the user that enable extra features or modify default behavior. However, the purpose and behavior of all of those options is beyond the scope of this introductory tutorial. You can read about salmon\u2019s many options in the documentation . After the salmon commands finish running, you should have a directory named quant , which will have a sub-directory for each sample. These sub-directories contain the quantification results of salmon, as well as a lot of other information salmon records about the sample and the run. The main output file (called quant.sf) is rather self-explanatory. For example, take a peek at the quantification file for sample HBR_Rep1 in quant/HBR_Rep1/quant.sf and you\u2019ll see a simple TSV format file listing the name (Name) of each transcript, its length (Length), effective length (EffectiveLength) (more details on this in the documentation), and its abundance in terms of Transcripts Per Million (TPM) and estimated number of reads (NumReads) originating from this transcript.","title":"Quantify reads using salmon"},{"location":"rna/#import-read-counts-using-tximport","text":"Using the tximport R package, you can import salmon\u2019s transcript-level quantifications and optionally aggregate them to the gene level for gene-level differential expression analysis. First, go in Rstudio server by typing the address to your server in your browser: http://MY_IP_ADDRESS:8787/ where you replace MY_IP_ADDRESS by the IP address of your Virtual Machine. Note To access Rstudio server on the virtual machine, you'll need a password Ask your instructor for the password! Note If you wish, you may work on Rstudio on your own laptop if it is powerful enough. You will need an up-to-date version of R, and can install the necessary packages using this script You will also need to download the toy_rna directory Once in Rstudio, set your working directory setwd ( '~/toy_rna' ) Then load the modules: library ( tximport ) library ( GenomicFeatures ) library ( readr ) Salmon did the quantifiation of the transcript level. We want to see which genes are differentially expressed, so we need to link the transcript names to the gene names. We can use our .gtf annotation for that, and the GenomicFeatures package: txdb <- makeTxDbFromGFF ( \"chr22_genes.gtf\" ) k <- keys ( txdb , keytype = \"GENEID\" ) tx2gene <- select ( txdb , keys = k , keytype = \"GENEID\" , columns = \"TXNAME\" ) head ( tx2gene ) Now we can import the salmon quantification. samples <- read.table ( \"samples.txt\" , header = TRUE ) files <- file.path ( \"quant\" , samples $ sample , \"quant.sf\" ) names ( files ) <- paste0 ( samples $ sample ) txi.salmon <- tximport ( files , type = \"salmon\" , tx2gene = tx2gene ) Take a look at the data: head ( txi.salmon $ counts )","title":"Import read counts using tximport"},{"location":"rna/#differential-expression-using-deseq2","text":"load DESeq2: library ( DESeq2 ) Instantiate the DESeqDataSet and generate result table. See ?DESeqDataSetFromTximport and ?DESeq for more information about the steps performed by the program. dds <- DESeqDataSetFromTximport ( txi.salmon , samples , ~ condition ) dds <- DESeq ( dds ) res <- results ( dds ) Run the summary command to get an idea of how many genes are up- and downregulated between the two conditions: summary ( res ) DESeq uses a negative binomial distribution. Such distributions have two parameters: mean and dispersion. The dispersion is a parameter describing how much the variance deviates from the mean. You can read more about the methods used by DESeq2 in the paper or the vignette Plot dispersions: plotDispEsts ( dds , main = \"Dispersion plot\" ) For clustering and heatmaps, we need to log transform our data: rld <- rlogTransformation ( dds ) head ( assay ( rld )) Then, we create a sample distance heatmap: library ( RColorBrewer ) library ( gplots ) ( mycols <- brewer.pal ( 8 , \"Dark2\" ) [1 : length ( unique ( samples $ condition )) ] ) sampleDists <- as.matrix ( dist ( t ( assay ( rld )))) heatmap.2 ( as.matrix ( sampleDists ), key = F , trace = \"none\" , col = colorpanel ( 100 , \"black\" , \"white\" ), ColSideColors = mycols[samples $ condition] , RowSideColors = mycols[samples $ condition] , margin = c ( 10 , 10 ), main = \"Sample Distance Matrix\" ) We can also plot a PCA: DESeq2 :: plotPCA ( rld , intgroup = \"condition\" ) It is time to look at some p-values: table ( res $ padj < 0.05 ) res <- res [order ( res $ padj ), ] resdata <- merge ( as.data.frame ( res ), as.data.frame ( counts ( dds , normalized = TRUE )), by = \"row.names\" , sort = FALSE ) names ( resdata ) [1] <- \"Gene\" head ( resdata ) Examine plot of p-values, the MA plot and the Volcano Plot: hist ( res $ pvalue , breaks = 50 , col = \"grey\" ) DESeq2 :: plotMA ( dds , ylim = c ( -1 , 1 ), cex = 1 ) # Volcano plot with ( res , plot ( log2FoldChange , - log10 ( pvalue ), pch = 20 , main = \"Volcano plot\" , xlim = c ( -2.5 , 2 ))) with ( subset ( res , padj < .05 ), points ( log2FoldChange , - log10 ( pvalue ), pch = 20 , col = \"red\" ))","title":"Differential expression using DESeq2"},{"location":"rna/#kegg-pathway-analysis","text":"As always, load the necessary packages: library ( AnnotationDbi ) library ( org.Hs.eg.db ) library ( pathview ) library ( gage ) library ( gageData ) Let\u2019s use the mapIds function to add more columns to the results. The row.names of our results table has the Ensembl gene ID (our key), so we need to specify keytype=ENSEMBL . The column argument tells the mapIds function which information we want, and the multiVals argument tells the function what to do if there are multiple possible values for a single input value. Here we ask to just give us back the first one that occurs in the database. Let\u2019s get the Entrez IDs, gene symbols, and full gene names. res $ symbol <- mapIds ( org.Hs.eg.db , keys = row.names ( res ), column = \"SYMBOL\" , keytype = \"ENSEMBL\" , multiVals = \"first\" ) res $ entrez <- mapIds ( org.Hs.eg.db , keys = row.names ( res ), column = \"ENTREZID\" , keytype = \"ENSEMBL\" , multiVals = \"first\" ) res $ name <- mapIds ( org.Hs.eg.db , keys = row.names ( res ), column = \"GENENAME\" , keytype = \"ENSEMBL\" , multiVals = \"first\" ) head ( res ) We\u2019re going to use the gage package for pathway analysis, and the pathview package to draw a pathway diagram. The gageData package has pre-compiled databases mapping genes to KEGG pathways and GO terms for common organisms: data ( kegg.sets.hs ) data ( sigmet.idx.hs ) kegg.sets.hs <- kegg.sets.hs[sigmet.idx.hs] head ( kegg.sets.hs , 3 ) Run the pathway analysis. See help on the gage function with ?gage . Specifically, you might want to try changing the value of same.dir. foldchanges <- res $ log2FoldChange names ( foldchanges ) <- res $ entrez keggres <- gage ( foldchanges , gsets = kegg.sets.hs , same.dir = TRUE ) lapply ( keggres , head ) Pull out the top 5 upregulated pathways, then further process that just to get the IDs. We\u2019ll use these KEGG pathway IDs downstream for plotting. The dplyr package is required to use the pipe ( %>% ) construct. library ( dplyr ) # Get the pathways keggrespathways <- data.frame ( id = rownames ( keggres $ greater ), keggres $ greater ) %>% tbl_df () %>% filter ( row_number () <= 5 ) %>% .$id %>% as.character () keggrespathways # Get the IDs. keggresids <- substr ( keggrespathways , start = 1 , stop = 8 ) keggresids Finally, the pathview() function in the pathview package makes the plots. Let\u2019s write a function so we can loop through and draw plots for the top 5 pathways we created above. # Define plotting function for applying later plot_pathway <- function ( pid ) pathview ( gene.data = foldchanges , pathway.id = pid , species = \"hsa\" , new.signature = FALSE ) # Unload dplyr since it conflicts with the next line detach ( \"package:dplyr\" , unload = T ) # plot multiple pathways (plots saved to disk and returns a throwaway list object) tmp <- sapply ( keggresids , function ( pid ) pathview ( gene.data = foldchanges , pathway.id = pid , species = \"hsa\" ))","title":"KEGG pathway analysis"},{"location":"rna/#thanks","text":"This material was inspired by Stephen Turner's blog post: Tutorial: RNA-seq differential expression & pathway analysis with Sailfish, DESeq2, GAGE, and Pathview: http://www.gettinggeneticsdone.com/2015/12/tutorial-rna-seq-differential.html","title":"Thanks"},{"location":"wms/","text":"Whole Metagenome Sequencin \u00b6 Table of Contents \u00b6 Introduction The Pig Microbiome Whole Metagenome Sequencing Softwares Required for this Tutorial Getting the Data and Checking their Quality Taxonomic Classification Visualization Introduction \u00b6 Microbiome used \u00b6 In this tutorial we will compare samples from the Pig Gut Microbiome to samples from the Human Gut Microbiome. Below you'll find a brief description of the two projects: The Pig Microbiome: Pig is a main species for livestock and biomedicine. The pig genome sequence was recently reported. To boost research, we established a catalogue of the genes of the gut microbiome based on faecal samples of 287 pigs from France, Denmark and China. More than 7.6 million non-redundant genes representing 719 metagenomic species were identified by deep metagenome sequencing, highlighting more similarities with the human than with the mouse catalogue. The pig and human catalogues share only 12.6 and 9.3 % of their genes, respectively, but 70 and 95% of their functional pathways. The pig gut microbiota is influenced by gender, age and breed. Analysis of the prevalence of antibiotics resistance genes (ARGs) reflected antibiotics supplementation in each farm system, and revealed that non-antibiotics-fed animals still harbour ARGs. The pig catalogue creates a resource for whole metagenomics-based studies, highly valuable for research in biomedicine and for sustainable knowledge-based pig farming The Human Microbiome: We are facing a global metabolic health crisis provoked by an obesity epidemic. Here we report the human gut microbial composition in a population sample of 123 non-obese and 169 obese Danish individuals. We find two groups of individuals that differ by the number of gut microbial genes and thus gut bacterial richness. They harbour known and previously unknown bacterial species at different proportions; individuals with a low bacterial richness (23% of the population) are characterized by more marked overall adiposity, insulin resistance and dyslipidaemia and a more pronounced inflammatory phenotype when compared with high bacterial richness individuals. The obese individuals among the former also gain more weight over time. Only a few bacterial species are sufficient to distinguish between individuals with high and low bacterial richness, and even between lean and obese. Our classifications based on variation in the gut microbiome identify subsets of individuals in the general white adult population who may be at increased risk of progressing to adiposity-associated co-morbidities Whole Metagenome Sequencing \u00b6 Whole Metagenome sequencing (WMS), or shotgun metagenome sequencing, is a relatively new and powerful sequencing approach that provides insight into community biodiversity and function. On the contrary of Metabarcoding, where only a specific region of the bacterial community (the 16s rRNA) is sequenced, WMS aims at sequencing all the genomic material present in the environment. The choice of shotgun or 16S approaches is usually dictated by the nature of the studies being conducted. For instance, 16S is well suited for analysis of large number of samples, i.e., multiple patients, longitudinal studies, etc. but offers limited taxonomical and functional resolution. WMS is generally more expensive but offers increased resolution, and allows the discovery of viruses as well as other mobile genetic elements. Softwares Required for this Tutorial \u00b6 FastQC Kraken R Pavian Prepare and organise your working directory \u00b6 You will first login to your virtual machine using the IP provided by the teachers. All the exercise will be performed on your VM in the cloud. Note When you login with the ssh command, please add the option -X at the end of it to be able to use graphical interface mkdir ~/wms cd ~/wms mkdir data mkdir results mkdir scripts Getting the Data and Checking their Quality \u00b6 As the data were very big, we have prepared performed a downsampling on all 6 datasets (3 pigs and 3 humans). We will first download and unpack the data. cd ~/wms/data curl -O -J -L https://osf.io/h9x6e/download tar xvf subset_wms.tar.gz cd sub_100000 We'll use FastQC to check the quality of our data. FastQC should be already installed on your VM, so you need to type fastqc *.fastq If the quality appears to be good, it's because it was probably the cleaned reads that were deposited into SRA. We can directly move to the classification step. Taxonomic Classification \u00b6 Kraken is a system for assigning taxonomic labels to short DNA sequences (i.e. reads) Kraken aims to achieve high sensitivity and high speed by utilizing exact alignments of k-mers and a novel classification algorithm (sic). In short, kraken uses a new approach with exact k-mer matching to assign taxonomy to short reads. It is extremely fast compared to traditional approaches (i.e. BLAST). By default, the authors of kraken built their database based on RefSeq Bacteria, Archaea and Viruses. We'll use it for the purpose of this tutorial. We will download a shrunk database (minikraken) provided by Kraken developers that is only 4GB. # First we create a databases directory in our home cd /mnt sudo mkdir databases cd databases # Then we download the minikraken database sudo wget https://ccb.jhu.edu/software/kraken/dl/minikraken_20171019_4GB.tgz sudo tar xzf minikraken_20171019_4GB.tgz KRAKEN_DB = /mnt/databases/minikraken_20171013_4GB cd Now run kraken on the reads # In the data/ directory cd ~/wms/data/sub_100000 for i in *_1.fastq do prefix = $( basename $i _1.fastq ) # print which sample is being processed echo $prefix kraken --db $KRAKEN_DB --threads 2 --fastq-input \\ ${ prefix } _1.fastq ${ prefix } _2.fastq > /home/student/wms/results/ ${ prefix } .tab kraken-report --db $KRAKEN_DB \\ /home/student/wms/results/ ${ prefix } .tab > /home/student/wms/results/ ${ prefix } _tax.txt done which produces a tab-delimited file with an assigned TaxID for each read. Kraken includes a script called kraken-report to transform this file into a \"tree\" view with the percentage of reads assigned to each taxa. We've run this script at each step in the loop. Take a look at the _tax.txt files! Visualization with Pavian \u00b6 Pavian is a web application for exploring metagenomics classification results. Install and run Pavian from R: options ( repos = c ( CRAN = \"http://cran.rstudio.com\" )) if ( ! require ( remotes )) { install.packages ( \"remotes\" ) } remotes :: install_github ( \"fbreitwieser/pavian\" ) pavian :: runApp ( port = 5000 ) Then you will explore and compare the results produced by Kraken.","title":"Whole Metagenome Sequencing"},{"location":"wms/#whole-metagenome-sequencin","text":"","title":"Whole Metagenome Sequencin"},{"location":"wms/#table-of-contents","text":"Introduction The Pig Microbiome Whole Metagenome Sequencing Softwares Required for this Tutorial Getting the Data and Checking their Quality Taxonomic Classification Visualization","title":"Table of Contents"},{"location":"wms/#introduction","text":"","title":"Introduction"},{"location":"wms/#microbiome-used","text":"In this tutorial we will compare samples from the Pig Gut Microbiome to samples from the Human Gut Microbiome. Below you'll find a brief description of the two projects: The Pig Microbiome: Pig is a main species for livestock and biomedicine. The pig genome sequence was recently reported. To boost research, we established a catalogue of the genes of the gut microbiome based on faecal samples of 287 pigs from France, Denmark and China. More than 7.6 million non-redundant genes representing 719 metagenomic species were identified by deep metagenome sequencing, highlighting more similarities with the human than with the mouse catalogue. The pig and human catalogues share only 12.6 and 9.3 % of their genes, respectively, but 70 and 95% of their functional pathways. The pig gut microbiota is influenced by gender, age and breed. Analysis of the prevalence of antibiotics resistance genes (ARGs) reflected antibiotics supplementation in each farm system, and revealed that non-antibiotics-fed animals still harbour ARGs. The pig catalogue creates a resource for whole metagenomics-based studies, highly valuable for research in biomedicine and for sustainable knowledge-based pig farming The Human Microbiome: We are facing a global metabolic health crisis provoked by an obesity epidemic. Here we report the human gut microbial composition in a population sample of 123 non-obese and 169 obese Danish individuals. We find two groups of individuals that differ by the number of gut microbial genes and thus gut bacterial richness. They harbour known and previously unknown bacterial species at different proportions; individuals with a low bacterial richness (23% of the population) are characterized by more marked overall adiposity, insulin resistance and dyslipidaemia and a more pronounced inflammatory phenotype when compared with high bacterial richness individuals. The obese individuals among the former also gain more weight over time. Only a few bacterial species are sufficient to distinguish between individuals with high and low bacterial richness, and even between lean and obese. Our classifications based on variation in the gut microbiome identify subsets of individuals in the general white adult population who may be at increased risk of progressing to adiposity-associated co-morbidities","title":"Microbiome used"},{"location":"wms/#whole-metagenome-sequencing","text":"Whole Metagenome sequencing (WMS), or shotgun metagenome sequencing, is a relatively new and powerful sequencing approach that provides insight into community biodiversity and function. On the contrary of Metabarcoding, where only a specific region of the bacterial community (the 16s rRNA) is sequenced, WMS aims at sequencing all the genomic material present in the environment. The choice of shotgun or 16S approaches is usually dictated by the nature of the studies being conducted. For instance, 16S is well suited for analysis of large number of samples, i.e., multiple patients, longitudinal studies, etc. but offers limited taxonomical and functional resolution. WMS is generally more expensive but offers increased resolution, and allows the discovery of viruses as well as other mobile genetic elements.","title":"Whole Metagenome Sequencing"},{"location":"wms/#softwares-required-for-this-tutorial","text":"FastQC Kraken R Pavian","title":"Softwares Required for this Tutorial"},{"location":"wms/#prepare-and-organise-your-working-directory","text":"You will first login to your virtual machine using the IP provided by the teachers. All the exercise will be performed on your VM in the cloud. Note When you login with the ssh command, please add the option -X at the end of it to be able to use graphical interface mkdir ~/wms cd ~/wms mkdir data mkdir results mkdir scripts","title":"Prepare and organise your working directory"},{"location":"wms/#getting-the-data-and-checking-their-quality","text":"As the data were very big, we have prepared performed a downsampling on all 6 datasets (3 pigs and 3 humans). We will first download and unpack the data. cd ~/wms/data curl -O -J -L https://osf.io/h9x6e/download tar xvf subset_wms.tar.gz cd sub_100000 We'll use FastQC to check the quality of our data. FastQC should be already installed on your VM, so you need to type fastqc *.fastq If the quality appears to be good, it's because it was probably the cleaned reads that were deposited into SRA. We can directly move to the classification step.","title":"Getting the Data and Checking their Quality"},{"location":"wms/#taxonomic-classification","text":"Kraken is a system for assigning taxonomic labels to short DNA sequences (i.e. reads) Kraken aims to achieve high sensitivity and high speed by utilizing exact alignments of k-mers and a novel classification algorithm (sic). In short, kraken uses a new approach with exact k-mer matching to assign taxonomy to short reads. It is extremely fast compared to traditional approaches (i.e. BLAST). By default, the authors of kraken built their database based on RefSeq Bacteria, Archaea and Viruses. We'll use it for the purpose of this tutorial. We will download a shrunk database (minikraken) provided by Kraken developers that is only 4GB. # First we create a databases directory in our home cd /mnt sudo mkdir databases cd databases # Then we download the minikraken database sudo wget https://ccb.jhu.edu/software/kraken/dl/minikraken_20171019_4GB.tgz sudo tar xzf minikraken_20171019_4GB.tgz KRAKEN_DB = /mnt/databases/minikraken_20171013_4GB cd Now run kraken on the reads # In the data/ directory cd ~/wms/data/sub_100000 for i in *_1.fastq do prefix = $( basename $i _1.fastq ) # print which sample is being processed echo $prefix kraken --db $KRAKEN_DB --threads 2 --fastq-input \\ ${ prefix } _1.fastq ${ prefix } _2.fastq > /home/student/wms/results/ ${ prefix } .tab kraken-report --db $KRAKEN_DB \\ /home/student/wms/results/ ${ prefix } .tab > /home/student/wms/results/ ${ prefix } _tax.txt done which produces a tab-delimited file with an assigned TaxID for each read. Kraken includes a script called kraken-report to transform this file into a \"tree\" view with the percentage of reads assigned to each taxa. We've run this script at each step in the loop. Take a look at the _tax.txt files!","title":"Taxonomic Classification"},{"location":"wms/#visualization-with-pavian","text":"Pavian is a web application for exploring metagenomics classification results. Install and run Pavian from R: options ( repos = c ( CRAN = \"http://cran.rstudio.com\" )) if ( ! require ( remotes )) { install.packages ( \"remotes\" ) } remotes :: install_github ( \"fbreitwieser/pavian\" ) pavian :: runApp ( port = 5000 ) Then you will explore and compare the results produced by Kraken.","title":"Visualization with Pavian"}]}